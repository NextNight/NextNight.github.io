<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Kala</title>
  
  <subtitle>Do Best Kala</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://nextnight.github.io/"/>
  <updated>2018-09-29T07:32:02.063Z</updated>
  <id>http://nextnight.github.io/</id>
  
  <author>
    <name>Best Kala</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>SNA-社交网络分析</title>
    <link href="http://nextnight.github.io/2018/09/29/SNA-%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C%E5%88%86%E6%9E%90/"/>
    <id>http://nextnight.github.io/2018/09/29/SNA-社交网络分析/</id>
    <published>2018-09-29T07:32:02.000Z</published>
    <updated>2018-09-29T07:32:02.063Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="class1" scheme="http://nextnight.github.io/categories/class1/"/>
    
      <category term="class2" scheme="http://nextnight.github.io/categories/class1/class2/"/>
    
    
      <category term="tg1" scheme="http://nextnight.github.io/tags/tg1/"/>
    
      <category term="tag2" scheme="http://nextnight.github.io/tags/tag2/"/>
    
  </entry>
  
  <entry>
    <title>ML-DTree决策树</title>
    <link href="http://nextnight.github.io/2018/09/29/ML-DTree%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>http://nextnight.github.io/2018/09/29/ML-DTree决策树/</id>
    <published>2018-09-29T07:16:44.000Z</published>
    <updated>2018-09-29T07:18:02.425Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="ML" scheme="http://nextnight.github.io/categories/ML/"/>
    
    
      <category term="决策树" scheme="http://nextnight.github.io/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>ML-Svm支持向量机</title>
    <link href="http://nextnight.github.io/2018/09/29/ML-Svm%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <id>http://nextnight.github.io/2018/09/29/ML-Svm支持向量机/</id>
    <published>2018-09-29T07:15:10.000Z</published>
    <updated>2018-09-29T07:17:46.640Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="ML" scheme="http://nextnight.github.io/categories/ML/"/>
    
    
      <category term="支持向量机" scheme="http://nextnight.github.io/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    
      <category term="svm" scheme="http://nextnight.github.io/tags/svm/"/>
    
  </entry>
  
  <entry>
    <title>ML-Logistic回归</title>
    <link href="http://nextnight.github.io/2018/09/29/ML-Logistic%E5%9B%9E%E5%BD%92/"/>
    <id>http://nextnight.github.io/2018/09/29/ML-Logistic回归/</id>
    <published>2018-09-29T07:13:53.000Z</published>
    <updated>2018-09-29T07:17:31.024Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="ML" scheme="http://nextnight.github.io/categories/ML/"/>
    
    
      <category term="逻辑回归" scheme="http://nextnight.github.io/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>Match-DaGuanCup</title>
    <link href="http://nextnight.github.io/2018/09/29/Match-DaGuanCup/"/>
    <id>http://nextnight.github.io/2018/09/29/Match-DaGuanCup/</id>
    <published>2018-09-29T07:02:57.000Z</published>
    <updated>2018-09-29T07:08:18.192Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="Match" scheme="http://nextnight.github.io/categories/Match/"/>
    
    
      <category term="文本分类" scheme="http://nextnight.github.io/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>Match-LianTong</title>
    <link href="http://nextnight.github.io/2018/09/29/Match-LianTong/"/>
    <id>http://nextnight.github.io/2018/09/29/Match-LianTong/</id>
    <published>2018-09-29T06:46:00.000Z</published>
    <updated>2018-09-29T07:08:03.400Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="Match" scheme="http://nextnight.github.io/categories/Match/"/>
    
    
      <category term="分类推荐" scheme="http://nextnight.github.io/tags/%E5%88%86%E7%B1%BB%E6%8E%A8%E8%8D%90/"/>
    
  </entry>
  
  <entry>
    <title>Match-YunYiCup</title>
    <link href="http://nextnight.github.io/2018/09/29/Match-Yunyicup/"/>
    <id>http://nextnight.github.io/2018/09/29/Match-Yunyicup/</id>
    <published>2018-09-29T06:42:48.000Z</published>
    <updated>2018-09-29T07:07:08.749Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="Match" scheme="http://nextnight.github.io/categories/Match/"/>
    
    
      <category term="文本分类" scheme="http://nextnight.github.io/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>Graph-BayesNetWork</title>
    <link href="http://nextnight.github.io/2018/09/29/Graph-BayesNetWork/"/>
    <id>http://nextnight.github.io/2018/09/29/Graph-BayesNetWork/</id>
    <published>2018-09-29T06:40:58.000Z</published>
    <updated>2018-09-29T07:35:49.614Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="概率图模型" scheme="http://nextnight.github.io/categories/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="BayesNetWork" scheme="http://nextnight.github.io/tags/BayesNetWork/"/>
    
      <category term="贝叶斯网络" scheme="http://nextnight.github.io/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Graph-CRF条件随机场</title>
    <link href="http://nextnight.github.io/2018/09/29/Graph-CRF%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"/>
    <id>http://nextnight.github.io/2018/09/29/Graph-CRF条件随机场/</id>
    <published>2018-09-29T06:30:53.000Z</published>
    <updated>2018-09-29T07:35:18.776Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="概率图模型" scheme="http://nextnight.github.io/categories/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="CRF" scheme="http://nextnight.github.io/tags/CRF/"/>
    
      <category term="条件随机场" scheme="http://nextnight.github.io/tags/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>Graph-HMM隐马尔科夫</title>
    <link href="http://nextnight.github.io/2018/09/29/Graph-HMM%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB/"/>
    <id>http://nextnight.github.io/2018/09/29/Graph-HMM隐马尔科夫/</id>
    <published>2018-09-29T06:30:37.000Z</published>
    <updated>2018-09-29T07:34:54.564Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="概率图模型" scheme="http://nextnight.github.io/categories/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="HMM" scheme="http://nextnight.github.io/tags/HMM/"/>
    
  </entry>
  
  <entry>
    <title>Math-矩阵分解</title>
    <link href="http://nextnight.github.io/2018/09/29/Math-%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/"/>
    <id>http://nextnight.github.io/2018/09/29/Math-矩阵分解/</id>
    <published>2018-09-29T06:27:30.000Z</published>
    <updated>2018-09-29T07:29:18.171Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="Math" scheme="http://nextnight.github.io/categories/Math/"/>
    
    
      <category term="矩阵分解" scheme="http://nextnight.github.io/tags/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/"/>
    
  </entry>
  
  <entry>
    <title>Math-凸优化问题</title>
    <link href="http://nextnight.github.io/2018/09/29/Math-%E5%87%B8%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98/"/>
    <id>http://nextnight.github.io/2018/09/29/Math-凸优化问题/</id>
    <published>2018-09-29T06:25:00.000Z</published>
    <updated>2018-09-29T07:18:50.840Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="凸优化" scheme="http://nextnight.github.io/categories/%E5%87%B8%E4%BC%98%E5%8C%96/"/>
    
    
      <category term="凸优化" scheme="http://nextnight.github.io/tags/%E5%87%B8%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Math-泰勒展式</title>
    <link href="http://nextnight.github.io/2018/09/29/Math-%E6%B3%B0%E5%8B%92%E5%B1%95%E5%BC%8F/"/>
    <id>http://nextnight.github.io/2018/09/29/Math-泰勒展式/</id>
    <published>2018-09-29T06:21:53.000Z</published>
    <updated>2018-09-29T07:29:40.308Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="Math" scheme="http://nextnight.github.io/categories/Math/"/>
    
    
      <category term="泰勒展式" scheme="http://nextnight.github.io/tags/%E6%B3%B0%E5%8B%92%E5%B1%95%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>Math-极大似然估计</title>
    <link href="http://nextnight.github.io/2018/09/29/Math-%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/"/>
    <id>http://nextnight.github.io/2018/09/29/Math-极大似然估计/</id>
    <published>2018-09-29T06:21:27.000Z</published>
    <updated>2018-09-29T07:18:26.798Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="Math" scheme="http://nextnight.github.io/categories/Math/"/>
    
    
      <category term="MLE" scheme="http://nextnight.github.io/tags/MLE/"/>
    
      <category term="极大似然估计" scheme="http://nextnight.github.io/tags/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>文本处理流程</title>
    <link href="http://nextnight.github.io/2018/09/29/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B/"/>
    <id>http://nextnight.github.io/2018/09/29/文本处理流程/</id>
    <published>2018-09-29T05:49:41.000Z</published>
    <updated>2018-09-29T06:57:26.303Z</updated>
    
    <content type="html"><![CDATA[<h2 id="文本处理的流程"><a href="#文本处理的流程" class="headerlink" title="文本处理的流程"></a>文本处理的流程</h2><p><code>文本</code>–&gt;<code>分词</code>–&gt;<code>向量化</code>–&gt;<code>建模</code></p><h2 id="1、分词方法"><a href="#1、分词方法" class="headerlink" title="1、分词方法"></a>1、分词方法</h2><blockquote><p>分词是NLP的基础，无论是文档还是语句都是有基本的词构成，因此分词的好坏直接影响到文本的表示。</p></blockquote><ol><li>基本匹配分词</li></ol><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">此方法按照不同的扫描方式，逐个查找词库进行分词。</span><br><span class="line">根据扫描方式可细分为：</span><br><span class="line">    正向最大匹配</span><br><span class="line">    反向最大匹配</span><br><span class="line">    双向最大匹配</span><br><span class="line">    最小切分<span class="comment">(即最短路径)</span></span><br><span class="line">总之就是各种不同的启发规则</span><br></pre></td></tr></table></figure><ol start="2"><li>全局切分+语言模型分词</li></ol><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby"> 基本操作：</span></span><br><span class="line"><span class="ruby">  <span class="number">1</span>、它首先切分出与词库匹配的所有可能的词，</span></span><br><span class="line"><span class="ruby">  <span class="number">2</span>、再运用统计语言模型决定最优的切分结果。【一般采用viterbi算法寻找找最优的组合】</span></span><br><span class="line"><span class="ruby">- 优点：</span></span><br><span class="line"><span class="ruby">  <span class="number">1</span>、在于可以解决分词中的歧义问题。</span></span><br><span class="line"><span class="ruby">- 示例：</span></span><br><span class="line"><span class="ruby">  对于文本串“南京市长江大桥”，首先进行词条检索(一般用Trie存储)，找到匹配的所有词条（南京，市，长江，大桥，南京市，长江大桥，市长，江大桥，江大，桥），以词网格(word lattices)形式表示，接着做路径搜索，基于统计语言模型(例如n-gram)找到最优路径，最后可能还需要命名实体识别。</span></span><br></pre></td></tr></table></figure><ol start="3"><li>以字构词</li></ol><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">- 基本操作：</span><br><span class="line">  <span class="number">1</span>、将分词问题转化为了单个字的分类问题(序列标签)，为每一个字打上<span class="string">'B'</span>,<span class="string">'I'</span>,<span class="string">'E'</span>,<span class="string">'S'</span>五个标签中的一个。其中<span class="string">'B'</span>:一个词的开头，<span class="string">'I'</span>:一个词的中间，<span class="string">'E'</span>:一个词的结尾，<span class="string">'S'</span>:单个字成词。</span><br><span class="line">  <span class="number">2</span>、通过统计信息得到每个字的标签的概率，然后采用<span class="string">'Viterbi算法'</span>得到最优的结果。</span><br><span class="line">- 优点：</span><br><span class="line">  <span class="number">1</span>、具有新词发现功能，准确率较高。</span><br><span class="line">- 实现：</span><br><span class="line">  <span class="number">1</span>、<span class="string">'HMM分词'</span>,</span><br><span class="line">  <span class="number">2</span>、<span class="string">'CRF分词'</span>,</span><br><span class="line">  <span class="number">3</span>、<span class="string">'深度学习分词'</span></span><br><span class="line"></span><br><span class="line">- [<span class="number">1</span>] 深度学习分词的流程：</span><br><span class="line">    <span class="number">-1</span>、将每一个字<span class="symbol">Lookup</span> <span class="symbol">Table</span>映射到一个固定长度的特征向量.</span><br><span class="line">    <span class="number">-2</span>、经过一个标准的神经网络：liner-sigmod-liner三层<span class="string">'同样得到每个字属于B,I,E,S四个Tag的概率'</span>.</span><br><span class="line">    <span class="number">-3</span>、采用<span class="string">'Viterbi算法'</span>求得最优结果.</span><br><span class="line"></span><br><span class="line">- [<span class="number">2</span>] <span class="symbol">HMM</span>分词 <span class="symbol">VS</span> <span class="symbol">CRF</span>分词</span><br><span class="line">    <span class="symbol">CRF</span>：目前效果已经优于<span class="symbol">HMM</span>，无论是新词发现还是实体识别，人名识别。</span><br><span class="line">    优点:</span><br><span class="line">        <span class="number">1</span>、在于<span class="symbol">CRF</span>既可以像最大熵模型一样加各种领域feature。</span><br><span class="line">        <span class="number">2</span>、又避免了<span class="symbol">HMM</span>的齐次马尔科夫假设。</span><br></pre></td></tr></table></figure><h2 id="2、语言模型："><a href="#2、语言模型：" class="headerlink" title="2、语言模型："></a>2、语言模型：</h2><blockquote><p>语言模型是用来计算一句话生成概率的模型，</p></blockquote><h3 id="1、语言模型的演变"><a href="#1、语言模型的演变" class="headerlink" title="1、语言模型的演变"></a>1、语言模型的演变</h3><p>[1.1] bayes:</p><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby"> 定义：每个词出现的概率取决于前面所有的词</span></span><br><span class="line"><span class="ruby">- 公式：P(w_m) = p(w_m<span class="params">|w_&#123;m-1&#125;,w_&#123;m-2&#125;..w_1)</span></span></span><br><span class="line"><span class="ruby">- P(S)：P(S) = P(w_1)*p(w_2)...p(w_n),n为词或字个数,P(S)表示句子概率</span></span><br><span class="line"><span class="ruby">- 缺点：计算过于复杂，甚至难以计算</span></span><br><span class="line"><span class="ruby">- 简化：朴素贝叶斯，条件独立性假设</span></span><br></pre></td></tr></table></figure><p>[1.2] n-gram：n元词型<br><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby"> 定义：衍生自HMM，它利用马尔科夫假设，认为句子中每个单词只与其前n–<span class="number">1</span>个单词有关，即假设产生w_m这个词的条件概率只依赖于前n–<span class="number">1</span>个词。其中n越大，模型可区别性越强，n越小，模型可靠性越高。</span></span><br><span class="line"><span class="ruby">- 公式：p(w_m) = p(w_m<span class="params">|w_&#123;m-n+1&#125;...,w_&#123;m-1&#125;))</span></span></span><br><span class="line"><span class="ruby">- P(S)：P(S) = P(w_1)*p(w_2)...p(w_n),n为词或字个数,P(S)表示句子概率</span></span><br><span class="line"><span class="ruby">- 优点：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、简单有效，多元词型组合可以得到更丰富的信息</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、考虑了词的位置关系</span></span><br><span class="line"><span class="ruby">- 不足：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、没有考虑到词语的相似性</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、没有考虑词法，语法，以及语义信息</span></span><br><span class="line"><span class="ruby">    <span class="number">3</span>、仍然存在数据稀疏的问题</span></span><br></pre></td></tr></table></figure></p><p>[1.3] ffnnlm：前馈神经网络语言模型<br><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby"> 定义：基于n-gram的语言模型，他将词<span class="string">"w_m"</span>的前n-<span class="number">1</span>个词p(w<span class="number">_</span>&#123;m-n+<span class="number">1</span>&#125;...p(w<span class="number">_</span>&#123;m-<span class="number">1</span>&#125;))映射到词向量空间，然后把它们拼接起来得到一个更大的词向量作为神经网络的输入，<span class="string">'输出'</span>的是p(w_m)</span></span><br><span class="line"><span class="ruby">- 优点：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、词语之间的相似性可以通过词向量来体现</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、自带平滑功能</span></span><br></pre></td></tr></table></figure></p><p>[1.4] rnnlm:循环神经网络语言模型<br><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby"> rnnlm特点：可以存在有向环，将上一次的输出作为本次的输入。</span></span><br><span class="line"><span class="ruby">- rnnlm和ffnnlm的最大区别是：ffnnmm要求输入的上下文是固定长度的，也就是说n-gram中的 n要求是个固定值，而rnnlm不限制上下文的长度，可以真正充分地利用所有上文信息来预测下一个词，本次预测的中间隐层信息(例如下图中的context信息)可以在下一次预测里循环使用。</span></span><br><span class="line"><span class="ruby">- 基本结构：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、输入层：</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、隐藏层：</span></span><br><span class="line"><span class="ruby">    <span class="number">3</span>、输出层：</span></span><br><span class="line"><span class="ruby">- 预测步骤：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、单词w<span class="number">_</span>&#123;m-<span class="number">1</span>&#125;映射到词向量，记作input(t)</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、连接上一次训练的隐藏层context(t–<span class="number">1</span>)，经过sigmoid function，生成当前t时刻的context(t)</span></span><br><span class="line"><span class="ruby">    <span class="number">3</span>、利用softmax function，预测P(w_m)</span></span><br><span class="line"><span class="ruby">- 缺点：基于RNN的语言模型利用BPTT(BackPropagation through time)算法比较难于训练，原因就是深度神经网络里比较普遍的梯度消失问题。</span></span><br><span class="line"><span class="ruby">- 优点：训练精度高</span></span><br></pre></td></tr></table></figure></p><p>[1.5] lstmlm:长短记忆神经网络<br><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby"> lstmlm特点：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、也是一种RNN网络，通过结构的修改避免了梯度消失。</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、能够记忆更多的信息。</span></span><br></pre></td></tr></table></figure></p><h3 id="2、N-gram的基本原理"><a href="#2、N-gram的基本原理" class="headerlink" title="2、N-gram的基本原理"></a>2、N-gram的基本原理</h3><blockquote><p>N-gram即N元语法模型，采用马尔科夫假设，认为一个词生成的概率取决于他前面的N-1个词。</p></blockquote><figure class="highlight erlang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">- 约定：</span><br><span class="line">    s:代表句子</span><br><span class="line">    w_i:第i个词</span><br><span class="line">    w_n:代表第n个词</span><br><span class="line">    N:词数</span><br><span class="line">- 句子s生成的概率如下：</span><br><span class="line">    P<span class="params">(s)</span> = p<span class="params">(w_1)</span>p<span class="params">(w_2)</span>p<span class="params">(w_3)</span>...p(w_n)</span><br><span class="line">- "为了避免数据溢出、提高性能，通常会使用取 log 后使用加法运算替代乘法运算"即：</span><br><span class="line">- log<span class="params">(p(s))</span> = log<span class="params">(p(w_1))</span>+log<span class="params">(p(w_2))</span>+log<span class="params">(p(w_3))</span>...log(p(w_n))</span><br></pre></td></tr></table></figure><p>[2.1]、<code>bigram</code>:假设一个词生成的概率、取决于他前面1个词</p><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">- 句子s生成的概率如下：</span><br><span class="line">    P<span class="comment">(s)</span> = p<span class="comment">(w_1)</span>p<span class="comment">(w_2)</span>p<span class="comment">(w_3)</span>...p<span class="comment">(w_n)</span></span><br><span class="line">- bigram下词w_m生成的概率：</span><br><span class="line">    p<span class="comment">(w_1)</span> = p<span class="comment">(w_1)</span></span><br><span class="line">    p<span class="comment">(w_2)</span> = p<span class="comment">(w_2|w_1)</span></span><br><span class="line">    p<span class="comment">(w_3)</span> = p<span class="comment">(w_3|w_2)</span></span><br><span class="line">    ...</span><br><span class="line">    p<span class="comment">(w_m)</span> = p<span class="comment">(w_m|w_m-1)</span></span><br><span class="line">- 得到句子生成的概率：</span><br><span class="line">    P<span class="comment">(s)</span> = p<span class="comment">(w_1)</span>p<span class="comment">(w_2|w_1)</span>p<span class="comment">(w_3|w_2)</span>..p<span class="comment">(w_m|w_m-1)</span>..p<span class="comment">(w_n|w_n-1)</span>-</span><br></pre></td></tr></table></figure><p>[2.2]、<code>trigram</code>:假设一个词的生成概率取决于他前面的2个词</p><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">- 句子s生成的概率如下：</span><br><span class="line">    P<span class="comment">(s)</span> = p<span class="comment">(w_1)</span>p<span class="comment">(w_2)</span>p<span class="comment">(w_3)</span>...p<span class="comment">(w_n)</span></span><br><span class="line">- trigram下词w_m生成的概率：</span><br><span class="line">    p<span class="comment">(w_1)</span> = p<span class="comment">(w_1)</span></span><br><span class="line">    p<span class="comment">(w_2)</span> = p<span class="comment">(w_2|w_1)</span></span><br><span class="line">    p<span class="comment">(w_3)</span> = p<span class="comment">(w_3|w_&#123;1,2&#125;)</span></span><br><span class="line">    ...</span><br><span class="line">    p<span class="comment">(w_m)</span> = p<span class="comment">(w_m|w_&#123;m-2,m-1&#125;)</span></span><br><span class="line">- 得到句子生成的概率：</span><br><span class="line">    P<span class="comment">(s)</span> = p<span class="comment">(w_1)</span>p<span class="comment">(w_2|w_1)</span>p<span class="comment">(w_3|w_1,w_2)</span>..p<span class="comment">(w_m|w_m-2,w_m-1)</span>..p<span class="comment">(w_n|w_n-1,w_n-2)</span>-</span><br></pre></td></tr></table></figure><blockquote><p>如上知道了如何去计算一个句子生成的概率，那么该如何去算其中的每一项呢？其实不过还是频率来表示概率。</p><p><code>bigram</code>中<code>P(w_m) = p(w_m|w_m-1)=count(w_m,w_m-1)/count(w_m-1)</code>即计算当前词的概率用当前词和前一个词共同出现的次数比上前一个词出现的次数。</p></blockquote><p>[2.3] bigram示例：</p><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- 语料：</span><br><span class="line">    <span class="built_in">s1</span>：我喜欢西红柿炒鸡蛋</span><br><span class="line">    <span class="built_in">s2</span>：西红柿是我最爱</span><br><span class="line"><span class="symbol">    s3:</span> 我喜欢什么</span><br></pre></td></tr></table></figure><p>单词词频：</p><table><thead><tr><th>word</th><th>我</th><th>喜欢</th><th>西红柿</th><th>炒</th><th>鸡蛋</th><th>是</th><th>最爱</th><th>什么</th></tr></thead><tbody><tr><td>count</td><td>2</td><td>2</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td></tr><tr><td>p(word)</td><td>0.2</td><td>0.2</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td></tr></tbody></table><figure class="highlight erlang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- 句子s1出现的概率：</span><br><span class="line">    p<span class="params">(s1)</span> = p<span class="params">(我)</span>p<span class="params">(喜欢)</span>p<span class="params">(西红柿)</span>p<span class="params">(炒)</span>p<span class="params">(鸡蛋)</span></span><br><span class="line">        = p<span class="params">(我)</span>p<span class="params">(喜欢|我)</span>p<span class="params">(西红柿|喜欢)</span>p<span class="params">(炒|西红柿)</span>p<span class="params">(鸡蛋|炒)</span></span><br><span class="line">- 根据顺序共现得到bigram的共现词频如下：</span><br></pre></td></tr></table></figure><p>顺序共现词频：</p><table><thead><tr><th style="text-align:center">word</th><th>我</th><th>喜欢</th><th>西红柿</th><th>炒</th><th>鸡蛋</th><th>是</th><th>最爱</th><th>什么</th></tr></thead><tbody><tr><td style="text-align:center">我</td><td></td><td>2</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td style="text-align:center">喜欢</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td style="text-align:center">西红柿</td><td></td><td></td><td></td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td style="text-align:center">炒</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td style="text-align:center">鸡蛋</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td style="text-align:center">是</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td style="text-align:center">最爱</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td style="text-align:center">什么</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><figure class="highlight erlang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">- 最后得到句子s1的概率如下：</span><br><span class="line">  p<span class="params">(s1)</span> = p<span class="params">(我)</span>p<span class="params">(喜欢)</span>p<span class="params">(西红柿)</span>p<span class="params">(炒)</span>p<span class="params">(鸡蛋)</span></span><br><span class="line">        = p<span class="params">(我)</span>p<span class="params">(喜欢|我)</span>p<span class="params">(西红柿|喜欢)</span>p<span class="params">(炒|西红柿)</span>p<span class="params">(鸡蛋|炒)</span></span><br><span class="line">        = 0.<span class="number">2</span> * (count(我喜欢)/count(我))</span><br><span class="line">              * (count(喜欢西红柿)/count(喜欢))</span><br><span class="line">              * (count(西红柿炒)/count(西红柿))</span><br><span class="line">              * (count(炒鸡蛋)/count(炒))</span><br></pre></td></tr></table></figure><p>同理可得trigram，n-gram的计算方式，分别去统计N元词出现的次数。得到条件概率。</p><p>[2.4]、数据平滑</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">- 数据平滑：大规模数据统计方法与有限的训练语料之间必然产生数据稀疏问题，导致零概率问题。为了解决数据稀疏问题，人们为理论模型实用化而进行了众多尝试与努力，诞生了一系列经典的平滑技术。</span><br><span class="line">- 基本思想：“降低已出现 n-gram 的条件概率分布，以使未出现的 n-gram 条件概率分布非零”，且经数据平滑后一定保证概率和为<span class="number">1</span>，</span><br><span class="line">- 平滑技术：</span><br><span class="line">      <span class="number">1</span>、加一平滑：又称拉普拉斯平滑</span><br><span class="line">      Pmle(w_m|w_m<span class="number">-1</span>) = count(w_m,w_m<span class="number">-1</span>)/count(w_m<span class="number">-1</span>)</span><br><span class="line">      Padd1(w_m|w_m<span class="number">-1</span>) = count(w_m,w_m<span class="number">-1</span>)+<span class="number">1</span>/count(w_m<span class="number">-1</span>)+VV代表<span class="number">2</span>元词型的个数。V个二元词型保证概率和为<span class="number">1</span>。</span><br></pre></td></tr></table></figure><h2 id="3、向量化"><a href="#3、向量化" class="headerlink" title="3、向量化"></a>3、向量化</h2><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">文本的向量化主要有以下几种：</span><br><span class="line">    <span class="selector-attr">[1]</span> <span class="selector-tag">countvec</span>：词袋模型，词频表示</span><br><span class="line">    <span class="selector-attr">[2]</span> <span class="selector-tag">Tf-idf</span>：基于<span class="selector-tag">bow</span>的权重表示</span><br><span class="line">    <span class="selector-attr">[3]</span> 主题模型<span class="selector-pseudo">:PLSA</span>，<span class="selector-tag">LDA</span></span><br><span class="line">    <span class="selector-attr">[4]</span> 词嵌入：<span class="selector-tag">word2vec</span>,<span class="selector-tag">glove</span>,<span class="selector-tag">fastext</span></span><br><span class="line">约定：</span><br><span class="line">    1、词袋：数据集中所有的的词构成的词集或词典。</span><br><span class="line">    2、词频：一篇文档中某个词出现的次数</span><br></pre></td></tr></table></figure><h3 id="1、词频向量：countvec"><a href="#1、词频向量：countvec" class="headerlink" title="1、词频向量：countvec"></a>1、词频向量：countvec</h3><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby">   定义：把每一篇文档表示为一个长度为词袋大小的向量，向量的每一个维度表示为词袋中的每一个词在此文档中出现的次数。</span></span><br><span class="line"><span class="ruby"></span></span><br><span class="line"><span class="ruby">-   优缺点：</span></span><br><span class="line"><span class="ruby">     <span class="number">1</span>、能够一定程度反映词频信息，但是对于多篇文档中都出现的高频词没有银锭的区分度，</span></span><br><span class="line"><span class="ruby">     <span class="number">2</span>、词袋模型向量大多是高维稀疏矩阵。</span></span><br></pre></td></tr></table></figure><h3 id="2、权重向量：tf-idf"><a href="#2、权重向量：tf-idf" class="headerlink" title="2、权重向量：tf-idf"></a>2、权重向量：tf-idf</h3><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby"> 定义：词频(TF)x逆文档词频(IDF)用于表示一个词的权重。</span></span><br><span class="line"><span class="ruby">- 优点：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、降低多个文档都出现的高频词汇对重要度的影响。</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、提升低频词汇的重要度。</span></span><br><span class="line"><span class="ruby">    <span class="number">3</span>、tf-idf+<span class="string">'N-gram'</span>特征一定程度能够反映语序特征</span></span><br><span class="line"><span class="ruby">- 缺点：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、权重向量也是高维稀疏矩阵.</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、不能完全反映语序信息</span></span><br><span class="line"><span class="ruby">    <span class="number">3</span>、不能体现词语间的相似度信息。</span></span><br></pre></td></tr></table></figure><h3 id="3、主题向量：plsa-lda"><a href="#3、主题向量：plsa-lda" class="headerlink" title="3、主题向量：plsa,lda"></a>3、主题向量：plsa,lda</h3><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby"> 定义：通过引入隐变量<span class="string">'主题'</span>将词频向量或者权重向量映射到一个固定维度的稠密向量。</span></span><br><span class="line"><span class="ruby">- 优点：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、降维作用，降低了模型的复杂度和计算量。</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、包含了一定的语义信息。</span></span><br><span class="line"><span class="ruby">- 缺点：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、主题数难以确定。</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、主题作为一种概率分布表示，难以表述其具体含义，可解释性较差。</span></span><br><span class="line"><span class="ruby">    <span class="number">3</span>、数据量大的时候主题训练耗时较长。</span></span><br></pre></td></tr></table></figure><h3 id="4、词嵌入向量-word2vec-glove-fasttext"><a href="#4、词嵌入向量-word2vec-glove-fasttext" class="headerlink" title="4、词嵌入向量:word2vec,glove,fasttext"></a>4、词嵌入向量:word2vec,glove,fasttext</h3><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby"> 定义：通过训练将每一个词映射到固定长度的词向量空间中，每个词就是一个点，同时引入距离的概念，就可以描述词语的相似度。</span></span><br><span class="line"><span class="ruby">- 优点：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、考虑到了词语的相似度信息。</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、维度固定且低维，便于后续计算。</span></span><br><span class="line"><span class="ruby">    <span class="number">3</span>、自带平滑功能</span></span><br><span class="line"><span class="ruby">- 缺点：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、好的词向量表示依赖于足够的训练预料，尤其是领域性较强的文本依赖于相关的领域文本，否则可能得不到足够好的词向量。</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、词向量的训练需要额外的步骤。且大语料的训练耗时。</span></span><br></pre></td></tr></table></figure><h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby"> 模型结构：</span></span><br><span class="line"><span class="ruby">    <span class="params">|--&gt;输入层</span></span></span><br><span class="line"><span class="ruby">        <span class="params">|--&gt;隐藏层</span></span></span><br><span class="line"><span class="ruby">            <span class="params">|--&gt;softmax</span></span></span><br><span class="line"><span class="ruby">- 两种实现方式：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、cbow</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、skip-gram</span></span><br><span class="line"><span class="ruby">- 两种优化方式：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、层次化softmax</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、负采样</span></span><br></pre></td></tr></table></figure><p>🐱目标函数：</p><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby"> <span class="symbol">w:</span>词w</span></span><br><span class="line"><span class="ruby">- <span class="symbol">C:</span>语料库</span></span><br><span class="line"><span class="ruby">- Context(w)<span class="symbol">:</span>词w的上下文信息</span></span><br></pre></td></tr></table></figure><ul><li><p>cbow:p(w|Context(w))：上下文词出现的情况下，词w出现的概率，即共现的概率。也是优化的目标，最大化这个概率。那么对与整个语料来说，目标函数就是最大化如下的概率积</p><p>$$L = \prod_{w\epsilon C}p(w|Context(w))$$</p></li><li><p>skip-gram:以当前词预测上下文词的概率，即目标函数就是最大化当前词出现的时候上下文词出现的概率积。</p><p>$$L = \prod_{w\epsilon C}p(Context(w)|w)$$</p></li><li><p>最大对数似然：</p><p>$$L = \sum_{w\epsilon C}log(p(w|Cotext(w))))$$</p><p>$$L = \sum_{w\epsilon C}log(p(Context(w)|w))$$</p></li></ul><figure class="highlight erlang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- 训练过程：</span><br><span class="line">    1、输入层：随机初始化w的context<span class="params">(w)</span>的每一个词<span class="params">(前后各c个)</span>的词向量V<span class="params">(w)</span>。</span><br><span class="line">    2、投影层：将词w的context<span class="params">(w)</span>的所有V<span class="params">(w)</span>求和。</span><br><span class="line">    3、输出层：层次化softmax层，一颗二叉树，用计算得到的输出每个词的概率。</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;文本处理的流程&quot;&gt;&lt;a href=&quot;#文本处理的流程&quot; class=&quot;headerlink&quot; title=&quot;文本处理的流程&quot;&gt;&lt;/a&gt;文本处理的流程&lt;/h2&gt;&lt;p&gt;&lt;code&gt;文本&lt;/code&gt;–&amp;gt;&lt;code&gt;分词&lt;/code&gt;–&amp;gt;&lt;code&gt;向量
      
    
    </summary>
    
      <category term="ML" scheme="http://nextnight.github.io/categories/ML/"/>
    
      <category term="NLP" scheme="http://nextnight.github.io/categories/ML/NLP/"/>
    
    
      <category term="分词" scheme="http://nextnight.github.io/tags/%E5%88%86%E8%AF%8D/"/>
    
      <category term="向量化" scheme="http://nextnight.github.io/tags/%E5%90%91%E9%87%8F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>mac jupyter中文显示设置</title>
    <link href="http://nextnight.github.io/2018/09/13/Mac-jupyter%E4%B8%AD%E6%96%87%E6%98%BE%E7%A4%BA%E8%AE%BE%E7%BD%AE/"/>
    <id>http://nextnight.github.io/2018/09/13/Mac-jupyter中文显示设置/</id>
    <published>2018-09-13T15:08:34.000Z</published>
    <updated>2018-09-29T07:12:29.467Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MAC-Jupyter使用matplotlib时设置中文"><a href="#MAC-Jupyter使用matplotlib时设置中文" class="headerlink" title="MAC-Jupyter使用matplotlib时设置中文"></a>MAC-Jupyter使用matplotlib时设置中文</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.font_manager <span class="keyword">import</span> _rebuild</span><br><span class="line">_rebuild()</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置中文显示</span></span><br><span class="line"></span><br><span class="line">mpl.rcParams[<span class="string">'font.family'</span>]=[<span class="string">'Microsoft Yahei'</span>]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;MAC-Jupyter使用matplotlib时设置中文&quot;&gt;&lt;a href=&quot;#MAC-Jupyter使用matplotlib时设置中文&quot; class=&quot;headerlink&quot; title=&quot;MAC-Jupyter使用matplotlib时设置中文&quot;&gt;&lt;/a&gt;MA
      
    
    </summary>
    
      <category term="Python" scheme="http://nextnight.github.io/categories/Python/"/>
    
    
      <category term="Jupyter" scheme="http://nextnight.github.io/tags/Jupyter/"/>
    
      <category term="matplotlib" scheme="http://nextnight.github.io/tags/matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>Pandas基本操作</title>
    <link href="http://nextnight.github.io/2018/09/13/Pandas%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    <id>http://nextnight.github.io/2018/09/13/Pandas基本操作/</id>
    <published>2018-09-13T14:58:52.000Z</published>
    <updated>2018-09-19T01:51:49.094Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Pandas-操作分类"><a href="#Pandas-操作分类" class="headerlink" title="Pandas 操作分类"></a>Pandas 操作分类</h1><ul><li>基本设置</li><li>数据描述</li><li>数据清洗</li><li>数据索引</li><li>数据连接</li><li>分组聚合</li><li>文本操作</li></ul><h1 id="基本设置"><a href="#基本设置" class="headerlink" title="基本设置"></a>基本设置</h1><p>① ：设置不使用科学计数法，保留5位小数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.set_option(<span class="string">'display.float_format'</span>, <span class="keyword">lambda</span> x: <span class="string">'%.5f'</span> % x)</span><br></pre></td></tr></table></figure></p><p>② : 设置显示结果的宽度(不截断换行)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.set_option(<span class="string">'display.width'</span>, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure></p><h1 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pd.read_csv()</span><br><span class="line">pd.read_table()</span><br><span class="line">pd.read_excle()</span><br><span class="line">pd.read_html()</span><br><span class="line">pd.read_json()</span><br><span class="line">pd.read_sql()</span><br></pre></td></tr></table></figure><h1 id="数据描述"><a href="#数据描述" class="headerlink" title="数据描述"></a>数据描述</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">"path"</span>)</span><br></pre></td></tr></table></figure><p>①：查看数据属性<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看属性</span></span><br><span class="line">df.shape</span><br><span class="line">df.index</span><br><span class="line">df.columns</span><br><span class="line">df.size</span><br><span class="line"></span><br><span class="line"><span class="comment"># 去除格式，获取数据数组</span></span><br><span class="line">df.values</span><br></pre></td></tr></table></figure></p><p>②：查看数据信息<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据信息</span></span><br><span class="line">df.head(n) <span class="comment"># 查看前n行</span></span><br><span class="line">df.tail(n) <span class="comment"># 查看尾n行</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看数据信息</span></span><br><span class="line">df.info()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看数据描述：</span></span><br><span class="line">df.describe()            <span class="comment"># 所有列描述</span></span><br><span class="line">df[<span class="string">'col_name'</span>].describe()<span class="comment"># 某一列描述</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计某列每个值的量:默认会从大到小排序</span></span><br><span class="line">df[<span class="string">'clo_name'</span>].value_counts(sort=<span class="keyword">True</span>,ascending=<span class="keyword">False</span>,bins=<span class="keyword">None</span>,dropna=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure></p><h1 id="数据索引"><a href="#数据索引" class="headerlink" title="数据索引"></a>数据索引</h1><h3 id="索引设置"><a href="#索引设置" class="headerlink" title="索引设置"></a>索引设置</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""index,索引操作</span></span><br><span class="line"><span class="string">  1、set_index()</span></span><br><span class="line"><span class="string">  2、reset_index()</span></span><br><span class="line"><span class="string">  3、rename()</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 设置索引</span></span><br><span class="line">df.set_index(<span class="string">'A'</span>) <span class="comment"># 设置列A为索引列</span></span><br><span class="line"><span class="comment"># 重置索引:恢复默认的自增索引</span></span><br><span class="line">df.reset_index(inplace=<span class="keyword">True</span>) <span class="comment"># 设置inplace的时候会修改原始df,并且没有返回值</span></span><br><span class="line">df = df.reset_index()        <span class="comment"># 不设置inplace=True需接受返回值，且原始df不变</span></span><br><span class="line">df.rename(index=<span class="keyword">lambda</span> x:x+<span class="number">1</span>)<span class="comment"># 所有索引值+1</span></span><br><span class="line"><span class="string">"""修改列名</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">df.rename(&#123;<span class="string">"A"</span>:<span class="string">"AAAAA"</span>,<span class="string">"B"</span>:<span class="string">"BBBB"</span>&#125;,inplace=<span class="keyword">True</span>) <span class="comment"># 把A修改成AAAAA</span></span><br><span class="line">df.rename(columns=<span class="keyword">lambda</span> x: x + <span class="number">2</span>) <span class="comment"># 将全体列重命名</span></span><br></pre></td></tr></table></figure><h3 id="索引数据"><a href="#索引数据" class="headerlink" title="索引数据"></a>索引数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""直接列索引：</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">df[<span class="number">1</span>] <span class="comment"># 第一列</span></span><br><span class="line">df[<span class="string">'Q'</span>] <span class="comment">#Q列</span></span><br><span class="line"></span><br><span class="line"><span class="string">""""行索引：</span></span><br><span class="line"><span class="string">  1、iloc</span></span><br><span class="line"><span class="string">  2、loc</span></span><br><span class="line"><span class="string">  3、ix</span></span><br><span class="line"><span class="string">"""</span><span class="string">"</span></span><br><span class="line"><span class="string">df.loc[df['Q']=0]  # 索引满足Q=0的虽偶有行数据</span></span><br><span class="line"><span class="string">df.loc[(df['Q'] &amp; df['T']=1)]</span></span><br><span class="line"><span class="string">df.loc[(df['Q'] &amp; df['T']=1) 'x'] = -1 # 满足条件的所有行数据给X 列赋值为-1</span></span><br></pre></td></tr></table></figure><h1 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h1><h3 id="数据缺失处理"><a href="#数据缺失处理" class="headerlink" title="数据缺失处理"></a>数据缺失处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""判断缺失:</span></span><br><span class="line"><span class="string">  1、isnull(),</span></span><br><span class="line"><span class="string">  2、isna()</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">df.isnull() <span class="comment"># 返回每个数据是否是null</span></span><br><span class="line">df.isna()   <span class="comment"># 返回每个数据是否是na</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""缺失填充:</span></span><br><span class="line"><span class="string">  1、fillna(),</span></span><br><span class="line"><span class="string">  2、ffill(),</span></span><br><span class="line"><span class="string">  3、bfill()</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">df.ffill()</span><br><span class="line">df.bfill()</span><br><span class="line">df.fillna(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">"""缺失删除:dropna()</span></span><br><span class="line"><span class="string">param:</span></span><br><span class="line"><span class="string">  axis:1,0 axis=1 删除含有na的列,axis=0 删除含有na的行,</span></span><br><span class="line"><span class="string">  how:any,all how='any'表示某列所有行存在为null即删除,how='all'，表示某行所有列都为null才删除</span></span><br><span class="line"><span class="string">  subset:选择子集</span></span><br><span class="line"><span class="string">  inplace:是否覆盖原数据</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">df = df.dropna(self, axis=<span class="number">0</span>, how=<span class="string">'any'</span>, thresh=<span class="keyword">None</span>, subset=<span class="keyword">None</span>,inplace=<span class="keyword">False</span>)</span><br><span class="line">df.dropna(self, axis=<span class="number">1</span>, how=<span class="string">'all'</span>, thresh=<span class="keyword">None</span>, subset=[<span class="string">'col1'</span>,<span class="string">'col2'</span>],inplace=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h3 id="数据异常处理"><a href="#数据异常处理" class="headerlink" title="数据异常处理"></a>数据异常处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""数据删除:drop()</span></span><br><span class="line"><span class="string">param:</span></span><br><span class="line"><span class="string">  axis:0,1 分别对应indexs,columns</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">df.drop(self, labels=<span class="keyword">None</span>, axis=<span class="number">0</span>, index=<span class="keyword">None</span>, columns=<span class="keyword">None</span>, level=<span class="keyword">None</span>,</span><br><span class="line">             inplace=<span class="keyword">False</span>, errors=<span class="string">'raise'</span>)</span><br><span class="line"></span><br><span class="line">df.drop([<span class="string">'a'</span>,<span class="string">'b'</span>],axis=<span class="number">1</span>,inplace=<span class="keyword">True</span>) <span class="comment"># 删除a，b列</span></span><br><span class="line">df.drop([<span class="number">0</span>,<span class="number">1</span>],axis=<span class="number">0</span>,inplace=<span class="keyword">True</span>) <span class="comment"># 删除0，1行</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""数据删除：del</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">del</span> df[<span class="string">'A'</span>]  <span class="comment"># 删除A列</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""数值替换:replace()</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">df.replace([<span class="number">1</span>,<span class="number">3</span>],[<span class="string">'one'</span>,<span class="string">'three'</span>]) <span class="comment"># 1替换成one,3替换成three</span></span><br><span class="line">df.rename(&#123;<span class="number">1</span>:<span class="string">'one'</span>,<span class="number">3</span>:<span class="string">'three'</span>&#125;)</span><br></pre></td></tr></table></figure><h3 id="数据过滤-选择"><a href="#数据过滤-选择" class="headerlink" title="数据过滤,选择"></a>数据过滤,选择</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df[(df[<span class="string">'age'</span>]&gt;<span class="number">10</span>) &amp; (df[<span class="string">'age'</span>]&lt;<span class="number">40</span>) ]</span><br><span class="line">df.loc[(df[<span class="string">'age'</span>]&gt;<span class="number">10</span>) &amp; (df[<span class="string">'age'</span>]&lt;<span class="number">40</span>) ]</span><br><span class="line">df[df[<span class="string">'age'</span>]==<span class="number">20</span>]</span><br><span class="line"></span><br><span class="line">df[df[<span class="string">'age'</span>].isin([<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>]) ]</span><br></pre></td></tr></table></figure><h3 id="数据排序"><a href="#数据排序" class="headerlink" title="数据排序"></a>数据排序</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""排序：sort_values(by='column',inplace=None,acsending=True)  </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">df.sort_values(by=<span class="string">''</span>,inplace=<span class="keyword">True</span>,acsending=<span class="keyword">False</span>) <span class="comment"># acsending=False 从大到小</span></span><br></pre></td></tr></table></figure><h3 id="数据编码"><a href="#数据编码" class="headerlink" title="数据编码"></a>数据编码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""one_hot:get_dummies()</span></span><br><span class="line"><span class="string">"""</span><span class="string">"</span></span><br><span class="line"><span class="string">dt_one_hot = pd.get_dummies(dt[['A','B']]) # 对A，B 列进行onehot编码</span></span><br></pre></td></tr></table></figure><h3 id="数据分箱"><a href="#数据分箱" class="headerlink" title="数据分箱"></a>数据分箱</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""df.cut()</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">df_col1 = pd.cut(df[<span class="string">'column1'</span>], bins=[<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">0.8</span>, <span class="number">2</span>, <span class="number">20</span>, <span class="number">1000</span>], labels=np.arange(<span class="number">1</span>, <span class="number">6</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Pandas-操作分类&quot;&gt;&lt;a href=&quot;#Pandas-操作分类&quot; class=&quot;headerlink&quot; title=&quot;Pandas 操作分类&quot;&gt;&lt;/a&gt;Pandas 操作分类&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;基本设置&lt;/li&gt;
&lt;li&gt;数据描述&lt;/li&gt;
&lt;li&gt;
      
    
    </summary>
    
      <category term="Python" scheme="http://nextnight.github.io/categories/Python/"/>
    
      <category term="Pandas" scheme="http://nextnight.github.io/categories/Python/Pandas/"/>
    
    
      <category term="Python" scheme="http://nextnight.github.io/tags/Python/"/>
    
      <category term="Pandas" scheme="http://nextnight.github.io/tags/Pandas/"/>
    
  </entry>
  
  <entry>
    <title>Numpy基本操作</title>
    <link href="http://nextnight.github.io/2018/09/10/Numpy%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    <id>http://nextnight.github.io/2018/09/10/Numpy基本操作/</id>
    <published>2018-09-10T14:45:39.000Z</published>
    <updated>2018-09-15T02:25:03.415Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="string">"""numpy的基本应用</span></span><br><span class="line"><span class="string">1、四个属性</span></span><br><span class="line"><span class="string">    ndim:获取数据的维度数</span></span><br><span class="line"><span class="string">    shape:收取数据的维度</span></span><br><span class="line"><span class="string">    dtype:获取数据的类型</span></span><br><span class="line"><span class="string">    size:</span></span><br><span class="line"><span class="string">2、N个方法</span></span><br><span class="line"><span class="string">    1、zeros,ones,empty,arange</span></span><br><span class="line"><span class="string">    2、max,min,std,mean,sum,var,median,cumsum</span></span><br><span class="line"><span class="string">    3、astype,reshape</span></span><br><span class="line"><span class="string">    4、argmax,argmin,argsort</span></span><br><span class="line"><span class="string">    5、all,any,fill,where，diff</span></span><br><span class="line"><span class="string">    6、vstack,hstack</span></span><br><span class="line"><span class="string">    7、unique</span></span><br><span class="line"><span class="string">    8、load,save:操作文件</span></span><br><span class="line"><span class="string">    9、insert,arange,reshape</span></span><br><span class="line"><span class="string">    10、bincount</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">numpy_filed</span><span class="params">()</span>:</span></span><br><span class="line">    ll = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">                   [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">7</span>]])</span><br><span class="line">    print(ll)</span><br><span class="line">    print(ll.ndim)  <span class="comment"># 2；表示2个维度</span></span><br><span class="line">    print(ll.shape)  <span class="comment"># (5,):表示五个元素</span></span><br><span class="line">    print(ll.dtype)  <span class="comment"># 元素类型int64:表示数据类型</span></span><br><span class="line">    print(ll.size)  <span class="comment"># 10：元素个数</span></span><br><span class="line">    print(type(ll))  <span class="comment"># 对象类型&lt;class 'numpy.ndarray'&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">numpy_method</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    max,min,std,mean,sum,var,median,cumsum</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    astype:转换数据类型</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    argmax,argmin,argsort</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    all,any,fill,where</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    vstack,hstack</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    load,save:操作文件</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    insert,arange,reshape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    ll = np.array([[<span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">                   [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">7</span>]])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># max,min,std,mean,sum,var,median,cumsum</span></span><br><span class="line">    print(ll.max())        <span class="comment"># 全局最大值</span></span><br><span class="line">    print(ll.max(axis=<span class="number">1</span>))  <span class="comment"># axis=1：行最大值 [6 7]</span></span><br><span class="line">    print(ll.max(axis=<span class="number">0</span>))  <span class="comment"># axis=0 列最大 [2 4 4 5 7]</span></span><br><span class="line">    print(ll.cumsum())     <span class="comment"># 前所有项的和组成的序列</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># astype: 转换数据类型</span></span><br><span class="line">    ll = ll.astype(dtype=np.float64)</span><br><span class="line">    print(ll.dtype)        <span class="comment"># float64</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># argmax, argmin, argsort</span></span><br><span class="line">    print(ll.argmax(axis=<span class="number">1</span>)) <span class="comment"># axis=1 返回每行的最大数值的索引[4,4]</span></span><br><span class="line">    print(ll.argmin(axis=<span class="number">0</span>)) <span class="comment"># axis=0 返回每列的最小值的索引 [1 1 1 0 1]</span></span><br><span class="line">    print(ll.argsort(axis=<span class="number">0</span>))<span class="comment"># 按行排序返回排序后的索引，未指定排序的列，就是返回所有的列排序的结果</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># all,any,fill,where，diff</span></span><br><span class="line">    <span class="comment"># all()全部满足条件，any()存在满足条件的</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># vstack,hstack</span></span><br><span class="line">    p1 = np.zeros((<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">    p2 = np.ones((<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">    print(<span class="string">"纵向叠加：\n"</span>,np.vstack((p1,p2))) <span class="comment"># 数据拼接，维度不变</span></span><br><span class="line">    print(<span class="string">"横向叠加：\n"</span>,np.hstack((p1,p2))) <span class="comment"># 数据连接，行数不变</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># unique：找出唯一值并返回排序后的结果</span></span><br><span class="line">    print(np.unique(ll))      <span class="comment"># [1. 2. 3. 4. 5. 6. 7.]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    numpy_filed()</span><br><span class="line">    numpy_method()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas
      
    
    </summary>
    
      <category term="Python" scheme="http://nextnight.github.io/categories/Python/"/>
    
      <category term="Numpy" scheme="http://nextnight.github.io/categories/Python/Numpy/"/>
    
    
      <category term="Python" scheme="http://nextnight.github.io/tags/Python/"/>
    
      <category term="Numpy" scheme="http://nextnight.github.io/tags/Numpy/"/>
    
  </entry>
  
  <entry>
    <title>Python代码片段</title>
    <link href="http://nextnight.github.io/2018/09/10/Python%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5/"/>
    <id>http://nextnight.github.io/2018/09/10/Python代码片段/</id>
    <published>2018-09-10T14:03:51.000Z</published>
    <updated>2018-09-29T07:25:46.051Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、多变量赋值"><a href="#1、多变量赋值" class="headerlink" title="1、多变量赋值"></a>1、多变量赋值</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a, b, c, d = <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span></span><br></pre></td></tr></table></figure><h1 id="2、列表赋值"><a href="#2、列表赋值" class="headerlink" title="2、列表赋值"></a>2、列表赋值</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">names = [<span class="string">"ami"</span>, <span class="string">"kimi"</span>, <span class="string">"jsm"</span>]</span><br><span class="line">a, b, c = names</span><br></pre></td></tr></table></figure><h1 id="3、条件表达式"><a href="#3、条件表达式" class="headerlink" title="3、条件表达式"></a>3、条件表达式</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="number">8</span></span><br><span class="line">d = x <span class="keyword">if</span> x &gt; <span class="number">5</span> <span class="keyword">else</span> <span class="number">10</span></span><br></pre></td></tr></table></figure><h1 id="4、列表推导式"><a href="#4、列表推导式" class="headerlink" title="4、列表推导式"></a>4、列表推导式</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>) <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">0</span>]</span><br></pre></td></tr></table></figure><h1 id="5、条件判断-不是用and"><a href="#5、条件判断-不是用and" class="headerlink" title="5、条件判断:不是用and"></a>5、条件判断:不是用and</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="number">90</span></span><br><span class="line"><span class="keyword">if</span> <span class="number">80</span> &lt; x &lt; <span class="number">100</span>: print(x)</span><br></pre></td></tr></table></figure><h1 id="6、判断是否在-不在某列表-字符串"><a href="#6、判断是否在-不在某列表-字符串" class="headerlink" title="6、判断是否在/不在某列表,字符串"></a>6、判断是否在/不在某列表,字符串</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="number">1</span> <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]: print(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="number">1</span> <span class="keyword">not</span> <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]: print(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="string">'1'</span> <span class="keyword">in</span> <span class="string">"123"</span>: print(<span class="number">2</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="string">'1'</span> <span class="keyword">not</span> <span class="keyword">in</span> <span class="string">"123"</span>: print(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h1 id="7、隐含类型转换判空"><a href="#7、隐含类型转换判空" class="headerlink" title="7、隐含类型转换判空"></a>7、隐含类型转换判空</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a, b, c, d = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], &#123;&#125;, <span class="string">''</span>, []</span><br><span class="line"><span class="keyword">if</span> a:</span><br><span class="line">    print(<span class="string">"a not empty"</span>)</span><br><span class="line"><span class="keyword">if</span> b:</span><br><span class="line">    print(<span class="string">"b not empty"</span>)</span><br><span class="line"><span class="keyword">if</span> c:</span><br><span class="line">    print(<span class="string">"c not empty"</span>)</span><br><span class="line"><span class="keyword">if</span> d:</span><br><span class="line">    print(<span class="string">"d not empty"</span>)</span><br></pre></td></tr></table></figure><h1 id="8、判断多个条件是否成立-any，all"><a href="#8、判断多个条件是否成立-any，all" class="headerlink" title="8、判断多个条件是否成立:any，all"></a>8、判断多个条件是否成立:any，all</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a, b, c = <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span></span><br><span class="line"><span class="keyword">if</span> any([a &gt; <span class="number">1</span>, b &lt; <span class="number">2</span>, c == <span class="number">3</span>]): <span class="keyword">pass</span>  <span class="comment"># === a&gt;1 or b&lt;2 or c==3</span></span><br><span class="line"><span class="keyword">if</span> all([a &gt; <span class="number">1</span>, b &lt; <span class="number">2</span>, c == <span class="number">3</span>]): <span class="keyword">pass</span>  <span class="comment"># === a&gt;1 and b&lt;2 and c==3</span></span><br></pre></td></tr></table></figure><h1 id="9、列表推导式-过滤"><a href="#9、列表推导式-过滤" class="headerlink" title="9、列表推导式+过滤"></a>9、列表推导式+过滤</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ls = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="string">"a"</span>, <span class="number">4</span>, <span class="string">"v"</span>, <span class="number">5.5</span>]</span><br><span class="line">rs = [i <span class="keyword">for</span> i <span class="keyword">in</span> ls <span class="keyword">if</span> type(i) <span class="keyword">in</span> [int, float]]</span><br><span class="line">print(rs)</span><br></pre></td></tr></table></figure><h1 id="10、同时获取下标和数据：enumerate"><a href="#10、同时获取下标和数据：enumerate" class="headerlink" title="10、同时获取下标和数据：enumerate"></a>10、同时获取下标和数据：enumerate</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nums = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line"><span class="keyword">for</span> index, num <span class="keyword">in</span> enumerate(nums):</span><br><span class="line">    print(<span class="string">"索引为&#123;&#125;的数据是&#123;&#125;"</span>.format(index, num))</span><br></pre></td></tr></table></figure><h1 id="11、线程sleep"><a href="#11、线程sleep" class="headerlink" title="11、线程sleep"></a>11、线程sleep</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">time.sleep(<span class="number">1</span>)  <span class="comment"># 休眠1秒</span></span><br></pre></td></tr></table></figure><h1 id="12、print-输出覆盖"><a href="#12、print-输出覆盖" class="headerlink" title="12、print 输出覆盖"></a>12、print 输出覆盖</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">i, n = <span class="number">0</span>, <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    time.sleep(<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>: print(i + <span class="number">1</span>, end = <span class="string">'\r'</span>)</span><br></pre></td></tr></table></figure><h1 id="13、lambda匿名函数"><a href="#13、lambda匿名函数" class="headerlink" title="13、lambda匿名函数"></a>13、lambda匿名函数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">names = [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'xxx'</span>, <span class="string">'vx'</span>, <span class="string">'ccc'</span>]</span><br><span class="line">rs = filter(<span class="keyword">lambda</span> x:len(x) &lt;= <span class="number">1</span>, names)</span><br><span class="line">print(list(rs))  <span class="comment"># ['a', 'b']</span></span><br></pre></td></tr></table></figure><h1 id="14、yield生成器收集系列值，不需要return"><a href="#14、yield生成器收集系列值，不需要return" class="headerlink" title="14、yield生成器收集系列值，不需要return"></a>14、yield生成器收集系列值，不需要return</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span><span class="params">()</span>:</span></span><br><span class="line">    a = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        a += i</span><br><span class="line">        <span class="keyword">yield</span> a</span><br><span class="line"><span class="comment"># [0, 1, 3, 6, 10, 15, 21, 28, 36, 45]</span></span><br><span class="line">print(list(fun()))</span><br></pre></td></tr></table></figure><h1 id="15、装饰器给函数添加插入日志，性能测试等非核心功能"><a href="#15、装饰器给函数添加插入日志，性能测试等非核心功能" class="headerlink" title="15、装饰器给函数添加插入日志，性能测试等非核心功能"></a>15、装饰器给函数添加插入日志，性能测试等非核心功能</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runtime</span><span class="params">(func)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">          start = time.time()</span><br><span class="line">          result = func(*args, **kwargs)</span><br><span class="line">          end = time.time()</span><br><span class="line">          print(<span class="string">"&#123;&#125; is called,used &#123;&#125;s."</span>.format(func.__name__, start - end))</span><br><span class="line">          <span class="keyword">return</span> result</span><br><span class="line">    <span class="keyword">return</span> (wrapper)</span><br><span class="line"></span><br><span class="line"><span class="meta">@runtime</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">()</span>:</span></span><br><span class="line">    s = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">          time.sleep(<span class="number">1</span>)</span><br><span class="line">          s += i</span><br><span class="line">process()</span><br></pre></td></tr></table></figure><h1 id="16、内存拷贝"><a href="#16、内存拷贝" class="headerlink" title="16、内存拷贝"></a>16、内存拷贝</h1><p>使用：<code>copy</code>包中的<code>copy()函数</code>和<code>deepcopy()</code>函数</p><blockquote><p>16.1：赋值：指向同一块地址</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = &#123;<span class="number">1</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]&#125;</span><br><span class="line">print(<span class="string">'a的内存地址 %s'</span> % id(a))      <span class="comment"># 4386109840</span></span><br><span class="line">print(<span class="string">'a1的内存地址 %s'</span> % id(a[<span class="number">1</span>]))  <span class="comment"># 4391711560</span></span><br><span class="line">b = a</span><br></pre></td></tr></table></figure><blockquote><p>16.2：浅拷贝：指向不同的引用，但是不同引用指向相同内容(只拷贝对象，但不拷贝对象内部的对象)</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">b = copy.copy(a)</span><br><span class="line">print(<span class="string">'a的内存地址 %s'</span> % id(a))      <span class="comment"># 4386109840</span></span><br><span class="line">print(<span class="string">'b的内存地址 %s'</span> % id(b))      <span class="comment"># 4386110200</span></span><br><span class="line">print(<span class="string">'a1的内存地址 %s'</span> % id(a[<span class="number">1</span>]))  <span class="comment"># 4391711560</span></span><br><span class="line">print(<span class="string">'b1的内存地址 %s'</span> % id(b[<span class="number">1</span>]))  <span class="comment"># 4391711560</span></span><br></pre></td></tr></table></figure><blockquote><p>16.3：深拷贝：对象及对象内部的对象都复制一份</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">b = copy.deepcopy(a)</span><br><span class="line">print(<span class="string">'a的内存地址 %s'</span> % id(a))      <span class="comment"># 4386109840</span></span><br><span class="line">print(<span class="string">'b的内存地址 %s'</span> % id(b))      <span class="comment"># 4391729264</span></span><br><span class="line">print(<span class="string">'a1的内存地址 %s'</span> % id(a[<span class="number">1</span>]))  <span class="comment"># 4391711560</span></span><br><span class="line">print(<span class="string">'b1的内存地址 %s'</span> % id(b[<span class="number">1</span>]))  <span class="comment"># 4391711368</span></span><br></pre></td></tr></table></figure><h2 id="17、参数传递"><a href="#17、参数传递" class="headerlink" title="17、参数传递"></a>17、参数传递</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1、多变量赋值&quot;&gt;&lt;a href=&quot;#1、多变量赋值&quot; class=&quot;headerlink&quot; title=&quot;1、多变量赋值&quot;&gt;&lt;/a&gt;1、多变量赋值&lt;/h1&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class
      
    
    </summary>
    
      <category term="Python" scheme="http://nextnight.github.io/categories/Python/"/>
    
    
      <category term="Python" scheme="http://nextnight.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>数据结构与算法</title>
    <link href="http://nextnight.github.io/2018/08/14/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    <id>http://nextnight.github.io/2018/08/14/数据结构与算法/</id>
    <published>2018-08-14T03:57:08.000Z</published>
    <updated>2018-08-27T06:56:00.076Z</updated>
    
    <content type="html"><![CDATA[<p>是时候沉下心来学习一波数据结构与算法了，好的算法能够轻松的而解决各种难题，而好的数据结构是实现算法的前提，算法设计依托与不同的数据结构，同样，算法也是解决不同形式的数据问题。计划步骤：</p><ul><li>1、深入操作基本数据结构</li><li>2、深入回顾基础算法</li><li>3、深入理解基本算法思想</li><li>4、算法刷题，书籍阅读</li></ul><h1 id="算法思想"><a href="#算法思想" class="headerlink" title="算法思想"></a>算法思想</h1><ul><li><p>递归</p></li><li><p>穷举</p></li><li><p>递推</p></li><li><p>贪心</p></li><li><p>回溯</p></li><li><p>分治</p></li><li><p>动态规划</p></li></ul><h1 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h1><ul><li><p>数组</p></li><li><p>链表</p></li><li><p>栈</p></li><li><p>队列</p></li><li><p>字符串</p></li><li><p>🌲树</p></li><li><p>trie树</p></li><li><p>哈希</p></li><li><p>图</p></li></ul><h1 id="基础算法"><a href="#基础算法" class="headerlink" title="基础算法"></a>基础算法</h1><h4 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h4><ul><li><p>交换排序</p><ul><li><p>选择排序</p></li><li><p>插入排序</p></li><li><p>冒泡排序</p></li><li><p>快速排序</p></li><li><p>堆排序</p></li><li><p>希尔排序</p></li><li><p>归并排序</p></li></ul></li><li><p>线性排序</p><ul><li>桶排序</li></ul></li></ul><h4 id="查找"><a href="#查找" class="headerlink" title="查找"></a>查找</h4><ul><li><p>顺序查找</p></li><li><p>二分查找</p></li><li><p>分块查找</p></li><li><p>动态查找</p><ul><li><p>二叉排序树</p></li><li><p>平衡二叉树</p></li><li><p>B树，B+树</p></li></ul></li><li><p>Hash查找</p></li></ul><h1 id="经典算法"><a href="#经典算法" class="headerlink" title="经典算法"></a>经典算法</h1><ul><li><p>快速排序</p></li><li><p>BFS/DFS</p></li><li><p>KMP</p></li><li><p>A*寻路</p></li><li><p>Dijkstra</p></li><li><p>遗传算法</p></li><li><p>动态规划</p></li></ul><h1 id="海量数据处理"><a href="#海量数据处理" class="headerlink" title="海量数据处理"></a>海量数据处理</h1><ul><li><p>Hash</p></li><li><p>Bitmap</p></li><li><p>Bloom filter</p></li><li><p>Trie树</p></li><li><p>Index</p></li><li><p>Inverted Index</p></li><li><p>simhash</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;是时候沉下心来学习一波数据结构与算法了，好的算法能够轻松的而解决各种难题，而好的数据结构是实现算法的前提，算法设计依托与不同的数据结构，同样，算法也是解决不同形式的数据问题。计划步骤：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1、深入操作基本数据结构&lt;/li&gt;
&lt;li&gt;2、深入回顾基础
      
    
    </summary>
    
      <category term="算法" scheme="http://nextnight.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="数据结构" scheme="http://nextnight.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
      <category term="算法" scheme="http://nextnight.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
