<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Kala</title>
  
  <subtitle>Do Best Kala</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://nextnight.github.io/"/>
  <updated>2018-09-30T10:04:23.249Z</updated>
  <id>http://nextnight.github.io/</id>
  
  <author>
    <name>Best Kala</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Match-文本数据操作</title>
    <link href="http://nextnight.github.io/2018/09/30/Match-%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C/"/>
    <id>http://nextnight.github.io/2018/09/30/Match-文本数据操作/</id>
    <published>2018-09-30T03:58:57.000Z</published>
    <updated>2018-09-30T10:04:23.249Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>这里主要是记录文本相关的比赛中积累的代码片段和操作，量变产生质变，每一次探索和积累都是一小步的成长，不求走的多快，多远，我一直在行走，在努力。</p></blockquote><p>文本数据的处理主要是<code>分词</code>，<code>去停用词</code>(停用词中包括各种特殊符号以及对于构造向量无关的词)，<code>向量化</code>（词袋模型，词嵌入），词嵌入涉及到的词向量的训练。得到向量化的数据之后基本就同结构化数据一样操作了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入数据，方便后续操作,数据其中一列为sentence,即文本内容</span></span><br><span class="line">train = pd.read_csv(<span class="string">"data/train.csv"</span>,index=<span class="keyword">None</span>, encoding=<span class="string">'utf-8'</span>)</span><br></pre></td></tr></table></figure><h2 id="1、数据查看"><a href="#1、数据查看" class="headerlink" title="1、数据查看"></a>1、数据查看</h2><blockquote><p>文本数据一般会查看数据的长度，确定是长文本还是短文本，我们还可以分析文本对于固定模式的首尾相同的数据进行首尾数据额删除，不过至于删除多少需要测试，首尾多长的文本会是相同的内容，可以采用编辑距离计算一个平均距离，选取不同的长度分别得到平均距离，可以最终选择一个平均距离最短对应的长度作为我们要删除的文本长度。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_date</span><span class="params">()</span>：</span></span><br><span class="line"><span class="function">  # 文本为字符串</span></span><br><span class="line">  train['len'] = train['sentence'].apply(lambda x:len(x))</span><br><span class="line">  train[<span class="string">'len'</span>].describe()</span><br><span class="line">  <span class="comment"># 文本已经切分</span></span><br><span class="line">  train[<span class="string">'len'</span>] = train[<span class="string">'sentence'</span>].apply(<span class="keyword">lambda</span> x:len(x.split(<span class="string">' '</span>)))</span><br><span class="line">  train[<span class="string">'len'</span>].describe()</span><br><span class="line">  <span class="comment"># 文章长度分箱</span></span><br><span class="line">  bins = [<span class="number">0</span>, <span class="number">100</span>, <span class="number">500</span>, <span class="number">1000</span>, <span class="number">2000</span>, <span class="number">5000</span>, <span class="number">100000</span>]</span><br><span class="line">  cats = pd.cut(train[<span class="string">'len'</span>], bins=bins,labels=<span class="keyword">None</span>)</span><br><span class="line">  print(pd.value_counts(cats))</span><br></pre></td></tr></table></figure><h2 id="2、加载停用词"><a href="#2、加载停用词" class="headerlink" title="2、加载停用词"></a>2、加载停用词</h2><blockquote><p>文本处理的基本操作，这里可以直接将得到的停用词列表作为全局变量，否则对每条数据分词的时候都需要读取一遍。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stop_words_list</span><span class="params">(path)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    @ func 读取停用词列表</span></span><br><span class="line"><span class="string">    @ param path:停用词路径</span></span><br><span class="line"><span class="string">    停用词为每一行一个词存储</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    stop_words = [line.strip() <span class="keyword">for</span> line <span class="keyword">in</span> open(path, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>).readlines()]</span><br><span class="line">    <span class="keyword">return</span> stop_words</span><br></pre></td></tr></table></figure><h2 id="3、分词去停用词"><a href="#3、分词去停用词" class="headerlink" title="3、分词去停用词"></a>3、分词去停用词</h2><blockquote><p>遍历所有数据，对每一条进行如下的分词去停用词操作。<code>df[&#39;senctence&#39;].apply(lambda x: seg_sentence_all(x))</code></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">seg_sentence_all</span><span class="params">(sentence)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    对句子进行分词,去停用词</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    path = <span class="string">"data/stop_word_all.txt"</span></span><br><span class="line">    sentence_seged = jieba.cut(sentence.strip())</span><br><span class="line">    stop_words = stop_words_list(path)  <span class="comment"># 这里加载停用词的路径</span></span><br><span class="line">    cut_s = <span class="string">''</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sentence_seged:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stop_words:</span><br><span class="line">            <span class="keyword">if</span> word != <span class="string">'\t'</span>:</span><br><span class="line">                str += word.strip()</span><br><span class="line">                str += <span class="string">' '</span></span><br><span class="line">    <span class="keyword">return</span> str.strip()</span><br></pre></td></tr></table></figure><h2 id="4、多类别共现词"><a href="#4、多类别共现词" class="headerlink" title="4、多类别共现词"></a>4、多类别共现词</h2><blockquote><p>在文本分类中，多个类别中频繁多次出现的词可能是停用词，也可能是标点符号，对于脱敏数据，我们无法查看到原始数据，无法确定停用词的时候可以使用如下方法来找出一部分贡献词作为停用词。</p><p>同样可以对所有分词后的数据做词的交集，更加精确粗暴。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_stop_word</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 为每个类构建词袋</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">'bowlist.csv'</span>):</span><br><span class="line">        column = <span class="string">'word_seg'</span></span><br><span class="line">        train = pd.read_csv(path_train, encoding=<span class="string">'utf-8'</span>, sep=<span class="string">','</span>)</span><br><span class="line">        bowlist = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="number">1</span>, <span class="number">20</span>, <span class="number">1</span>):</span><br><span class="line">            st = train[train[<span class="string">'class'</span>] == i]</span><br><span class="line">            cv = CountVectorizer(ngram_range=(<span class="number">1</span>, <span class="number">1</span>), lowercase=<span class="keyword">False</span>, max_df=<span class="number">0.9</span>, min_df=<span class="number">2</span>)</span><br><span class="line">            cv.fit(st[column].values)</span><br><span class="line">            vocal = cv.get_feature_names()</span><br><span class="line">            vst = set(vocal)</span><br><span class="line">            bowlist.extend(list(vst))</span><br><span class="line">        df = pd.DataFrame(bowlist)</span><br><span class="line">        df.to_csv(<span class="string">"bowlist.csv"</span>, index=<span class="keyword">False</span>)</span><br><span class="line">    <span class="comment"># 查看在多个类中出现的词</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        df = pd.read_csv(<span class="string">"bowlist.csv"</span>)</span><br><span class="line">        print(df)</span><br><span class="line">    dt = pd.DataFrame(df[<span class="string">'0'</span>].value_counts().reset_index())</span><br><span class="line">    dt.columns = [<span class="string">'word'</span>, <span class="string">'count'</span>]</span><br><span class="line">    <span class="comment"># 19个类别，如果在多个类别都出现则可作为停用词</span></span><br><span class="line">    dt[dt[<span class="string">'count'</span>] &gt; <span class="number">12</span>][<span class="string">'word'</span>].to_csv(<span class="string">'stop_word.csv'</span>, encoding=<span class="string">'utf-8'</span>, index=<span class="keyword">None</span>, header=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure><h2 id="5、Tf-Idf向量化"><a href="#5、Tf-Idf向量化" class="headerlink" title="5、Tf-Idf向量化"></a>5、Tf-Idf向量化</h2><blockquote><p>TF-Idf传统文本处理的常用向量化操作 【他表示的是一个词的重要程度】。TF:表示词频，一个词在一篇文档中出现的频率，越大表示越重要，越具备代表性。但是它同时在很多文档中都出现了，那么他就不具备代表性。所有重要性与频率正比，与出现的文档数成反比，因此使用逆文档频率Idf表示降低频率的影响。Idf=lg(N/n)N:表示文档总数，n表示出现该词的文档数。TF=count/len(doc),count:词在文档doc出现次数,len(doc):表示文档词数。</p></blockquote><h4 id="5-1、sklearn中的tf-idf"><a href="#5-1、sklearn中的tf-idf" class="headerlink" title="5.1、sklearn中的tf-idf"></a>5.1、sklearn中的tf-idf</h4><blockquote><p>重要的参数：</p><pre><code>- stop_words：可以传递一个停用词列表到这用于构建向量的时候去除停用词- ngram_range:也就是n-gram,N元词型合适的n-gram对结果有很大的帮助- lowercase：False,强转小写，默认True，但是对于中文会报错。</code></pre></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">TfidfVec</span><span class="params">(dt_train, dt_test)</span>:</span></span><br><span class="line">    Tfidf = TfidfVectorizer(min_df=<span class="number">2</span>, max_df=<span class="number">0.9</span>, </span><br><span class="line">                            use_idf=<span class="number">1</span>, smooth_idf=<span class="number">1</span>, </span><br><span class="line">                            sublinear_tf=<span class="number">1</span>, ngram_range=(<span class="number">1</span>, <span class="number">3</span>),</span><br><span class="line">                            stop_words=load_stopws(),lowercase=<span class="keyword">False</span>)</span><br><span class="line">    Tfidf.fit(dt_train[<span class="string">'article'</span>])</span><br><span class="line">    print(Tfidf.max_features)</span><br><span class="line">    train_word_vec = Tfidf.transform(dt_train[<span class="string">'article'</span>])</span><br><span class="line">    test_word_vec = Tfidf.transform(dt_test[<span class="string">'article'</span>])</span><br><span class="line">    <span class="keyword">return</span> train_word_vec, test_word_vec</span><br></pre></td></tr></table></figure><h4 id="5-2、Gensim中的tf-idf"><a href="#5-2、Gensim中的tf-idf" class="headerlink" title="5.2、Gensim中的tf-idf"></a>5.2、Gensim中的tf-idf</h4><blockquote><p>Gensim是一个非常常用的NLP库，里面包含各种在官方基础上封装的库翻遍我们使用，包括常用的Word2Vec,LDA…</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Gensim_Tfidf</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 生成语料</span></span><br><span class="line">    sentences = [<span class="string">"我喜欢北京三点的太阳"</span>, <span class="string">"番茄炒西红柿是我的最爱"</span>, <span class="string">"我不喜欢今天雾霾的西安"</span>]</span><br><span class="line">    words = []</span><br><span class="line">    <span class="keyword">for</span> doc <span class="keyword">in</span> sentences:</span><br><span class="line">        words.append(list(jieba.cut(doc)))</span><br><span class="line">    print(words)</span><br><span class="line">    <span class="comment"># 生成词典</span></span><br><span class="line">    dic = corpora.Dictionary(words)</span><br><span class="line">    <span class="comment"># 生成语料库(每一个文档生成(词,出现的次数))的列表=即一个词频向量的稀疏表示</span></span><br><span class="line">    cps = [dic.doc2bow(text) <span class="keyword">for</span> text <span class="keyword">in</span> words]</span><br><span class="line">         <span class="comment"># 训练tfidf模型</span></span><br><span class="line">    tfidf = TfidfModel(corpus=cps)</span><br><span class="line">    <span class="comment"># 转换词频向量为tfidf向量</span></span><br><span class="line">    cps_tfidf = tfidf[cps]</span><br><span class="line">        <span class="keyword">return</span> cps_tfidf</span><br></pre></td></tr></table></figure><h2 id="6、LSA向量化"><a href="#6、LSA向量化" class="headerlink" title="6、LSA向量化"></a>6、LSA向量化</h2><blockquote><p>LSA,也可以叫做LSI,浅语义模型，是一种基于TF或者TF-IDF的降维手段，同时也是描述词语之间的语义关系的语义模型，它认为词和文档的共现矩阵当中存在着词之间语义关系，他们具有一定的相似性，构建词-文档的共现矩阵(通常是词频矩阵或TF-IDF权重矩阵)在进行SVD矩阵分解，在这个稠密的低维空间中。任意两个行向量即两个词的向量求相似度可以表示语义关系。</p><p>Gensim中已经集成LSA的模型可以直接调用。直接将cps或者cps_tfidf喂给LsiModel就可以得到降维后的数据，可用与分类，聚类。做相似度计算。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_lsi</span><span class="params">(cps, dic)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    LSI：浅语义分析</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    print(<span class="string">"\n=================LSI================="</span>)</span><br><span class="line">    lsi = LsiModel(corpus=cps, num_topics=<span class="number">2</span>, id2word=dic)</span><br><span class="line">    <span class="comment"># 输出训练得到的主题</span></span><br><span class="line">    tp = lsi.print_topics(num_topics=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 查看每个文档的LSI主题分布</span></span><br><span class="line">    cps_lsi = lsi[cps]</span><br><span class="line">        <span class="keyword">return</span> cps_lsi</span><br></pre></td></tr></table></figure><h2 id="7、LDA向量化"><a href="#7、LDA向量化" class="headerlink" title="7、LDA向量化"></a>7、LDA向量化</h2><blockquote><p>LDA,又称主题模型，它认为在词和文档之间存在摸个隐变量 [主题] ，每个主题由多个词构成且每个词有一定的概率出现。而一篇文档可以由多个主题构成，且每个主题同样有一定的概率。那么如何得到一篇文档呢?我们知道这篇文档的主题分布，只需要以这个概率分布不断的去选择主题，再分别为每个主题按照主题-词的概率分布去选择词，迭代稳定后就可以的得到这篇文档。所以说我们可以将一篇用词表示的文档，用主题的概率分布来表示。表示之后的文档向量是一个稠密的低维向量，包含了某种语义联系，同时又进行了降维。</p><p>Gensim中同样进行了LDA的实现，同上，将cps或者cps_tfidf喂给LdaModel就可以得到降维后的数据。可用与分类，聚类，做相似度计算。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_lda</span><span class="params">(cps,dic)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    LDA:主题模型</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    print(<span class="string">"\n=================LDA================="</span>)</span><br><span class="line">    lda = LdaModel(corpus=cps, num_topics=<span class="number">2</span>, id2word=dic)</span><br><span class="line">    <span class="comment"># 输出五个主题</span></span><br><span class="line">    tp = lda.print_topics(num_topics=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出每个文档的主题</span></span><br><span class="line">    cps_lda = lda[cps]</span><br><span class="line">    <span class="keyword">return</span> cps_lda</span><br></pre></td></tr></table></figure><h2 id="8、Word2vec词向量"><a href="#8、Word2vec词向量" class="headerlink" title="8、Word2vec词向量"></a>8、Word2vec词向量</h2><blockquote><p>词向量表示是的是一个词的向量化表示，one-hot，tf,tf-idf都是一种词向量表示法，而word2vec也是一种词向量表示法，他通过训练一个词的上下文表示以一个词的概率这种深度学习分类模型来得到一组隐层权重来表示一个词，这个权重就是词向量它能够表示更多的语义信息和语序信息。</p><p>以下是Gensim包中的word2vec词向量训练，主要参数</p><ul><li><p>windows:上下文词的个数</p></li><li><p>size:需要训练的词向量的维度</p></li></ul><p>影响词向量的主要参数就是这两个，关于word2vec和其他参数，其他地方在做介绍。</p></blockquote><h4 id="8-1、Gensim-word2vec训练词向量"><a href="#8-1、Gensim-word2vec训练词向量" class="headerlink" title="8.1、Gensim.word2vec训练词向量"></a>8.1、Gensim.word2vec训练词向量</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models.word2vec <span class="keyword">import</span> Word2Vec</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word2vec_train</span><span class="params">()</span>:</span></span><br><span class="line">      <span class="string">"""训练词向量"""</span></span><br><span class="line">    train = pd.read_csv(path_train, encoding=<span class="string">'utf-8'</span>, sep=<span class="string">','</span>)</span><br><span class="line">        <span class="comment"># train['word_seg']表示分词后的列</span></span><br><span class="line">    model = Word2Vec(train[<span class="string">'word_seg'</span>].values, window=<span class="number">5</span>, sg=<span class="number">0</span>, size=<span class="number">200</span>, min_count=<span class="number">1</span>,</span><br><span class="line">                     negative=<span class="number">3</span>, sample=<span class="number">0.001</span>, hs=<span class="number">1</span>, workers=<span class="number">4</span>,cbow_mean=<span class="number">1</span>)</span><br><span class="line">    model.wv.save_word2vec_format(<span class="string">"model.ve"</span>, binary=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><blockquote><p>可以通过修改以上两个参数，得到不同的词向量选择质量高的，或者如下同时训练多个词向量</p></blockquote><h4 id="8-2、同时训练多个窗口的词向量"><a href="#8-2、同时训练多个窗口的词向量" class="headerlink" title="8.2、同时训练多个窗口的词向量"></a>8.2、同时训练多个窗口的词向量</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]:</span><br><span class="line">    model = Word2Vec(data_all, window=i, sg=<span class="number">0</span>, size=<span class="number">100</span>, min_count=<span class="number">1</span>, negative=<span class="number">3</span>,</span><br><span class="line">                     sample=<span class="number">0.001</span>, hs=<span class="number">1</span>, workers=<span class="number">4</span>,cbow_mean=<span class="number">1</span>)</span><br><span class="line">    model.save(os.path.join(mc.data_path, <span class="string">'model.model'</span>))</span><br><span class="line">    model.wv.save_word2vec_format(os.path.join(mc.data_path, <span class="string">"model.ve"</span> + str(i)), binary=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><h4 id="8-3、构建词向量字典"><a href="#8-3、构建词向量字典" class="headerlink" title="8.3、构建词向量字典"></a>8.3、构建词向量字典</h4><blockquote><p>训练得到的词向量是一个文件，第一行有词的个数，第二行起每一行第一列是词，其他列对应的是词向量，以空格分割，要想把数据中的词对应上还需要建立词典。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_embeddings_dict</span><span class="params">()</span>:</span></span><br><span class="line">    embeddings_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(mc.data_path, <span class="string">'model.ve'</span>), encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            values = line.split(<span class="string">' '</span>)</span><br><span class="line">            coefs = np.asarray(values[<span class="number">1</span>:], dtype=<span class="string">'float64'</span>)</span><br><span class="line">            embeddings_dict[values[<span class="number">0</span>]] = coefs</span><br><span class="line">    <span class="keyword">return</span> embeddings_dict</span><br></pre></td></tr></table></figure><h4 id="8-4、词向量构建文档"><a href="#8-4、词向量构建文档" class="headerlink" title="8.4、词向量构建文档"></a>8.4、词向量构建文档</h4><blockquote><p>一条数据寄一个文档，一个文档有10个词，每一个词是一个100维的词向量，那么这个文档就可以用word_count*100的矩阵表示。当然我们也可以通过把每一个词的词向量加和求平均值，这样一篇文档就是一个100维的向量了。</p></blockquote><ul><li>文档向量=词向量加和平均</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_embedding_vector</span><span class="params">(s, embeddings_dict)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    词嵌入构建句子向量</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    svt = np.zeros(<span class="number">100</span>, dtype=<span class="string">'float64'</span>)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> s.split(<span class="string">' '</span>):  <span class="comment"># ==x.split(' ')</span></span><br><span class="line">        word_v = embeddings_dict.get(word)</span><br><span class="line">        <span class="keyword">if</span> word_v <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            svt += word_v</span><br><span class="line">    <span class="keyword">return</span> svt</span><br></pre></td></tr></table></figure><ul><li>文档向量=文档中所有词的词向量组成的矩阵</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">EMBEDDING_DIM = <span class="number">300</span></span><br><span class="line">nb_words = min(MAX_NB_WORDS,len(word_index))</span><br><span class="line">word_embedding_matrix = np.zeros((nb_words + <span class="number">1</span>, EMBEDDING_DIM))</span><br><span class="line"><span class="keyword">for</span> word, i <span class="keyword">in</span> word_index.items():</span><br><span class="line">    <span class="keyword">if</span> i &gt; MAX_NB_WORDS:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    embedding_vector = embeddings_index.get(str(word))</span><br><span class="line">    <span class="keyword">if</span> embedding_vector <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        word_embedding_matrix[i] = embedding_vector</span><br></pre></td></tr></table></figure><blockquote><p>对每一个文档获取如上的矩阵，那么着呢哥哥训练集，就是一个三维额矩阵。Text-cnn采用的就是这种结构。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;这里主要是记录文本相关的比赛中积累的代码片段和操作，量变产生质变，每一次探索和积累都是一小步的成长，不求走的多快，多远，我一直在行走，在努力。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;文本数据的处理主要是&lt;code&gt;分词&lt;/code&gt;，&lt;code
      
    
    </summary>
    
      <category term="Match" scheme="http://nextnight.github.io/categories/Match/"/>
    
      <category term="NLP" scheme="http://nextnight.github.io/categories/Match/NLP/"/>
    
    
      <category term="文本处理" scheme="http://nextnight.github.io/tags/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>SNA-社交网络分析</title>
    <link href="http://nextnight.github.io/2018/09/29/SNA-%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C%E5%88%86%E6%9E%90/"/>
    <id>http://nextnight.github.io/2018/09/29/SNA-社交网络分析/</id>
    <published>2018-09-29T07:32:02.000Z</published>
    <updated>2018-09-30T05:39:36.796Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="class1" scheme="http://nextnight.github.io/categories/class1/"/>
    
      <category term="class2" scheme="http://nextnight.github.io/categories/class1/class2/"/>
    
    
      <category term="tg1" scheme="http://nextnight.github.io/tags/tg1/"/>
    
      <category term="tag2" scheme="http://nextnight.github.io/tags/tag2/"/>
    
  </entry>
  
  <entry>
    <title>ML-DTree决策树</title>
    <link href="http://nextnight.github.io/2018/09/29/ML-DTree%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>http://nextnight.github.io/2018/09/29/ML-DTree决策树/</id>
    <published>2018-09-29T07:16:44.000Z</published>
    <updated>2018-09-29T07:18:02.425Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="ML" scheme="http://nextnight.github.io/categories/ML/"/>
    
    
      <category term="决策树" scheme="http://nextnight.github.io/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>ML-Svm支持向量机</title>
    <link href="http://nextnight.github.io/2018/09/29/ML-Svm%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <id>http://nextnight.github.io/2018/09/29/ML-Svm支持向量机/</id>
    <published>2018-09-29T07:15:10.000Z</published>
    <updated>2018-09-29T07:17:46.640Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="ML" scheme="http://nextnight.github.io/categories/ML/"/>
    
    
      <category term="支持向量机" scheme="http://nextnight.github.io/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    
      <category term="svm" scheme="http://nextnight.github.io/tags/svm/"/>
    
  </entry>
  
  <entry>
    <title>ML-Logistic回归</title>
    <link href="http://nextnight.github.io/2018/09/29/ML-Logistic%E5%9B%9E%E5%BD%92/"/>
    <id>http://nextnight.github.io/2018/09/29/ML-Logistic回归/</id>
    <published>2018-09-29T07:13:53.000Z</published>
    <updated>2018-09-29T07:17:31.024Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="ML" scheme="http://nextnight.github.io/categories/ML/"/>
    
    
      <category term="逻辑回归" scheme="http://nextnight.github.io/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>Match-DaGuanCup</title>
    <link href="http://nextnight.github.io/2018/09/29/Match-DaGuanCup/"/>
    <id>http://nextnight.github.io/2018/09/29/Match-DaGuanCup/</id>
    <published>2018-09-29T07:02:57.000Z</published>
    <updated>2018-09-29T07:08:18.192Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="Match" scheme="http://nextnight.github.io/categories/Match/"/>
    
    
      <category term="文本分类" scheme="http://nextnight.github.io/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>Match-LianTong</title>
    <link href="http://nextnight.github.io/2018/09/29/Match-LianTong/"/>
    <id>http://nextnight.github.io/2018/09/29/Match-LianTong/</id>
    <published>2018-09-29T06:46:00.000Z</published>
    <updated>2018-09-29T08:39:01.759Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="Match" scheme="http://nextnight.github.io/categories/Match/"/>
    
    
      <category term="分类推荐" scheme="http://nextnight.github.io/tags/%E5%88%86%E7%B1%BB%E6%8E%A8%E8%8D%90/"/>
    
  </entry>
  
  <entry>
    <title>Match-YunYiCup</title>
    <link href="http://nextnight.github.io/2018/09/29/Match-Yunyicup/"/>
    <id>http://nextnight.github.io/2018/09/29/Match-Yunyicup/</id>
    <published>2018-09-29T06:42:48.000Z</published>
    <updated>2018-09-29T07:07:08.749Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="Match" scheme="http://nextnight.github.io/categories/Match/"/>
    
    
      <category term="文本分类" scheme="http://nextnight.github.io/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>Graph-BayesNetWork</title>
    <link href="http://nextnight.github.io/2018/09/29/Graph-BayesNetWork/"/>
    <id>http://nextnight.github.io/2018/09/29/Graph-BayesNetWork/</id>
    <published>2018-09-29T06:40:58.000Z</published>
    <updated>2018-09-29T07:35:49.614Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="概率图模型" scheme="http://nextnight.github.io/categories/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="BayesNetWork" scheme="http://nextnight.github.io/tags/BayesNetWork/"/>
    
      <category term="贝叶斯网络" scheme="http://nextnight.github.io/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Graph-CRF条件随机场</title>
    <link href="http://nextnight.github.io/2018/09/29/Graph-CRF%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"/>
    <id>http://nextnight.github.io/2018/09/29/Graph-CRF条件随机场/</id>
    <published>2018-09-29T06:30:53.000Z</published>
    <updated>2018-09-29T07:35:18.776Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="概率图模型" scheme="http://nextnight.github.io/categories/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="CRF" scheme="http://nextnight.github.io/tags/CRF/"/>
    
      <category term="条件随机场" scheme="http://nextnight.github.io/tags/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>Graph-HMM隐马尔科夫</title>
    <link href="http://nextnight.github.io/2018/09/29/Graph-HMM%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB/"/>
    <id>http://nextnight.github.io/2018/09/29/Graph-HMM隐马尔科夫/</id>
    <published>2018-09-29T06:30:37.000Z</published>
    <updated>2018-09-29T07:34:54.564Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="概率图模型" scheme="http://nextnight.github.io/categories/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="HMM" scheme="http://nextnight.github.io/tags/HMM/"/>
    
  </entry>
  
  <entry>
    <title>Math-矩阵分解</title>
    <link href="http://nextnight.github.io/2018/09/29/Math-%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/"/>
    <id>http://nextnight.github.io/2018/09/29/Math-矩阵分解/</id>
    <published>2018-09-29T06:27:30.000Z</published>
    <updated>2018-09-29T07:29:18.171Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="Math" scheme="http://nextnight.github.io/categories/Math/"/>
    
    
      <category term="矩阵分解" scheme="http://nextnight.github.io/tags/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/"/>
    
  </entry>
  
  <entry>
    <title>Math-凸优化问题</title>
    <link href="http://nextnight.github.io/2018/09/29/Math-%E5%87%B8%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98/"/>
    <id>http://nextnight.github.io/2018/09/29/Math-凸优化问题/</id>
    <published>2018-09-29T06:25:00.000Z</published>
    <updated>2018-09-29T07:18:50.840Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="凸优化" scheme="http://nextnight.github.io/categories/%E5%87%B8%E4%BC%98%E5%8C%96/"/>
    
    
      <category term="凸优化" scheme="http://nextnight.github.io/tags/%E5%87%B8%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Math-泰勒展式</title>
    <link href="http://nextnight.github.io/2018/09/29/Math-%E6%B3%B0%E5%8B%92%E5%B1%95%E5%BC%8F/"/>
    <id>http://nextnight.github.io/2018/09/29/Math-泰勒展式/</id>
    <published>2018-09-29T06:21:53.000Z</published>
    <updated>2018-09-29T07:29:40.308Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="Math" scheme="http://nextnight.github.io/categories/Math/"/>
    
    
      <category term="泰勒展式" scheme="http://nextnight.github.io/tags/%E6%B3%B0%E5%8B%92%E5%B1%95%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>Math-极大似然估计</title>
    <link href="http://nextnight.github.io/2018/09/29/Math-%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/"/>
    <id>http://nextnight.github.io/2018/09/29/Math-极大似然估计/</id>
    <published>2018-09-29T06:21:27.000Z</published>
    <updated>2018-09-29T07:18:26.798Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="Math" scheme="http://nextnight.github.io/categories/Math/"/>
    
    
      <category term="MLE" scheme="http://nextnight.github.io/tags/MLE/"/>
    
      <category term="极大似然估计" scheme="http://nextnight.github.io/tags/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>文本处理流程</title>
    <link href="http://nextnight.github.io/2018/09/29/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B/"/>
    <id>http://nextnight.github.io/2018/09/29/文本处理流程/</id>
    <published>2018-09-29T05:49:41.000Z</published>
    <updated>2018-09-30T10:03:59.110Z</updated>
    
    <content type="html"><![CDATA[<h2 id="文本处理的流程"><a href="#文本处理的流程" class="headerlink" title="文本处理的流程"></a>文本处理的流程</h2><p><code>文本</code>–&gt;<code>分词</code>–&gt;<code>向量化</code>–&gt;<code>建模</code></p><h2 id="1、分词方法"><a href="#1、分词方法" class="headerlink" title="1、分词方法"></a>1、分词方法</h2><blockquote><p>分词是NLP的基础，无论是文档还是语句都是有基本的词构成，因此分词的好坏直接影响到文本的表示。</p></blockquote><ol><li>基本匹配分词</li></ol><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">此方法按照不同的扫描方式，逐个查找词库进行分词。</span><br><span class="line">根据扫描方式可细分为：</span><br><span class="line">    正向最大匹配</span><br><span class="line">    反向最大匹配</span><br><span class="line">    双向最大匹配</span><br><span class="line">    最小切分<span class="comment">(即最短路径)</span></span><br><span class="line">总之就是各种不同的启发规则</span><br></pre></td></tr></table></figure><ol start="2"><li>全局切分+语言模型分词</li></ol><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby"> 基本操作：</span></span><br><span class="line"><span class="ruby">  <span class="number">1</span>、它首先切分出与词库匹配的所有可能的词，</span></span><br><span class="line"><span class="ruby">  <span class="number">2</span>、再运用统计语言模型决定最优的切分结果。【一般采用viterbi算法寻找找最优的组合】</span></span><br><span class="line"><span class="ruby">- 优点：</span></span><br><span class="line"><span class="ruby">  <span class="number">1</span>、在于可以解决分词中的歧义问题。</span></span><br><span class="line"><span class="ruby">- 示例：</span></span><br><span class="line"><span class="ruby">  对于文本串“南京市长江大桥”，首先进行词条检索(一般用Trie存储)，找到匹配的所有词条（南京，市，长江，大桥，南京市，长江大桥，市长，江大桥，江大，桥），以词网格(word lattices)形式表示，接着做路径搜索，基于统计语言模型(例如n-gram)找到最优路径，最后可能还需要命名实体识别。</span></span><br></pre></td></tr></table></figure><ol start="3"><li>以字构词</li></ol><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">- 基本操作：</span><br><span class="line">  <span class="number">1</span>、将分词问题转化为了单个字的分类问题(序列标签)，为每一个字打上<span class="string">'B'</span>,<span class="string">'I'</span>,<span class="string">'E'</span>,<span class="string">'S'</span>五个标签中的一个。其中<span class="string">'B'</span>:一个词的开头，<span class="string">'I'</span>:一个词的中间，<span class="string">'E'</span>:一个词的结尾，<span class="string">'S'</span>:单个字成词。</span><br><span class="line">  <span class="number">2</span>、通过统计信息得到每个字的标签的概率，然后采用<span class="string">'Viterbi算法'</span>得到最优的结果。</span><br><span class="line">- 优点：</span><br><span class="line">  <span class="number">1</span>、具有新词发现功能，准确率较高。</span><br><span class="line">- 实现：</span><br><span class="line">  <span class="number">1</span>、<span class="string">'HMM分词'</span>,</span><br><span class="line">  <span class="number">2</span>、<span class="string">'CRF分词'</span>,</span><br><span class="line">  <span class="number">3</span>、<span class="string">'深度学习分词'</span></span><br><span class="line"></span><br><span class="line">- [<span class="number">1</span>] 深度学习分词的流程：</span><br><span class="line">    <span class="number">-1</span>、将每一个字<span class="symbol">Lookup</span> <span class="symbol">Table</span>映射到一个固定长度的特征向量.</span><br><span class="line">    <span class="number">-2</span>、经过一个标准的神经网络：liner-sigmod-liner三层<span class="string">'同样得到每个字属于B,I,E,S四个Tag的概率'</span>.</span><br><span class="line">    <span class="number">-3</span>、采用<span class="string">'Viterbi算法'</span>求得最优结果.</span><br><span class="line"></span><br><span class="line">- [<span class="number">2</span>] <span class="symbol">HMM</span>分词 <span class="symbol">VS</span> <span class="symbol">CRF</span>分词</span><br><span class="line">    <span class="symbol">CRF</span>：目前效果已经优于<span class="symbol">HMM</span>，无论是新词发现还是实体识别，人名识别。</span><br><span class="line">    优点:</span><br><span class="line">        <span class="number">1</span>、在于<span class="symbol">CRF</span>既可以像最大熵模型一样加各种领域feature。</span><br><span class="line">        <span class="number">2</span>、又避免了<span class="symbol">HMM</span>的齐次马尔科夫假设。</span><br></pre></td></tr></table></figure><h2 id="2、语言模型："><a href="#2、语言模型：" class="headerlink" title="2、语言模型："></a>2、语言模型：</h2><blockquote><p>语言模型是用来计算一句话生成概率的模型，</p></blockquote><h3 id="1、语言模型的演变"><a href="#1、语言模型的演变" class="headerlink" title="1、语言模型的演变"></a>1、语言模型的演变</h3><p>[1.1] bayes:</p><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby"> 定义：每个词出现的概率取决于前面所有的词</span></span><br><span class="line"><span class="ruby">- 公式：P(w_m) = p(w_m<span class="params">|w_&#123;m-1&#125;,w_&#123;m-2&#125;..w_1)</span></span></span><br><span class="line"><span class="ruby">- P(S)：P(S) = P(w_1)*p(w_2)...p(w_n),n为词或字个数,P(S)表示句子概率</span></span><br><span class="line"><span class="ruby">- 缺点：计算过于复杂，甚至难以计算</span></span><br><span class="line"><span class="ruby">- 简化：朴素贝叶斯，条件独立性假设</span></span><br></pre></td></tr></table></figure><p>[1.2] n-gram：n元词型</p><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby"> 定义：衍生自HMM，它利用马尔科夫假设，认为句子中每个单词只与其前n–<span class="number">1</span>个单词有关，即假设产生w_m这个词的条件概率只依赖于前n–<span class="number">1</span>个词。其中n越大，模型可区别性越强，n越小，模型可靠性越高。</span></span><br><span class="line"><span class="ruby">- 公式：p(w_m) = p(w_m<span class="params">|w_&#123;m-n+1&#125;...,w_&#123;m-1&#125;))</span></span></span><br><span class="line"><span class="ruby">- P(S)：P(S) = P(w_1)*p(w_2)...p(w_n),n为词或字个数,P(S)表示句子概率</span></span><br><span class="line"><span class="ruby">- 优点：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、简单有效，多元词型组合可以得到更丰富的信息</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、考虑了词的位置关系</span></span><br><span class="line"><span class="ruby">- 不足：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、没有考虑到词语的相似性</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、没有考虑词法，语法，以及语义信息</span></span><br><span class="line"><span class="ruby">    <span class="number">3</span>、仍然存在数据稀疏的问题</span></span><br></pre></td></tr></table></figure><p>[1.3] ffnnlm：前馈神经网络语言模型</p><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby"> 定义：基于n-gram的语言模型，他将词<span class="string">"w_m"</span>的前n-<span class="number">1</span>个词p(w<span class="number">_</span>&#123;m-n+<span class="number">1</span>&#125;...p(w<span class="number">_</span>&#123;m-<span class="number">1</span>&#125;))映射到词向量空间，然后把它们拼接起来得到一个更大的词向量作为神经网络的输入，<span class="string">'输出'</span>的是p(w_m)</span></span><br><span class="line"><span class="ruby">- 优点：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、词语之间的相似性可以通过词向量来体现</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、自带平滑功能</span></span><br></pre></td></tr></table></figure><p>[1.4] rnnlm:循环神经网络语言模型</p><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby"> rnnlm特点：可以存在有向环，将上一次的输出作为本次的输入。</span></span><br><span class="line"><span class="ruby">- rnnlm和ffnnlm的最大区别是：ffnnmm要求输入的上下文是固定长度的，也就是说n-gram中的 n要求是个固定值，而rnnlm不限制上下文的长度，可以真正充分地利用所有上文信息来预测下一个词，本次预测的中间隐层信息(例如下图中的context信息)可以在下一次预测里循环使用。</span></span><br><span class="line"><span class="ruby">- 基本结构：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、输入层：</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、隐藏层：</span></span><br><span class="line"><span class="ruby">    <span class="number">3</span>、输出层：</span></span><br><span class="line"><span class="ruby">- 预测步骤：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、单词w<span class="number">_</span>&#123;m-<span class="number">1</span>&#125;映射到词向量，记作input(t)</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、连接上一次训练的隐藏层context(t–<span class="number">1</span>)，经过sigmoid function，生成当前t时刻的context(t)</span></span><br><span class="line"><span class="ruby">    <span class="number">3</span>、利用softmax function，预测P(w_m)</span></span><br><span class="line"><span class="ruby">- 缺点：基于RNN的语言模型利用BPTT(BackPropagation through time)算法比较难于训练，原因就是深度神经网络里比较普遍的梯度消失问题。</span></span><br><span class="line"><span class="ruby">- 优点：训练精度高</span></span><br></pre></td></tr></table></figure><p>[1.5] lstmlm:长短记忆神经网络</p><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby"> lstmlm特点：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、也是一种RNN网络，通过结构的修改避免了梯度消失。</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、能够记忆更多的信息。</span></span><br></pre></td></tr></table></figure><h3 id="2、N-gram的基本原理"><a href="#2、N-gram的基本原理" class="headerlink" title="2、N-gram的基本原理"></a>2、N-gram的基本原理</h3><blockquote><p>N-gram即N元语法模型，采用马尔科夫假设，认为一个词生成的概率取决于他前面的N-1个词。</p></blockquote><figure class="highlight erlang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">- 约定：</span><br><span class="line">    s:代表句子</span><br><span class="line">    w_i:第i个词</span><br><span class="line">    w_n:代表第n个词</span><br><span class="line">    N:词数</span><br><span class="line">- 句子s生成的概率如下：</span><br><span class="line">    P<span class="params">(s)</span> = p<span class="params">(w_1)</span>p<span class="params">(w_2)</span>p<span class="params">(w_3)</span>...p(w_n)</span><br><span class="line">- "为了避免数据溢出、提高性能，通常会使用取 log 后使用加法运算替代乘法运算"即：</span><br><span class="line">- log<span class="params">(p(s))</span> = log<span class="params">(p(w_1))</span>+log<span class="params">(p(w_2))</span>+log<span class="params">(p(w_3))</span>...log(p(w_n))</span><br></pre></td></tr></table></figure><p>[2.1]、<code>bigram</code>:假设一个词生成的概率、取决于他前面1个词</p><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">- 句子s生成的概率如下：</span><br><span class="line">    P<span class="comment">(s)</span> = p<span class="comment">(w_1)</span>p<span class="comment">(w_2)</span>p<span class="comment">(w_3)</span>...p<span class="comment">(w_n)</span></span><br><span class="line">- bigram下词w_m生成的概率：</span><br><span class="line">    p<span class="comment">(w_1)</span> = p<span class="comment">(w_1)</span></span><br><span class="line">    p<span class="comment">(w_2)</span> = p<span class="comment">(w_2|w_1)</span></span><br><span class="line">    p<span class="comment">(w_3)</span> = p<span class="comment">(w_3|w_2)</span></span><br><span class="line">    ...</span><br><span class="line">    p<span class="comment">(w_m)</span> = p<span class="comment">(w_m|w_m-1)</span></span><br><span class="line">- 得到句子生成的概率：</span><br><span class="line">    P<span class="comment">(s)</span> = p<span class="comment">(w_1)</span>p<span class="comment">(w_2|w_1)</span>p<span class="comment">(w_3|w_2)</span>..p<span class="comment">(w_m|w_m-1)</span>..p<span class="comment">(w_n|w_n-1)</span>-</span><br></pre></td></tr></table></figure><p>[2.2]、<code>trigram</code>:假设一个词的生成概率取决于他前面的2个词</p><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">- 句子s生成的概率如下：</span><br><span class="line">    P<span class="comment">(s)</span> = p<span class="comment">(w_1)</span>p<span class="comment">(w_2)</span>p<span class="comment">(w_3)</span>...p<span class="comment">(w_n)</span></span><br><span class="line">- trigram下词w_m生成的概率：</span><br><span class="line">    p<span class="comment">(w_1)</span> = p<span class="comment">(w_1)</span></span><br><span class="line">    p<span class="comment">(w_2)</span> = p<span class="comment">(w_2|w_1)</span></span><br><span class="line">    p<span class="comment">(w_3)</span> = p<span class="comment">(w_3|w_&#123;1,2&#125;)</span></span><br><span class="line">    ...</span><br><span class="line">    p<span class="comment">(w_m)</span> = p<span class="comment">(w_m|w_&#123;m-2,m-1&#125;)</span></span><br><span class="line">- 得到句子生成的概率：</span><br><span class="line">    P<span class="comment">(s)</span> = p<span class="comment">(w_1)</span>p<span class="comment">(w_2|w_1)</span>p<span class="comment">(w_3|w_1,w_2)</span>..p<span class="comment">(w_m|w_m-2,w_m-1)</span>..p<span class="comment">(w_n|w_n-1,w_n-2)</span>-</span><br></pre></td></tr></table></figure><blockquote><p>如上知道了如何去计算一个句子生成的概率，那么该如何去算其中的每一项呢？其实不过还是频率来表示概率。</p><p><code>bigram</code>中<code>P(w_m) = p(w_m|w_m-1)=count(w_m,w_m-1)/count(w_m-1)</code>即计算当前词的概率用当前词和前一个词共同出现的次数比上前一个词出现的次数。</p></blockquote><p>[2.3] bigram示例：</p><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- 语料：</span><br><span class="line">    <span class="built_in">s1</span>：我喜欢西红柿炒鸡蛋</span><br><span class="line">    <span class="built_in">s2</span>：西红柿是我最爱</span><br><span class="line"><span class="symbol">    s3:</span> 我喜欢什么</span><br></pre></td></tr></table></figure><p>单词词频：</p><table><thead><tr><th>word</th><th>我</th><th>喜欢</th><th>西红柿</th><th>炒</th><th>鸡蛋</th><th>是</th><th>最爱</th><th>什么</th></tr></thead><tbody><tr><td>count</td><td>2</td><td>2</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td></tr><tr><td>p(word)</td><td>0.2</td><td>0.2</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td></tr></tbody></table><figure class="highlight erlang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- 句子s1出现的概率：</span><br><span class="line">    p<span class="params">(s1)</span> = p<span class="params">(我)</span>p<span class="params">(喜欢)</span>p<span class="params">(西红柿)</span>p<span class="params">(炒)</span>p<span class="params">(鸡蛋)</span></span><br><span class="line">        = p<span class="params">(我)</span>p<span class="params">(喜欢|我)</span>p<span class="params">(西红柿|喜欢)</span>p<span class="params">(炒|西红柿)</span>p<span class="params">(鸡蛋|炒)</span></span><br><span class="line">- 根据顺序共现得到bigram的共现词频如下：</span><br></pre></td></tr></table></figure><p>顺序共现词频：</p><table><thead><tr><th style="text-align:center">word</th><th>我</th><th>喜欢</th><th>西红柿</th><th>炒</th><th>鸡蛋</th><th>是</th><th>最爱</th><th>什么</th></tr></thead><tbody><tr><td style="text-align:center">我</td><td></td><td>2</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td style="text-align:center">喜欢</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td style="text-align:center">西红柿</td><td></td><td></td><td></td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td style="text-align:center">炒</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td style="text-align:center">鸡蛋</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td style="text-align:center">是</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td style="text-align:center">最爱</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td style="text-align:center">什么</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><figure class="highlight erlang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">- 最后得到句子s1的概率如下：</span><br><span class="line">  p<span class="params">(s1)</span> = p<span class="params">(我)</span>p<span class="params">(喜欢)</span>p<span class="params">(西红柿)</span>p<span class="params">(炒)</span>p<span class="params">(鸡蛋)</span></span><br><span class="line">        = p<span class="params">(我)</span>p<span class="params">(喜欢|我)</span>p<span class="params">(西红柿|喜欢)</span>p<span class="params">(炒|西红柿)</span>p<span class="params">(鸡蛋|炒)</span></span><br><span class="line">        = 0.<span class="number">2</span> * (count(我喜欢)/count(我))</span><br><span class="line">              * (count(喜欢西红柿)/count(喜欢))</span><br><span class="line">              * (count(西红柿炒)/count(西红柿))</span><br><span class="line">              * (count(炒鸡蛋)/count(炒))</span><br></pre></td></tr></table></figure><p>同理可得trigram，n-gram的计算方式，分别去统计N元词出现的次数。得到条件概率。</p><p>[2.4]、数据平滑</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">- 数据平滑：大规模数据统计方法与有限的训练语料之间必然产生数据稀疏问题，导致零概率问题。为了解决数据稀疏问题，人们为理论模型实用化而进行了众多尝试与努力，诞生了一系列经典的平滑技术。</span><br><span class="line">- 基本思想：“降低已出现 n-gram 的条件概率分布，以使未出现的 n-gram 条件概率分布非零”，且经数据平滑后一定保证概率和为<span class="number">1</span>，</span><br><span class="line">- 平滑技术：</span><br><span class="line">      <span class="number">1</span>、加一平滑：又称拉普拉斯平滑</span><br><span class="line">      Pmle(w_m|w_m<span class="number">-1</span>) = count(w_m,w_m<span class="number">-1</span>)/count(w_m<span class="number">-1</span>)</span><br><span class="line">      Padd1(w_m|w_m<span class="number">-1</span>) = count(w_m,w_m<span class="number">-1</span>)+<span class="number">1</span>/count(w_m<span class="number">-1</span>)+VV代表<span class="number">2</span>元词型的个数。V个二元词型保证概率和为<span class="number">1</span>。</span><br></pre></td></tr></table></figure><h2 id="3、向量化"><a href="#3、向量化" class="headerlink" title="3、向量化"></a>3、向量化</h2><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">文本的向量化主要有以下几种：</span><br><span class="line">    <span class="selector-attr">[1]</span> <span class="selector-tag">countvec</span>：词袋模型，词频表示</span><br><span class="line">    <span class="selector-attr">[2]</span> <span class="selector-tag">Tf-idf</span>：基于<span class="selector-tag">bow</span>的权重表示</span><br><span class="line">    <span class="selector-attr">[3]</span> 主题模型<span class="selector-pseudo">:PLSA</span>，<span class="selector-tag">LDA</span></span><br><span class="line">    <span class="selector-attr">[4]</span> 词嵌入：<span class="selector-tag">word2vec</span>,<span class="selector-tag">glove</span>,<span class="selector-tag">fastext</span></span><br><span class="line">约定：</span><br><span class="line">    1、词袋：数据集中所有的的词构成的词集或词典。</span><br><span class="line">    2、词频：一篇文档中某个词出现的次数</span><br></pre></td></tr></table></figure><h3 id="1、词频向量：countvec"><a href="#1、词频向量：countvec" class="headerlink" title="1、词频向量：countvec"></a>1、词频向量：countvec</h3><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby">   定义：把每一篇文档表示为一个长度为词袋大小的向量，向量的每一个维度表示为词袋中的每一个词在此文档中出现的次数。</span></span><br><span class="line"><span class="ruby"></span></span><br><span class="line"><span class="ruby">-   优缺点：</span></span><br><span class="line"><span class="ruby">     <span class="number">1</span>、能够一定程度反映词频信息，但是对于多篇文档中都出现的高频词没有银锭的区分度，</span></span><br><span class="line"><span class="ruby">     <span class="number">2</span>、词袋模型向量大多是高维稀疏矩阵。</span></span><br></pre></td></tr></table></figure><h3 id="2、权重向量：tf-idf"><a href="#2、权重向量：tf-idf" class="headerlink" title="2、权重向量：tf-idf"></a>2、权重向量：tf-idf</h3><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby"> 定义：词频(TF)x逆文档词频(IDF)用于表示一个词的权重。</span></span><br><span class="line"><span class="ruby">- 优点：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、降低多个文档都出现的高频词汇对重要度的影响。</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、提升低频词汇的重要度。</span></span><br><span class="line"><span class="ruby">    <span class="number">3</span>、tf-idf+<span class="string">'N-gram'</span>特征一定程度能够反映语序特征</span></span><br><span class="line"><span class="ruby">- 缺点：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、权重向量也是高维稀疏矩阵.</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、不能完全反映语序信息</span></span><br><span class="line"><span class="ruby">    <span class="number">3</span>、不能体现词语间的相似度信息。</span></span><br></pre></td></tr></table></figure><h3 id="3、主题向量：plsa-lda"><a href="#3、主题向量：plsa-lda" class="headerlink" title="3、主题向量：plsa,lda"></a>3、主题向量：plsa,lda</h3><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby"> 定义：通过引入隐变量<span class="string">'主题'</span>将词频向量或者权重向量映射到一个固定维度的稠密向量。</span></span><br><span class="line"><span class="ruby">- 优点：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、降维作用，降低了模型的复杂度和计算量。</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、包含了一定的语义信息。</span></span><br><span class="line"><span class="ruby">- 缺点：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、主题数难以确定。</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、主题作为一种概率分布表示，难以表述其具体含义，可解释性较差。</span></span><br><span class="line"><span class="ruby">    <span class="number">3</span>、数据量大的时候主题训练耗时较长。</span></span><br></pre></td></tr></table></figure><h3 id="4、词嵌入向量-word2vec-glove-fasttext"><a href="#4、词嵌入向量-word2vec-glove-fasttext" class="headerlink" title="4、词嵌入向量:word2vec,glove,fasttext"></a>4、词嵌入向量:word2vec,glove,fasttext</h3><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby"> 定义：通过训练将每一个词映射到固定长度的词向量空间中，每个词就是一个点，同时引入距离的概念，就可以描述词语的相似度。</span></span><br><span class="line"><span class="ruby">- 优点：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、考虑到了词语的相似度信息。</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、维度固定且低维，便于后续计算。</span></span><br><span class="line"><span class="ruby">    <span class="number">3</span>、自带平滑功能</span></span><br><span class="line"><span class="ruby">- 缺点：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、好的词向量表示依赖于足够的训练预料，尤其是领域性较强的文本依赖于相关的领域文本，否则可能得不到足够好的词向量。</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、词向量的训练需要额外的步骤。且大语料的训练耗时。</span></span><br></pre></td></tr></table></figure><h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby"> 模型结构：</span></span><br><span class="line"><span class="ruby">    <span class="params">|--&gt;输入层</span></span></span><br><span class="line"><span class="ruby">        <span class="params">|--&gt;隐藏层</span></span></span><br><span class="line"><span class="ruby">            <span class="params">|--&gt;softmax</span></span></span><br><span class="line"><span class="ruby">- 两种实现方式：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、cbow</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、skip-gram</span></span><br><span class="line"><span class="ruby">- 两种优化方式：</span></span><br><span class="line"><span class="ruby">    <span class="number">1</span>、层次化softmax</span></span><br><span class="line"><span class="ruby">    <span class="number">2</span>、负采样</span></span><br></pre></td></tr></table></figure><p>🐱目标函数：</p><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby"> <span class="symbol">w:</span>词w</span></span><br><span class="line"><span class="ruby">- <span class="symbol">C:</span>语料库</span></span><br><span class="line"><span class="ruby">- Context(w)<span class="symbol">:</span>词w的上下文信息</span></span><br></pre></td></tr></table></figure><ul><li><p>cbow:p(w|Context(w))：上下文词出现的情况下，词w出现的概率，即共现的概率。也是优化的目标，最大化这个概率。那么对与整个语料来说，目标函数就是最大化如下的概率积</p><p>$$L = \prod_{w\epsilon C}p(w|Context(w))$$</p></li><li><p>skip-gram:以当前词预测上下文词的概率，即目标函数就是最大化当前词出现的时候上下文词出现的概率积。</p><p>$$L = \prod_{w\epsilon C}p(Context(w)|w)$$</p></li><li><p>最大对数似然：</p><p>$$L = \sum_{w\epsilon C}log(p(w|Cotext(w))))$$</p><p>$$L = \sum_{w\epsilon C}log(p(Context(w)|w))$$</p></li></ul><figure class="highlight erlang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- 训练过程：</span><br><span class="line">    1、输入层：随机初始化w的context<span class="params">(w)</span>的每一个词<span class="params">(前后各c个)</span>的词向量V<span class="params">(w)</span>。</span><br><span class="line">    2、投影层：将词w的context<span class="params">(w)</span>的所有V<span class="params">(w)</span>求和。</span><br><span class="line">    3、输出层：层次化softmax层，一颗二叉树，用计算得到的输出每个词的概率。</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;文本处理的流程&quot;&gt;&lt;a href=&quot;#文本处理的流程&quot; class=&quot;headerlink&quot; title=&quot;文本处理的流程&quot;&gt;&lt;/a&gt;文本处理的流程&lt;/h2&gt;&lt;p&gt;&lt;code&gt;文本&lt;/code&gt;–&amp;gt;&lt;code&gt;分词&lt;/code&gt;–&amp;gt;&lt;code&gt;向量
      
    
    </summary>
    
      <category term="ML" scheme="http://nextnight.github.io/categories/ML/"/>
    
      <category term="NLP" scheme="http://nextnight.github.io/categories/ML/NLP/"/>
    
    
      <category term="分词" scheme="http://nextnight.github.io/tags/%E5%88%86%E8%AF%8D/"/>
    
      <category term="向量化" scheme="http://nextnight.github.io/tags/%E5%90%91%E9%87%8F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>mac jupyter中文显示设置</title>
    <link href="http://nextnight.github.io/2018/09/13/Mac-jupyter%E4%B8%AD%E6%96%87%E6%98%BE%E7%A4%BA%E8%AE%BE%E7%BD%AE/"/>
    <id>http://nextnight.github.io/2018/09/13/Mac-jupyter中文显示设置/</id>
    <published>2018-09-13T15:08:34.000Z</published>
    <updated>2018-09-29T07:12:29.467Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MAC-Jupyter使用matplotlib时设置中文"><a href="#MAC-Jupyter使用matplotlib时设置中文" class="headerlink" title="MAC-Jupyter使用matplotlib时设置中文"></a>MAC-Jupyter使用matplotlib时设置中文</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.font_manager <span class="keyword">import</span> _rebuild</span><br><span class="line">_rebuild()</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置中文显示</span></span><br><span class="line"></span><br><span class="line">mpl.rcParams[<span class="string">'font.family'</span>]=[<span class="string">'Microsoft Yahei'</span>]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;MAC-Jupyter使用matplotlib时设置中文&quot;&gt;&lt;a href=&quot;#MAC-Jupyter使用matplotlib时设置中文&quot; class=&quot;headerlink&quot; title=&quot;MAC-Jupyter使用matplotlib时设置中文&quot;&gt;&lt;/a&gt;MA
      
    
    </summary>
    
      <category term="Python" scheme="http://nextnight.github.io/categories/Python/"/>
    
    
      <category term="Jupyter" scheme="http://nextnight.github.io/tags/Jupyter/"/>
    
      <category term="matplotlib" scheme="http://nextnight.github.io/tags/matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>Pandas基本操作</title>
    <link href="http://nextnight.github.io/2018/09/13/Pandas%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    <id>http://nextnight.github.io/2018/09/13/Pandas基本操作/</id>
    <published>2018-09-13T14:58:52.000Z</published>
    <updated>2018-09-19T01:51:49.094Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Pandas-操作分类"><a href="#Pandas-操作分类" class="headerlink" title="Pandas 操作分类"></a>Pandas 操作分类</h1><ul><li>基本设置</li><li>数据描述</li><li>数据清洗</li><li>数据索引</li><li>数据连接</li><li>分组聚合</li><li>文本操作</li></ul><h1 id="基本设置"><a href="#基本设置" class="headerlink" title="基本设置"></a>基本设置</h1><p>① ：设置不使用科学计数法，保留5位小数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.set_option(<span class="string">'display.float_format'</span>, <span class="keyword">lambda</span> x: <span class="string">'%.5f'</span> % x)</span><br></pre></td></tr></table></figure></p><p>② : 设置显示结果的宽度(不截断换行)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.set_option(<span class="string">'display.width'</span>, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure></p><h1 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pd.read_csv()</span><br><span class="line">pd.read_table()</span><br><span class="line">pd.read_excle()</span><br><span class="line">pd.read_html()</span><br><span class="line">pd.read_json()</span><br><span class="line">pd.read_sql()</span><br></pre></td></tr></table></figure><h1 id="数据描述"><a href="#数据描述" class="headerlink" title="数据描述"></a>数据描述</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">"path"</span>)</span><br></pre></td></tr></table></figure><p>①：查看数据属性<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看属性</span></span><br><span class="line">df.shape</span><br><span class="line">df.index</span><br><span class="line">df.columns</span><br><span class="line">df.size</span><br><span class="line"></span><br><span class="line"><span class="comment"># 去除格式，获取数据数组</span></span><br><span class="line">df.values</span><br></pre></td></tr></table></figure></p><p>②：查看数据信息<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据信息</span></span><br><span class="line">df.head(n) <span class="comment"># 查看前n行</span></span><br><span class="line">df.tail(n) <span class="comment"># 查看尾n行</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看数据信息</span></span><br><span class="line">df.info()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看数据描述：</span></span><br><span class="line">df.describe()            <span class="comment"># 所有列描述</span></span><br><span class="line">df[<span class="string">'col_name'</span>].describe()<span class="comment"># 某一列描述</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计某列每个值的量:默认会从大到小排序</span></span><br><span class="line">df[<span class="string">'clo_name'</span>].value_counts(sort=<span class="keyword">True</span>,ascending=<span class="keyword">False</span>,bins=<span class="keyword">None</span>,dropna=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure></p><h1 id="数据索引"><a href="#数据索引" class="headerlink" title="数据索引"></a>数据索引</h1><h3 id="索引设置"><a href="#索引设置" class="headerlink" title="索引设置"></a>索引设置</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""index,索引操作</span></span><br><span class="line"><span class="string">  1、set_index()</span></span><br><span class="line"><span class="string">  2、reset_index()</span></span><br><span class="line"><span class="string">  3、rename()</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 设置索引</span></span><br><span class="line">df.set_index(<span class="string">'A'</span>) <span class="comment"># 设置列A为索引列</span></span><br><span class="line"><span class="comment"># 重置索引:恢复默认的自增索引</span></span><br><span class="line">df.reset_index(inplace=<span class="keyword">True</span>) <span class="comment"># 设置inplace的时候会修改原始df,并且没有返回值</span></span><br><span class="line">df = df.reset_index()        <span class="comment"># 不设置inplace=True需接受返回值，且原始df不变</span></span><br><span class="line">df.rename(index=<span class="keyword">lambda</span> x:x+<span class="number">1</span>)<span class="comment"># 所有索引值+1</span></span><br><span class="line"><span class="string">"""修改列名</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">df.rename(&#123;<span class="string">"A"</span>:<span class="string">"AAAAA"</span>,<span class="string">"B"</span>:<span class="string">"BBBB"</span>&#125;,inplace=<span class="keyword">True</span>) <span class="comment"># 把A修改成AAAAA</span></span><br><span class="line">df.rename(columns=<span class="keyword">lambda</span> x: x + <span class="number">2</span>) <span class="comment"># 将全体列重命名</span></span><br></pre></td></tr></table></figure><h3 id="索引数据"><a href="#索引数据" class="headerlink" title="索引数据"></a>索引数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""直接列索引：</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">df[<span class="number">1</span>] <span class="comment"># 第一列</span></span><br><span class="line">df[<span class="string">'Q'</span>] <span class="comment">#Q列</span></span><br><span class="line"></span><br><span class="line"><span class="string">""""行索引：</span></span><br><span class="line"><span class="string">  1、iloc</span></span><br><span class="line"><span class="string">  2、loc</span></span><br><span class="line"><span class="string">  3、ix</span></span><br><span class="line"><span class="string">"""</span><span class="string">"</span></span><br><span class="line"><span class="string">df.loc[df['Q']=0]  # 索引满足Q=0的虽偶有行数据</span></span><br><span class="line"><span class="string">df.loc[(df['Q'] &amp; df['T']=1)]</span></span><br><span class="line"><span class="string">df.loc[(df['Q'] &amp; df['T']=1) 'x'] = -1 # 满足条件的所有行数据给X 列赋值为-1</span></span><br></pre></td></tr></table></figure><h1 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h1><h3 id="数据缺失处理"><a href="#数据缺失处理" class="headerlink" title="数据缺失处理"></a>数据缺失处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""判断缺失:</span></span><br><span class="line"><span class="string">  1、isnull(),</span></span><br><span class="line"><span class="string">  2、isna()</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">df.isnull() <span class="comment"># 返回每个数据是否是null</span></span><br><span class="line">df.isna()   <span class="comment"># 返回每个数据是否是na</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""缺失填充:</span></span><br><span class="line"><span class="string">  1、fillna(),</span></span><br><span class="line"><span class="string">  2、ffill(),</span></span><br><span class="line"><span class="string">  3、bfill()</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">df.ffill()</span><br><span class="line">df.bfill()</span><br><span class="line">df.fillna(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">"""缺失删除:dropna()</span></span><br><span class="line"><span class="string">param:</span></span><br><span class="line"><span class="string">  axis:1,0 axis=1 删除含有na的列,axis=0 删除含有na的行,</span></span><br><span class="line"><span class="string">  how:any,all how='any'表示某列所有行存在为null即删除,how='all'，表示某行所有列都为null才删除</span></span><br><span class="line"><span class="string">  subset:选择子集</span></span><br><span class="line"><span class="string">  inplace:是否覆盖原数据</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">df = df.dropna(self, axis=<span class="number">0</span>, how=<span class="string">'any'</span>, thresh=<span class="keyword">None</span>, subset=<span class="keyword">None</span>,inplace=<span class="keyword">False</span>)</span><br><span class="line">df.dropna(self, axis=<span class="number">1</span>, how=<span class="string">'all'</span>, thresh=<span class="keyword">None</span>, subset=[<span class="string">'col1'</span>,<span class="string">'col2'</span>],inplace=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h3 id="数据异常处理"><a href="#数据异常处理" class="headerlink" title="数据异常处理"></a>数据异常处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""数据删除:drop()</span></span><br><span class="line"><span class="string">param:</span></span><br><span class="line"><span class="string">  axis:0,1 分别对应indexs,columns</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">df.drop(self, labels=<span class="keyword">None</span>, axis=<span class="number">0</span>, index=<span class="keyword">None</span>, columns=<span class="keyword">None</span>, level=<span class="keyword">None</span>,</span><br><span class="line">             inplace=<span class="keyword">False</span>, errors=<span class="string">'raise'</span>)</span><br><span class="line"></span><br><span class="line">df.drop([<span class="string">'a'</span>,<span class="string">'b'</span>],axis=<span class="number">1</span>,inplace=<span class="keyword">True</span>) <span class="comment"># 删除a，b列</span></span><br><span class="line">df.drop([<span class="number">0</span>,<span class="number">1</span>],axis=<span class="number">0</span>,inplace=<span class="keyword">True</span>) <span class="comment"># 删除0，1行</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""数据删除：del</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">del</span> df[<span class="string">'A'</span>]  <span class="comment"># 删除A列</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""数值替换:replace()</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">df.replace([<span class="number">1</span>,<span class="number">3</span>],[<span class="string">'one'</span>,<span class="string">'three'</span>]) <span class="comment"># 1替换成one,3替换成three</span></span><br><span class="line">df.rename(&#123;<span class="number">1</span>:<span class="string">'one'</span>,<span class="number">3</span>:<span class="string">'three'</span>&#125;)</span><br></pre></td></tr></table></figure><h3 id="数据过滤-选择"><a href="#数据过滤-选择" class="headerlink" title="数据过滤,选择"></a>数据过滤,选择</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df[(df[<span class="string">'age'</span>]&gt;<span class="number">10</span>) &amp; (df[<span class="string">'age'</span>]&lt;<span class="number">40</span>) ]</span><br><span class="line">df.loc[(df[<span class="string">'age'</span>]&gt;<span class="number">10</span>) &amp; (df[<span class="string">'age'</span>]&lt;<span class="number">40</span>) ]</span><br><span class="line">df[df[<span class="string">'age'</span>]==<span class="number">20</span>]</span><br><span class="line"></span><br><span class="line">df[df[<span class="string">'age'</span>].isin([<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>]) ]</span><br></pre></td></tr></table></figure><h3 id="数据排序"><a href="#数据排序" class="headerlink" title="数据排序"></a>数据排序</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""排序：sort_values(by='column',inplace=None,acsending=True)  </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">df.sort_values(by=<span class="string">''</span>,inplace=<span class="keyword">True</span>,acsending=<span class="keyword">False</span>) <span class="comment"># acsending=False 从大到小</span></span><br></pre></td></tr></table></figure><h3 id="数据编码"><a href="#数据编码" class="headerlink" title="数据编码"></a>数据编码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""one_hot:get_dummies()</span></span><br><span class="line"><span class="string">"""</span><span class="string">"</span></span><br><span class="line"><span class="string">dt_one_hot = pd.get_dummies(dt[['A','B']]) # 对A，B 列进行onehot编码</span></span><br></pre></td></tr></table></figure><h3 id="数据分箱"><a href="#数据分箱" class="headerlink" title="数据分箱"></a>数据分箱</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""df.cut()</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">df_col1 = pd.cut(df[<span class="string">'column1'</span>], bins=[<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">0.8</span>, <span class="number">2</span>, <span class="number">20</span>, <span class="number">1000</span>], labels=np.arange(<span class="number">1</span>, <span class="number">6</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Pandas-操作分类&quot;&gt;&lt;a href=&quot;#Pandas-操作分类&quot; class=&quot;headerlink&quot; title=&quot;Pandas 操作分类&quot;&gt;&lt;/a&gt;Pandas 操作分类&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;基本设置&lt;/li&gt;
&lt;li&gt;数据描述&lt;/li&gt;
&lt;li&gt;
      
    
    </summary>
    
      <category term="Python" scheme="http://nextnight.github.io/categories/Python/"/>
    
      <category term="Pandas" scheme="http://nextnight.github.io/categories/Python/Pandas/"/>
    
    
      <category term="Python" scheme="http://nextnight.github.io/tags/Python/"/>
    
      <category term="Pandas" scheme="http://nextnight.github.io/tags/Pandas/"/>
    
  </entry>
  
  <entry>
    <title>Numpy基本操作</title>
    <link href="http://nextnight.github.io/2018/09/10/Numpy%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    <id>http://nextnight.github.io/2018/09/10/Numpy基本操作/</id>
    <published>2018-09-10T14:45:39.000Z</published>
    <updated>2018-09-15T02:25:03.415Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="string">"""numpy的基本应用</span></span><br><span class="line"><span class="string">1、四个属性</span></span><br><span class="line"><span class="string">    ndim:获取数据的维度数</span></span><br><span class="line"><span class="string">    shape:收取数据的维度</span></span><br><span class="line"><span class="string">    dtype:获取数据的类型</span></span><br><span class="line"><span class="string">    size:</span></span><br><span class="line"><span class="string">2、N个方法</span></span><br><span class="line"><span class="string">    1、zeros,ones,empty,arange</span></span><br><span class="line"><span class="string">    2、max,min,std,mean,sum,var,median,cumsum</span></span><br><span class="line"><span class="string">    3、astype,reshape</span></span><br><span class="line"><span class="string">    4、argmax,argmin,argsort</span></span><br><span class="line"><span class="string">    5、all,any,fill,where，diff</span></span><br><span class="line"><span class="string">    6、vstack,hstack</span></span><br><span class="line"><span class="string">    7、unique</span></span><br><span class="line"><span class="string">    8、load,save:操作文件</span></span><br><span class="line"><span class="string">    9、insert,arange,reshape</span></span><br><span class="line"><span class="string">    10、bincount</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">numpy_filed</span><span class="params">()</span>:</span></span><br><span class="line">    ll = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">                   [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">7</span>]])</span><br><span class="line">    print(ll)</span><br><span class="line">    print(ll.ndim)  <span class="comment"># 2；表示2个维度</span></span><br><span class="line">    print(ll.shape)  <span class="comment"># (5,):表示五个元素</span></span><br><span class="line">    print(ll.dtype)  <span class="comment"># 元素类型int64:表示数据类型</span></span><br><span class="line">    print(ll.size)  <span class="comment"># 10：元素个数</span></span><br><span class="line">    print(type(ll))  <span class="comment"># 对象类型&lt;class 'numpy.ndarray'&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">numpy_method</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    max,min,std,mean,sum,var,median,cumsum</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    astype:转换数据类型</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    argmax,argmin,argsort</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    all,any,fill,where</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    vstack,hstack</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    load,save:操作文件</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    insert,arange,reshape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    ll = np.array([[<span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">                   [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">7</span>]])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># max,min,std,mean,sum,var,median,cumsum</span></span><br><span class="line">    print(ll.max())        <span class="comment"># 全局最大值</span></span><br><span class="line">    print(ll.max(axis=<span class="number">1</span>))  <span class="comment"># axis=1：行最大值 [6 7]</span></span><br><span class="line">    print(ll.max(axis=<span class="number">0</span>))  <span class="comment"># axis=0 列最大 [2 4 4 5 7]</span></span><br><span class="line">    print(ll.cumsum())     <span class="comment"># 前所有项的和组成的序列</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># astype: 转换数据类型</span></span><br><span class="line">    ll = ll.astype(dtype=np.float64)</span><br><span class="line">    print(ll.dtype)        <span class="comment"># float64</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># argmax, argmin, argsort</span></span><br><span class="line">    print(ll.argmax(axis=<span class="number">1</span>)) <span class="comment"># axis=1 返回每行的最大数值的索引[4,4]</span></span><br><span class="line">    print(ll.argmin(axis=<span class="number">0</span>)) <span class="comment"># axis=0 返回每列的最小值的索引 [1 1 1 0 1]</span></span><br><span class="line">    print(ll.argsort(axis=<span class="number">0</span>))<span class="comment"># 按行排序返回排序后的索引，未指定排序的列，就是返回所有的列排序的结果</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># all,any,fill,where，diff</span></span><br><span class="line">    <span class="comment"># all()全部满足条件，any()存在满足条件的</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># vstack,hstack</span></span><br><span class="line">    p1 = np.zeros((<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">    p2 = np.ones((<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">    print(<span class="string">"纵向叠加：\n"</span>,np.vstack((p1,p2))) <span class="comment"># 数据拼接，维度不变</span></span><br><span class="line">    print(<span class="string">"横向叠加：\n"</span>,np.hstack((p1,p2))) <span class="comment"># 数据连接，行数不变</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># unique：找出唯一值并返回排序后的结果</span></span><br><span class="line">    print(np.unique(ll))      <span class="comment"># [1. 2. 3. 4. 5. 6. 7.]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    numpy_filed()</span><br><span class="line">    numpy_method()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas
      
    
    </summary>
    
      <category term="Python" scheme="http://nextnight.github.io/categories/Python/"/>
    
      <category term="Numpy" scheme="http://nextnight.github.io/categories/Python/Numpy/"/>
    
    
      <category term="Python" scheme="http://nextnight.github.io/tags/Python/"/>
    
      <category term="Numpy" scheme="http://nextnight.github.io/tags/Numpy/"/>
    
  </entry>
  
  <entry>
    <title>Python代码片段</title>
    <link href="http://nextnight.github.io/2018/09/10/Python%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5/"/>
    <id>http://nextnight.github.io/2018/09/10/Python代码片段/</id>
    <published>2018-09-10T14:03:51.000Z</published>
    <updated>2018-09-29T07:25:46.051Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、多变量赋值"><a href="#1、多变量赋值" class="headerlink" title="1、多变量赋值"></a>1、多变量赋值</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a, b, c, d = <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span></span><br></pre></td></tr></table></figure><h1 id="2、列表赋值"><a href="#2、列表赋值" class="headerlink" title="2、列表赋值"></a>2、列表赋值</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">names = [<span class="string">"ami"</span>, <span class="string">"kimi"</span>, <span class="string">"jsm"</span>]</span><br><span class="line">a, b, c = names</span><br></pre></td></tr></table></figure><h1 id="3、条件表达式"><a href="#3、条件表达式" class="headerlink" title="3、条件表达式"></a>3、条件表达式</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="number">8</span></span><br><span class="line">d = x <span class="keyword">if</span> x &gt; <span class="number">5</span> <span class="keyword">else</span> <span class="number">10</span></span><br></pre></td></tr></table></figure><h1 id="4、列表推导式"><a href="#4、列表推导式" class="headerlink" title="4、列表推导式"></a>4、列表推导式</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>) <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">0</span>]</span><br></pre></td></tr></table></figure><h1 id="5、条件判断-不是用and"><a href="#5、条件判断-不是用and" class="headerlink" title="5、条件判断:不是用and"></a>5、条件判断:不是用and</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="number">90</span></span><br><span class="line"><span class="keyword">if</span> <span class="number">80</span> &lt; x &lt; <span class="number">100</span>: print(x)</span><br></pre></td></tr></table></figure><h1 id="6、判断是否在-不在某列表-字符串"><a href="#6、判断是否在-不在某列表-字符串" class="headerlink" title="6、判断是否在/不在某列表,字符串"></a>6、判断是否在/不在某列表,字符串</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="number">1</span> <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]: print(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="number">1</span> <span class="keyword">not</span> <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]: print(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="string">'1'</span> <span class="keyword">in</span> <span class="string">"123"</span>: print(<span class="number">2</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="string">'1'</span> <span class="keyword">not</span> <span class="keyword">in</span> <span class="string">"123"</span>: print(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h1 id="7、隐含类型转换判空"><a href="#7、隐含类型转换判空" class="headerlink" title="7、隐含类型转换判空"></a>7、隐含类型转换判空</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a, b, c, d = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], &#123;&#125;, <span class="string">''</span>, []</span><br><span class="line"><span class="keyword">if</span> a:</span><br><span class="line">    print(<span class="string">"a not empty"</span>)</span><br><span class="line"><span class="keyword">if</span> b:</span><br><span class="line">    print(<span class="string">"b not empty"</span>)</span><br><span class="line"><span class="keyword">if</span> c:</span><br><span class="line">    print(<span class="string">"c not empty"</span>)</span><br><span class="line"><span class="keyword">if</span> d:</span><br><span class="line">    print(<span class="string">"d not empty"</span>)</span><br></pre></td></tr></table></figure><h1 id="8、判断多个条件是否成立-any，all"><a href="#8、判断多个条件是否成立-any，all" class="headerlink" title="8、判断多个条件是否成立:any，all"></a>8、判断多个条件是否成立:any，all</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a, b, c = <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span></span><br><span class="line"><span class="keyword">if</span> any([a &gt; <span class="number">1</span>, b &lt; <span class="number">2</span>, c == <span class="number">3</span>]): <span class="keyword">pass</span>  <span class="comment"># === a&gt;1 or b&lt;2 or c==3</span></span><br><span class="line"><span class="keyword">if</span> all([a &gt; <span class="number">1</span>, b &lt; <span class="number">2</span>, c == <span class="number">3</span>]): <span class="keyword">pass</span>  <span class="comment"># === a&gt;1 and b&lt;2 and c==3</span></span><br></pre></td></tr></table></figure><h1 id="9、列表推导式-过滤"><a href="#9、列表推导式-过滤" class="headerlink" title="9、列表推导式+过滤"></a>9、列表推导式+过滤</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ls = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="string">"a"</span>, <span class="number">4</span>, <span class="string">"v"</span>, <span class="number">5.5</span>]</span><br><span class="line">rs = [i <span class="keyword">for</span> i <span class="keyword">in</span> ls <span class="keyword">if</span> type(i) <span class="keyword">in</span> [int, float]]</span><br><span class="line">print(rs)</span><br></pre></td></tr></table></figure><h1 id="10、同时获取下标和数据：enumerate"><a href="#10、同时获取下标和数据：enumerate" class="headerlink" title="10、同时获取下标和数据：enumerate"></a>10、同时获取下标和数据：enumerate</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nums = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line"><span class="keyword">for</span> index, num <span class="keyword">in</span> enumerate(nums):</span><br><span class="line">    print(<span class="string">"索引为&#123;&#125;的数据是&#123;&#125;"</span>.format(index, num))</span><br></pre></td></tr></table></figure><h1 id="11、线程sleep"><a href="#11、线程sleep" class="headerlink" title="11、线程sleep"></a>11、线程sleep</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">time.sleep(<span class="number">1</span>)  <span class="comment"># 休眠1秒</span></span><br></pre></td></tr></table></figure><h1 id="12、print-输出覆盖"><a href="#12、print-输出覆盖" class="headerlink" title="12、print 输出覆盖"></a>12、print 输出覆盖</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">i, n = <span class="number">0</span>, <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    time.sleep(<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>: print(i + <span class="number">1</span>, end = <span class="string">'\r'</span>)</span><br></pre></td></tr></table></figure><h1 id="13、lambda匿名函数"><a href="#13、lambda匿名函数" class="headerlink" title="13、lambda匿名函数"></a>13、lambda匿名函数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">names = [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'xxx'</span>, <span class="string">'vx'</span>, <span class="string">'ccc'</span>]</span><br><span class="line">rs = filter(<span class="keyword">lambda</span> x:len(x) &lt;= <span class="number">1</span>, names)</span><br><span class="line">print(list(rs))  <span class="comment"># ['a', 'b']</span></span><br></pre></td></tr></table></figure><h1 id="14、yield生成器收集系列值，不需要return"><a href="#14、yield生成器收集系列值，不需要return" class="headerlink" title="14、yield生成器收集系列值，不需要return"></a>14、yield生成器收集系列值，不需要return</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span><span class="params">()</span>:</span></span><br><span class="line">    a = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        a += i</span><br><span class="line">        <span class="keyword">yield</span> a</span><br><span class="line"><span class="comment"># [0, 1, 3, 6, 10, 15, 21, 28, 36, 45]</span></span><br><span class="line">print(list(fun()))</span><br></pre></td></tr></table></figure><h1 id="15、装饰器给函数添加插入日志，性能测试等非核心功能"><a href="#15、装饰器给函数添加插入日志，性能测试等非核心功能" class="headerlink" title="15、装饰器给函数添加插入日志，性能测试等非核心功能"></a>15、装饰器给函数添加插入日志，性能测试等非核心功能</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runtime</span><span class="params">(func)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">          start = time.time()</span><br><span class="line">          result = func(*args, **kwargs)</span><br><span class="line">          end = time.time()</span><br><span class="line">          print(<span class="string">"&#123;&#125; is called,used &#123;&#125;s."</span>.format(func.__name__, start - end))</span><br><span class="line">          <span class="keyword">return</span> result</span><br><span class="line">    <span class="keyword">return</span> (wrapper)</span><br><span class="line"></span><br><span class="line"><span class="meta">@runtime</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">()</span>:</span></span><br><span class="line">    s = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">          time.sleep(<span class="number">1</span>)</span><br><span class="line">          s += i</span><br><span class="line">process()</span><br></pre></td></tr></table></figure><h1 id="16、内存拷贝"><a href="#16、内存拷贝" class="headerlink" title="16、内存拷贝"></a>16、内存拷贝</h1><p>使用：<code>copy</code>包中的<code>copy()函数</code>和<code>deepcopy()</code>函数</p><blockquote><p>16.1：赋值：指向同一块地址</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = &#123;<span class="number">1</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]&#125;</span><br><span class="line">print(<span class="string">'a的内存地址 %s'</span> % id(a))      <span class="comment"># 4386109840</span></span><br><span class="line">print(<span class="string">'a1的内存地址 %s'</span> % id(a[<span class="number">1</span>]))  <span class="comment"># 4391711560</span></span><br><span class="line">b = a</span><br></pre></td></tr></table></figure><blockquote><p>16.2：浅拷贝：指向不同的引用，但是不同引用指向相同内容(只拷贝对象，但不拷贝对象内部的对象)</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">b = copy.copy(a)</span><br><span class="line">print(<span class="string">'a的内存地址 %s'</span> % id(a))      <span class="comment"># 4386109840</span></span><br><span class="line">print(<span class="string">'b的内存地址 %s'</span> % id(b))      <span class="comment"># 4386110200</span></span><br><span class="line">print(<span class="string">'a1的内存地址 %s'</span> % id(a[<span class="number">1</span>]))  <span class="comment"># 4391711560</span></span><br><span class="line">print(<span class="string">'b1的内存地址 %s'</span> % id(b[<span class="number">1</span>]))  <span class="comment"># 4391711560</span></span><br></pre></td></tr></table></figure><blockquote><p>16.3：深拷贝：对象及对象内部的对象都复制一份</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">b = copy.deepcopy(a)</span><br><span class="line">print(<span class="string">'a的内存地址 %s'</span> % id(a))      <span class="comment"># 4386109840</span></span><br><span class="line">print(<span class="string">'b的内存地址 %s'</span> % id(b))      <span class="comment"># 4391729264</span></span><br><span class="line">print(<span class="string">'a1的内存地址 %s'</span> % id(a[<span class="number">1</span>]))  <span class="comment"># 4391711560</span></span><br><span class="line">print(<span class="string">'b1的内存地址 %s'</span> % id(b[<span class="number">1</span>]))  <span class="comment"># 4391711368</span></span><br></pre></td></tr></table></figure><h2 id="17、参数传递"><a href="#17、参数传递" class="headerlink" title="17、参数传递"></a>17、参数传递</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1、多变量赋值&quot;&gt;&lt;a href=&quot;#1、多变量赋值&quot; class=&quot;headerlink&quot; title=&quot;1、多变量赋值&quot;&gt;&lt;/a&gt;1、多变量赋值&lt;/h1&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class
      
    
    </summary>
    
      <category term="Python" scheme="http://nextnight.github.io/categories/Python/"/>
    
    
      <category term="Python" scheme="http://nextnight.github.io/tags/Python/"/>
    
  </entry>
  
</feed>
