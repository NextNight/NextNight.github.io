{"meta":{"title":"Kala","subtitle":"Do Best Kala","description":"DeepLearn MeachineLearn","author":"Best Kala","url":"http://nextnight.github.io"},"pages":[{"title":"关于","date":"2018-09-29T08:24:57.543Z","updated":"2018-09-29T08:24:57.543Z","comments":false,"path":"about/index.html","permalink":"http://nextnight.github.io/about/index.html","excerpt":"","text":""},{"title":"书单","date":"2018-09-28T02:12:26.198Z","updated":"2018-07-27T09:13:16.681Z","comments":false,"path":"books/index.html","permalink":"http://nextnight.github.io/books/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2018-09-28T02:12:26.198Z","updated":"2018-07-27T09:13:16.682Z","comments":true,"path":"links/index.html","permalink":"http://nextnight.github.io/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2018-09-28T02:12:26.198Z","updated":"2018-07-27T09:13:16.682Z","comments":false,"path":"repository/index.html","permalink":"http://nextnight.github.io/repository/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-09-29T08:24:35.226Z","updated":"2018-09-29T08:24:35.226Z","comments":false,"path":"tags/index.html","permalink":"http://nextnight.github.io/tags/index.html","excerpt":"","text":""},{"title":"分类","date":"2018-09-28T02:12:26.198Z","updated":"2018-07-27T09:13:16.681Z","comments":false,"path":"categories/index.html","permalink":"http://nextnight.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Match-文本数据操作","slug":"Match-文本数据操作","date":"2018-09-30T03:58:57.000Z","updated":"2018-09-30T10:18:52.595Z","comments":true,"path":"2018/09/30/Match-文本数据操作/","link":"","permalink":"http://nextnight.github.io/2018/09/30/Match-文本数据操作/","excerpt":"","text":"这里主要是记录文本相关的比赛中积累的代码片段和操作，量变产生质变，每一次探索和积累都是一小步的成长，不求走的多快，多远，我一直在行走，在努力。 文本数据的处理主要是分词，去停用词(停用词中包括各种特殊符号以及对于构造向量无关的词)，向量化（词袋模型，词嵌入），词嵌入涉及到的词向量的训练。得到向量化的数据之后基本就同结构化数据一样操作了。 12# 导入数据，方便后续操作,数据其中一列为sentence,即文本内容train = pd.read_csv(\"data/train.csv\",index=None, encoding='utf-8') 1、数据查看 文本数据一般会查看数据的长度，确定是长文本还是短文本，我们还可以分析文本对于固定模式的首尾相同的数据进行首尾数据额删除，不过至于删除多少需要测试，首尾多长的文本会是相同的内容，可以采用编辑距离计算一个平均距离，选取不同的长度分别得到平均距离，可以最终选择一个平均距离最短对应的长度作为我们要删除的文本长度。 1234567891011def check_date()： # 文本为字符串 train['len'] = train['sentence'].apply(lambda x:len(x)) train['len'].describe() # 文本已经切分 train['len'] = train['sentence'].apply(lambda x:len(x.split(' '))) train['len'].describe() # 文章长度分箱 bins = [0, 100, 500, 1000, 2000, 5000, 100000] cats = pd.cut(train['len'], bins=bins,labels=None) print(pd.value_counts(cats)) 2、加载停用词 文本处理的基本操作，这里可以直接将得到的停用词列表作为全局变量，否则对每条数据分词的时候都需要读取一遍。 12345678def stop_words_list(path): \"\"\" @ func 读取停用词列表 @ param path:停用词路径 停用词为每一行一个词存储 \"\"\" stop_words = [line.strip() for line in open(path, 'r', encoding='utf-8').readlines()] return stop_words 3、分词去停用词 遍历所有数据，对每一条进行如下的分词去停用词操作。df[&#39;senctence&#39;].apply(lambda x: seg_sentence_all(x)) 1234567891011121314def seg_sentence_all(sentence): \"\"\" 对句子进行分词,去停用词 \"\"\" path = \"data/stop_word_all.txt\" sentence_seged = jieba.cut(sentence.strip()) stop_words = stop_words_list(path) # 这里加载停用词的路径 cut_s = '' for word in sentence_seged: if word not in stop_words: if word != '\\t': str += word.strip() str += ' ' return str.strip() 4、多类别共现词 在文本分类中，多个类别中频繁多次出现的词可能是停用词，也可能是标点符号，对于脱敏数据，我们无法查看到原始数据，无法确定停用词的时候可以使用如下方法来找出一部分贡献词作为停用词。 同样可以对所有分词后的数据做词的交集，更加精确粗暴。 1234567891011121314151617181920212223def check_stop_word(): # 为每个类构建词袋 if not os.path.exists('bowlist.csv'): column = 'word_seg' train = pd.read_csv(path_train, encoding='utf-8', sep=',') bowlist = [] for i in np.arange(1, 20, 1): st = train[train['class'] == i] cv = CountVectorizer(ngram_range=(1, 1), lowercase=False, max_df=0.9, min_df=2) cv.fit(st[column].values) vocal = cv.get_feature_names() vst = set(vocal) bowlist.extend(list(vst)) df = pd.DataFrame(bowlist) df.to_csv(\"bowlist.csv\", index=False) # 查看在多个类中出现的词 else: df = pd.read_csv(\"bowlist.csv\") print(df) dt = pd.DataFrame(df['0'].value_counts().reset_index()) dt.columns = ['word', 'count'] # 19个类别，如果在多个类别都出现则可作为停用词 dt[dt['count'] &gt; 12]['word'].to_csv('stop_word.csv', encoding='utf-8', index=None, header=None) 5、Tf-Idf向量化 TF-Idf传统文本处理的常用向量化操作 【他表示的是一个词的重要程度】。TF:表示词频，一个词在一篇文档中出现的频率，越大表示越重要，越具备代表性。但是它同时在很多文档中都出现了，那么他就不具备代表性。所有重要性与频率正比，与出现的文档数成反比，因此使用逆文档频率Idf表示降低频率的影响。Idf=lg(N/n)N:表示文档总数，n表示出现该词的文档数。TF=count/len(doc),count:词在文档doc出现次数,len(doc):表示文档词数。 5.1、sklearn中的tf-idf 重要的参数： - stop_words：可以传递一个停用词列表到这用于构建向量的时候去除停用词 - ngram_range:也就是n-gram,N元词型合适的n-gram对结果有很大的帮助 - lowercase：False,强转小写，默认True，但是对于中文会报错。 12345678910def TfidfVec(dt_train, dt_test): Tfidf = TfidfVectorizer(min_df=2, max_df=0.9, use_idf=1, smooth_idf=1, sublinear_tf=1, ngram_range=(1, 3), stop_words=load_stopws(),lowercase=False) Tfidf.fit(dt_train['article']) print(Tfidf.max_features) train_word_vec = Tfidf.transform(dt_train['article']) test_word_vec = Tfidf.transform(dt_test['article']) return train_word_vec, test_word_vec 5.2、Gensim中的tf-idf Gensim是一个非常常用的NLP库，里面包含各种在官方基础上封装的库翻遍我们使用，包括常用的Word2Vec,LDA… 12345678910111213141516def Gensim_Tfidf(): # 生成语料 sentences = [\"我喜欢北京三点的太阳\", \"番茄炒西红柿是我的最爱\", \"我不喜欢今天雾霾的西安\"] words = [] for doc in sentences: words.append(list(jieba.cut(doc))) print(words) # 生成词典 dic = corpora.Dictionary(words) # 生成语料库(每一个文档生成(词,出现的次数))的列表=即一个词频向量的稀疏表示 cps = [dic.doc2bow(text) for text in words] # 训练tfidf模型 tfidf = TfidfModel(corpus=cps) # 转换词频向量为tfidf向量 cps_tfidf = tfidf[cps] return cps_tfidf 6、LSA向量化 LSA,也可以叫做LSI,浅语义模型，是一种基于TF或者TF-IDF的降维手段，同时也是描述词语之间的语义关系的语义模型，它认为词和文档的共现矩阵当中存在着词之间语义关系，他们具有一定的相似性，构建词-文档的共现矩阵(通常是词频矩阵或TF-IDF权重矩阵)在进行SVD矩阵分解，在这个稠密的低维空间中。任意两个行向量即两个词的向量求相似度可以表示语义关系。 Gensim中已经集成LSA的模型可以直接调用。直接将cps或者cps_tfidf喂给LsiModel就可以得到降维后的数据，可用与分类，聚类。做相似度计算。 123456789101112def train_lsi(cps, dic): \"\"\" LSI：浅语义分析 \"\"\" print(\"\\n=================LSI=================\") lsi = LsiModel(corpus=cps, num_topics=2, id2word=dic) # 输出训练得到的主题 tp = lsi.print_topics(num_topics=2) # 查看每个文档的LSI主题分布 cps_lsi = lsi[cps] return cps_lsi 7、LDA向量化 LDA,又称主题模型，它认为在词和文档之间存在摸个隐变量 [主题] ，每个主题由多个词构成且每个词有一定的概率出现。而一篇文档可以由多个主题构成，且每个主题同样有一定的概率。那么如何得到一篇文档呢?我们知道这篇文档的主题分布，只需要以这个概率分布不断的去选择主题，再分别为每个主题按照主题-词的概率分布去选择词，迭代稳定后就可以的得到这篇文档。所以说我们可以将一篇用词表示的文档，用主题的概率分布来表示。表示之后的文档向量是一个稠密的低维向量，包含了某种语义联系，同时又进行了降维。 Gensim中同样进行了LDA的实现，同上，将cps或者cps_tfidf喂给LdaModel就可以得到降维后的数据。可用与分类，聚类，做相似度计算。 123456789101112def train_lda(cps,dic): \"\"\" LDA:主题模型 \"\"\" print(\"\\n=================LDA=================\") lda = LdaModel(corpus=cps, num_topics=2, id2word=dic) # 输出五个主题 tp = lda.print_topics(num_topics=2) # 输出每个文档的主题 cps_lda = lda[cps] return cps_lda 8、Word2vec词向量 词向量表示是的是一个词的向量化表示，one-hot，tf,tf-idf都是一种词向量表示法，而word2vec也是一种词向量表示法，他通过训练一个词的上下文表示以一个词的概率这种深度学习分类模型来得到一组隐层权重来表示一个词，这个权重就是词向量它能够表示更多的语义信息和语序信息。 以下是Gensim包中的word2vec词向量训练，主要参数 windows:上下文词的个数 size:需要训练的词向量的维度 影响词向量的主要参数就是这两个，关于word2vec和其他参数，其他地方在做介绍。 8.1、Gensim.word2vec训练词向量123456789from gensim.models.word2vec import Word2Vecdef word2vec_train(): \"\"\"训练词向量\"\"\" train = pd.read_csv(path_train, encoding='utf-8', sep=',') # train['word_seg']表示分词后的列 model = Word2Vec(train['word_seg'].values, window=5, sg=0, size=200, min_count=1, negative=3, sample=0.001, hs=1, workers=4,cbow_mean=1) model.wv.save_word2vec_format(\"model.ve\", binary=False) 可以通过修改以上两个参数，得到不同的词向量选择质量高的，或者如下同时训练多个词向量 8.2、同时训练多个窗口的词向量12345for i in [4, 5, 6]: model = Word2Vec(data_all, window=i, sg=0, size=100, min_count=1, negative=3, sample=0.001, hs=1, workers=4,cbow_mean=1) model.save(os.path.join(mc.data_path, 'model.model')) model.wv.save_word2vec_format(os.path.join(mc.data_path, \"model.ve\" + str(i)), binary=False) 8.3、构建词向量字典 训练得到的词向量是一个文件，第一行有词的个数，第二行起每一行第一列是词，其他列对应的是词向量，以空格分割，要想把数据中的词对应上还需要建立词典。 12345678def build_embeddings_dict(): embeddings_dict = &#123;&#125; with open(os.path.join(mc.data_path, 'model.ve'), encoding='utf-8') as f: for line in f: values = line.split(' ') coefs = np.asarray(values[1:], dtype='float64') embeddings_dict[values[0]] = coefs return embeddings_dict 8.4、词向量构建文档 一条数据寄一个文档，一个文档有10个词，每一个词是一个100维的词向量，那么这个文档就可以用word_count*100的矩阵表示。当然我们也可以通过把每一个词的词向量加和求平均值，这样一篇文档就是一个100维的向量了。 文档向量=词向量加和平均 1234567891011def word_embedding_vector(s, embeddings_dict): \"\"\" 词嵌入构建句子向量 :return: \"\"\" svt = np.zeros(100, dtype='float64') for word in s.split(' '): # ==x.split(' ') word_v = embeddings_dict.get(word) if word_v is not None: svt += word_v return svt 文档向量=文档中所有词的词向量组成的矩阵 123456789EMBEDDING_DIM = 300nb_words = min(MAX_NB_WORDS,len(word_index))word_embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))for word, i in word_index.items(): if i &gt; MAX_NB_WORDS: continue embedding_vector = embeddings_index.get(str(word)) if embedding_vector is not None: word_embedding_matrix[i] = embedding_vector 对每一个文档获取如上的矩阵，那么着呢哥哥训练集，就是一个三维额矩阵。Text-cnn采用的就是这种结构。","categories":[{"name":"Match","slug":"Match","permalink":"http://nextnight.github.io/categories/Match/"},{"name":"NLP","slug":"Match/NLP","permalink":"http://nextnight.github.io/categories/Match/NLP/"}],"tags":[{"name":"文本处理","slug":"文本处理","permalink":"http://nextnight.github.io/tags/文本处理/"}]},{"title":"SNA-社交网络分析","slug":"SNA-社交网络分析","date":"2018-09-29T07:32:02.000Z","updated":"2018-09-30T05:39:36.796Z","comments":true,"path":"2018/09/29/SNA-社交网络分析/","link":"","permalink":"http://nextnight.github.io/2018/09/29/SNA-社交网络分析/","excerpt":"","text":"","categories":[{"name":"class1","slug":"class1","permalink":"http://nextnight.github.io/categories/class1/"},{"name":"class2","slug":"class1/class2","permalink":"http://nextnight.github.io/categories/class1/class2/"}],"tags":[{"name":"tg1","slug":"tg1","permalink":"http://nextnight.github.io/tags/tg1/"},{"name":"tag2","slug":"tag2","permalink":"http://nextnight.github.io/tags/tag2/"}]},{"title":"ML-DTree决策树","slug":"ML-DTree决策树","date":"2018-09-29T07:16:44.000Z","updated":"2018-09-29T07:18:02.425Z","comments":true,"path":"2018/09/29/ML-DTree决策树/","link":"","permalink":"http://nextnight.github.io/2018/09/29/ML-DTree决策树/","excerpt":"","text":"","categories":[{"name":"ML","slug":"ML","permalink":"http://nextnight.github.io/categories/ML/"}],"tags":[{"name":"决策树","slug":"决策树","permalink":"http://nextnight.github.io/tags/决策树/"}]},{"title":"ML-Svm支持向量机","slug":"ML-Svm支持向量机","date":"2018-09-29T07:15:10.000Z","updated":"2018-09-29T07:17:46.640Z","comments":true,"path":"2018/09/29/ML-Svm支持向量机/","link":"","permalink":"http://nextnight.github.io/2018/09/29/ML-Svm支持向量机/","excerpt":"","text":"","categories":[{"name":"ML","slug":"ML","permalink":"http://nextnight.github.io/categories/ML/"}],"tags":[{"name":"支持向量机","slug":"支持向量机","permalink":"http://nextnight.github.io/tags/支持向量机/"},{"name":"svm","slug":"svm","permalink":"http://nextnight.github.io/tags/svm/"}]},{"title":"ML-Logistic回归","slug":"ML-Logistic回归","date":"2018-09-29T07:13:53.000Z","updated":"2018-10-10T03:08:37.637Z","comments":true,"path":"2018/09/29/ML-Logistic回归/","link":"","permalink":"http://nextnight.github.io/2018/09/29/ML-Logistic回归/","excerpt":"","text":"MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)'],['\\\\','\\\\']]} }); Logist ① 什么是逻辑回归？ 逻辑回归是一种分类算法，它衍生于线性回归，我们都知道线性回归使用一条线去拟合数据，通常情况x和y没有界限。而我们可以通过取阈值来将这条线划分为两个部分，比如说&gt;0.5这部分设为1，小于0.5为0，这样就将数据划分为了两类。但是线性回归的数据通常不稳定，我们需要不断地去调整这个阈值才能将其分为两个想要的类别，因此逻辑回归诞生了。 逻辑回归在线性回归的基础上加上了一个阶跃函数，这个阶跃函数将无限区间的数值映射到了[【0,1】]这个区间之内。从阶跃函数sigmod的图像可以看出，当x到达0.5的时候，函数值y会发生突变。[同时他也表示是的函数值y为1的概率]P(y=1|x). ② 为什么函数值y表示的是类别为1的概率？ 1、假设有一个线性函数：${\\color{DarkGreen} z=wx+b}$ ,sigmod函数：${\\color{DarkGreen} y = \\frac{1}{1+e^{-z}}}$ , 2、假设p(y=1|x):表示y=1的概率，那么1-p就表示y=0的概率， 3、则对数几率：$ {\\color{DarkGreen} ln(\\frac{p}{1-p})}$ ,我们用这个对数几率ods去拟合线性函数 $ {\\color{DarkGreen} ln(\\frac{p}{1-p})=z}$ 可推导如下： $$\\begin{aligned}&amp;\\Rightarrow {\\color{DarkGreen} ln(\\frac{p}{1-p})=z}\\&amp;\\Rightarrow {\\color{DarkGreen} e^{z}=\\frac{p}{1-p}}\\&amp;\\Rightarrow {\\color{DarkGreen} e^{-z}=\\frac{1-p}{p}}\\&amp;\\Rightarrow {\\color{DarkGreen} e^{-z}=\\frac{1}{p}-1}\\&amp;\\Rightarrow {\\color{DarkGreen} e^{-z}+1=\\frac{1}{p}}\\&amp;\\Rightarrow {\\color{DarkGreen} p=\\frac{1}{e^{-z}+1}}\\\\end{aligned}$$ 如上可得:${\\color{DarkGreen} y=p}$ 则说明函数值y确实表示是概率值。 ③ 逻辑回归的损失函数是什么？ 1、logist的模型函数：${\\color{DarkGreen} h(z)=\\frac{1}{1+e^{-z}},(z=wx+b)}$ 2、假设使用最小二乘法OLS:cross function:$ {\\color{DarkGreen} J(w) = \\dfrac{1}{2}\\sum_{i}^N(h(z)^{(i)}) - y^{(i)})^2}$ 。得到的是一个非凸函数，有很多的局部极值，不利于解的最优值。 3、极大似然估计MLE来描述:既然 $h(z)$ 描述的是类别为1的概率, 类别为1的概率：${\\color{DarkGreen}p(y=1|x,w)= h(z)}$ 类别为0的概率：${\\color{DarkGreen}p(y=0|x,w)=1-h(z) }$ 将以上两式通用式：${\\color{DarkGreen}p(y|x,w)=h(z)^{y^i}.(1-h(z)^{1-y^i})}$ MLE:$$\\begin{aligned}L(w)\\&amp;=\\prod_{i=1}^n{p(y^i|x^i,w)}\\\\&amp;=\\prod_{i=1}^n{h(z)^{y^i}.(1-h(z))^{1-y^i}}\\\\end{aligned}$$ 简化MLE：取对数似然,概率积-&gt;概率和方便计算。$$\\begin{aligned}L(w)&amp;=\\prod_{i=1}^n{p(y^i|x^i,w)}\\&amp;=\\prod_{i=1}^n{h(z)^{y^i}.(1-h(z))^{1-y^i}}\\ln(L(w))&amp;=\\sum_{i=1}^{n}(y^iln(h(z)+(1-y^i)ln(1-h(z)))\\end{aligned}$$ 负对数似然：给以上的 $ln(L(w))$ 取负，得到负对数似然，对数似然有极大值，负对数似然有极小值，损失函数即是找到一个关于误差最小的函数则Logist的损失函数为：$$J(w) = -ln(L(w))=- \\sum_{i=1}^{n}(y^iln(h(z)+(1-y^i)ln(1-h(z)))$$ ④ 逻辑回归损失函数的图像？ 1、分别对损失函数y取值0,1得到如下函数： $$J(h(z),y;w)=\\begin{cases} -ln(h(z)) &amp; if\\quad y=1 \\ -ln(1-h(z)) &amp; if\\quad y=0 \\end{cases}$$ 2、得到图像如下： ⑤ 逻辑回归如何求解？ 1、逻辑回归求解的是最优的参数集，即使得误差最小的参数集合。2、逻辑回归使用梯度下降求解最优参数： 梯度的负方向是损失函数下降最快的方向，我们的目标是最小化损失函数，所以要沿梯度的方向不断地去修改参数W的值使得J(w)损失函数值不断减小，到达最低点，此时的参数w就是最优的。为什么梯度负方向是损失函数下降最快的方向？因为梯度方向是函数值增长最快的方向。方向导数：就是多元函数关于某一个维度的偏导数，而关于某一点的方向导数有无穷个，表示函数在各个方向上的增长速度。那么如何描述函数在这个点上增长最快的方向呢？？引入[ - 梯度- ] 的概念,梯度是函数在这个点上的各个方向导数的矢量和，即是各个偏导数构成的向量。 ⑥ SGD梯度下降如何求解？ 1、损失函数求偏导即梯度：$$\\begin{aligned}\\bigtriangledown =\\frac {\\partial J(w)}{\\partial w_j}&amp;= {-ln(L(w))}’ \\&amp;=-\\sum_ {i=1}^{n}({yln(h(z))+(1-y)ln(1-h(z)))}’ \\&amp;=-\\sum_ {i=1}^{n} y\\frac{1}{h(z)}{h(z)}’+(1-y)\\frac{1}{1-h(z)}{(1-h(z))}’ \\对h(z)求偏导：\\{h(z)}’ &amp;={ \\frac{1}{1+e^{-wx}}}’\\&amp;= {(1+e^{-wx})^{-1}}’\\&amp;= (1+e^{-wx})^{-2} {(1+e^{-wx})}’\\&amp;= (1+e^{-wx})^{-2}e^{-wx}{(-wx)}’\\&amp;= - (1+e^{-wx})^{-2}e^{-wx}{(wx)}’\\&amp;= -\\frac{e^{-wx}}{(1+e^{-wx})^2} {(wx)}’\\&amp;= - \\frac{1}{(1+e^{-wx})}\\frac{e^{-wx}}{(1+e^{-wx})} {(wx)}’\\&amp;= - \\frac{1}{(1+e^{-wx})}(1- \\frac{1}{(1+e^{-wx})}){(wx)}’\\&amp;= -h(z)(1-h(z)){(wx)}’\\对1-h(z)求偏导：\\(1-{h(z)})’ &amp;=-{h(z)})’\\&amp;=h(z)(1-h(z)){(wx)}’\\\\\\bigtriangledown =\\frac {\\partial J(w)}{\\partial w_j}&amp;= {-ln(L(w))}’ \\&amp;=-\\sum_ {i=1}^{n} y\\frac{1}{h(z)}(-h(z)(1-h(z)){(wx)}’)+(1-y)\\frac{1}{1-h(z)}(h(z)(1-h(z)){(wx)}’)\\&amp;=\\sum_ {i=1}^{n} y\\frac{1}{h(z)}(h(z)(1-h(z)){(wx)}’)-(1-y)\\frac{1}{1-h(z)}(h(z)(1-h(z)){(wx)}’)\\&amp;=\\sum_ {i=1}^{n}[y(1-h(z))-(1-y)h(z)]{(wx)}’\\&amp;=\\sum_ {i=1}^{n}[y-yh(z)-h(z)+yh(z)]{(wx)}’\\&amp;=\\sum_ {i=1}^{n}(y-h(z)){(wx)}’\\&amp;=\\sum_ {i=1}^{n}(y-h(z))x_j\\&amp;=(y-h(z))x_j\\\\bigtriangledown =\\frac {\\partial J(w)}{\\partial w_j}&amp;=\\sum_ {i=1}^{n}(y^i-h(z)^i){x_j}^i\\其中i表示第i条数据，j表示第j维\\\\end{aligned}$$ 2、上面得到了关于$W_j$ 的梯度$\\bigtriangledown$ ,迭代更新W,W的更新公式：$W_j=W_j+\\alpha\\bigtriangledown$ α表示学习率，3、批量梯度下降：123# 数据有：i行，j列，所有要更新j个w,每个w可以选择用多少条数据取更新它，默认是全量数据。即批量更新。for data_i in data_nums: wi = wi + α*梯度 简单来说：$\\overrightarrow { w}= (\\overrightarrow { y } -\\overrightarrow { h(z) } )\\overrightarrow { x }$ 一次得到所有的W更新。","categories":[{"name":"ML","slug":"ML","permalink":"http://nextnight.github.io/categories/ML/"}],"tags":[{"name":"逻辑回归","slug":"逻辑回归","permalink":"http://nextnight.github.io/tags/逻辑回归/"}]},{"title":"Match-DaGuanCup","slug":"Match-DaGuanCup","date":"2018-09-29T07:02:57.000Z","updated":"2018-09-29T07:08:18.192Z","comments":true,"path":"2018/09/29/Match-DaGuanCup/","link":"","permalink":"http://nextnight.github.io/2018/09/29/Match-DaGuanCup/","excerpt":"","text":"","categories":[{"name":"Match","slug":"Match","permalink":"http://nextnight.github.io/categories/Match/"}],"tags":[{"name":"文本分类","slug":"文本分类","permalink":"http://nextnight.github.io/tags/文本分类/"}]},{"title":"Match-LianTong","slug":"Match-LianTong","date":"2018-09-29T06:46:00.000Z","updated":"2018-09-29T08:39:01.759Z","comments":true,"path":"2018/09/29/Match-LianTong/","link":"","permalink":"http://nextnight.github.io/2018/09/29/Match-LianTong/","excerpt":"","text":"","categories":[{"name":"Match","slug":"Match","permalink":"http://nextnight.github.io/categories/Match/"}],"tags":[{"name":"分类推荐","slug":"分类推荐","permalink":"http://nextnight.github.io/tags/分类推荐/"}]},{"title":"Match-YunYiCup","slug":"Match-Yunyicup","date":"2018-09-29T06:42:48.000Z","updated":"2018-09-29T07:07:08.749Z","comments":true,"path":"2018/09/29/Match-Yunyicup/","link":"","permalink":"http://nextnight.github.io/2018/09/29/Match-Yunyicup/","excerpt":"","text":"","categories":[{"name":"Match","slug":"Match","permalink":"http://nextnight.github.io/categories/Match/"}],"tags":[{"name":"文本分类","slug":"文本分类","permalink":"http://nextnight.github.io/tags/文本分类/"}]},{"title":"Graph-BayesNetWork","slug":"Graph-BayesNetWork","date":"2018-09-29T06:40:58.000Z","updated":"2018-09-29T07:35:49.614Z","comments":true,"path":"2018/09/29/Graph-BayesNetWork/","link":"","permalink":"http://nextnight.github.io/2018/09/29/Graph-BayesNetWork/","excerpt":"","text":"","categories":[{"name":"概率图模型","slug":"概率图模型","permalink":"http://nextnight.github.io/categories/概率图模型/"}],"tags":[{"name":"BayesNetWork","slug":"BayesNetWork","permalink":"http://nextnight.github.io/tags/BayesNetWork/"},{"name":"贝叶斯网络","slug":"贝叶斯网络","permalink":"http://nextnight.github.io/tags/贝叶斯网络/"}]},{"title":"Graph-CRF条件随机场","slug":"Graph-CRF条件随机场","date":"2018-09-29T06:30:53.000Z","updated":"2018-09-29T07:35:18.776Z","comments":true,"path":"2018/09/29/Graph-CRF条件随机场/","link":"","permalink":"http://nextnight.github.io/2018/09/29/Graph-CRF条件随机场/","excerpt":"","text":"","categories":[{"name":"概率图模型","slug":"概率图模型","permalink":"http://nextnight.github.io/categories/概率图模型/"}],"tags":[{"name":"CRF","slug":"CRF","permalink":"http://nextnight.github.io/tags/CRF/"},{"name":"条件随机场","slug":"条件随机场","permalink":"http://nextnight.github.io/tags/条件随机场/"}]},{"title":"Graph-HMM隐马尔科夫","slug":"Graph-HMM隐马尔科夫","date":"2018-09-29T06:30:37.000Z","updated":"2018-09-29T07:34:54.564Z","comments":true,"path":"2018/09/29/Graph-HMM隐马尔科夫/","link":"","permalink":"http://nextnight.github.io/2018/09/29/Graph-HMM隐马尔科夫/","excerpt":"","text":"","categories":[{"name":"概率图模型","slug":"概率图模型","permalink":"http://nextnight.github.io/categories/概率图模型/"}],"tags":[{"name":"HMM","slug":"HMM","permalink":"http://nextnight.github.io/tags/HMM/"}]},{"title":"Math-矩阵分解","slug":"Math-矩阵分解","date":"2018-09-29T06:27:30.000Z","updated":"2018-09-29T07:29:18.171Z","comments":true,"path":"2018/09/29/Math-矩阵分解/","link":"","permalink":"http://nextnight.github.io/2018/09/29/Math-矩阵分解/","excerpt":"","text":"","categories":[{"name":"Math","slug":"Math","permalink":"http://nextnight.github.io/categories/Math/"}],"tags":[{"name":"矩阵分解","slug":"矩阵分解","permalink":"http://nextnight.github.io/tags/矩阵分解/"}]},{"title":"Math-凸优化问题","slug":"Math-凸优化问题","date":"2018-09-29T06:25:00.000Z","updated":"2018-09-29T07:18:50.840Z","comments":true,"path":"2018/09/29/Math-凸优化问题/","link":"","permalink":"http://nextnight.github.io/2018/09/29/Math-凸优化问题/","excerpt":"","text":"","categories":[{"name":"凸优化","slug":"凸优化","permalink":"http://nextnight.github.io/categories/凸优化/"}],"tags":[{"name":"凸优化","slug":"凸优化","permalink":"http://nextnight.github.io/tags/凸优化/"}]},{"title":"Math-泰勒展式","slug":"Math-泰勒展式","date":"2018-09-29T06:21:53.000Z","updated":"2018-09-29T07:29:40.308Z","comments":true,"path":"2018/09/29/Math-泰勒展式/","link":"","permalink":"http://nextnight.github.io/2018/09/29/Math-泰勒展式/","excerpt":"","text":"","categories":[{"name":"Math","slug":"Math","permalink":"http://nextnight.github.io/categories/Math/"}],"tags":[{"name":"泰勒展式","slug":"泰勒展式","permalink":"http://nextnight.github.io/tags/泰勒展式/"}]},{"title":"Math-极大似然估计","slug":"Math-极大似然估计","date":"2018-09-29T06:21:27.000Z","updated":"2018-09-29T07:18:26.798Z","comments":true,"path":"2018/09/29/Math-极大似然估计/","link":"","permalink":"http://nextnight.github.io/2018/09/29/Math-极大似然估计/","excerpt":"","text":"","categories":[{"name":"Math","slug":"Math","permalink":"http://nextnight.github.io/categories/Math/"}],"tags":[{"name":"MLE","slug":"MLE","permalink":"http://nextnight.github.io/tags/MLE/"},{"name":"极大似然估计","slug":"极大似然估计","permalink":"http://nextnight.github.io/tags/极大似然估计/"}]},{"title":"文本处理流程","slug":"文本处理流程","date":"2018-09-29T05:49:41.000Z","updated":"2018-10-10T02:52:29.041Z","comments":true,"path":"2018/09/29/文本处理流程/","link":"","permalink":"http://nextnight.github.io/2018/09/29/文本处理流程/","excerpt":"","text":"MathJax.Hub.Config({tex2jax: {inlineMath: [[‘$’,’$’], [‘\\(‘,’\\)’],[‘\\‘,’\\‘]]}}); \b文本处理的流程文本–&gt;分词–&gt;向量化–&gt;建模 1、分词方法 分词是NLP的基础，无论是文档还是语句都是有基本的词构成，因此分词的好坏直接影响到文本的表示。 基本匹配分词 1234567此方法按照不同的扫描方式，逐个查找词库进行分词。根据扫描方式可细分为： 正向最大匹配 反向最大匹配 双向最大匹配 最小切分(即最短路径)总之就是各种不同的启发规则 全局切分+语言模型分词 1234567- 基本操作： 1、它首先切分出与词库匹配的所有可能的词， 2、再运用统计语言模型决定最优的切分结果。【一般采用viterbi算法寻找找最优的组合】- 优点： 1、在于可以解决分词中的歧义问题。- 示例： 对于文本串“南京市长江大桥”，首先进行词条检索(一般用Trie存储)，找到匹配的所有词条（南京，市，长江，大桥，南京市，长江大桥，市长，江大桥，江大，桥），以词网格(word lattices)形式表示，接着做路径搜索，基于统计语言模型(例如n-gram)找到最优路径，最后可能还需要命名实体识别。 以字构词 1234567891011121314151617181920- 基本操作： 1、将分词问题转化为了单个字的分类问题(序列标签)，为每一个字打上'B','I','E','S'五个标签中的一个。其中'B':一个词的开头，'I':一个词的中间，'E':一个词的结尾，'S':单个字成词。 2、通过统计信息得到每个字的标签的概率，然后采用'Viterbi算法'得到最优的结果。- 优点： 1、具有新词发现功能，准确率较高。- 实现： 1、'HMM分词', 2、'CRF分词', 3、'深度学习分词'- [1] 深度学习分词的流程： -1、将每一个字Lookup Table映射到一个固定长度的特征向量. -2、经过一个标准的神经网络：liner-sigmod-liner三层'同样得到每个字属于B,I,E,S四个Tag的概率'. -3、采用'Viterbi算法'求得最优结果.- [2] HMM分词 VS CRF分词 CRF：目前效果已经优于HMM，无论是新词发现还是实体识别，人名识别。 优点: 1、在于CRF既可以像最大熵模型一样加各种领域feature。 2、又避免了HMM的齐次马尔科夫假设。 2、语言模型： 语言模型是用来计算一句话生成概率的模型， 1、语言模型的演变[1.1] bayes: 12345- 定义：每个词出现的概率取决于前面所有的词- 公式：P(w_m) = p(w_m|w_&#123;m-1&#125;,w_&#123;m-2&#125;..w_1)- P(S)：P(S) = P(w_1)*p(w_2)...p(w_n),n为词或字个数,P(S)表示句子概率- 缺点：计算过于复杂，甚至难以计算- 简化：朴素贝叶斯，条件独立性假设 [1.2] n-gram：n元词型 12345678910- 定义：衍生自HMM，它利用马尔科夫假设，认为句子中每个单词只与其前n–1个单词有关，即假设产生w_m这个词的条件概率只依赖于前n–1个词。其中n越大，模型可区别性越强，n越小，模型可靠性越高。- 公式：p(w_m) = p(w_m|w_&#123;m-n+1&#125;...,w_&#123;m-1&#125;))- P(S)：P(S) = P(w_1)*p(w_2)...p(w_n),n为词或字个数,P(S)表示句子概率- 优点： 1、简单有效，多元词型组合可以得到更丰富的信息 2、考虑了词的位置关系- 不足： 1、没有考虑到词语的相似性 2、没有考虑词法，语法，以及语义信息 3、仍然存在数据稀疏的问题 [1.3] ffnnlm：前馈神经网络语言模型 1234- 定义：基于n-gram的语言模型，他将词\"w_m\"的前n-1个词p(w_&#123;m-n+1&#125;...p(w_&#123;m-1&#125;))映射到词向量空间，然后把它们拼接起来得到一个更大的词向量作为神经网络的输入，'输出'的是p(w_m)- 优点： 1、词语之间的相似性可以通过词向量来体现 2、自带平滑功能 [1.4] rnnlm:循环神经网络语言模型 123456789101112- rnnlm特点：可以存在有向环，将上一次的输出作为本次的输入。- rnnlm和ffnnlm的最大区别是：ffnnmm要求输入的上下文是固定长度的，也就是说n-gram中的 n要求是个固定值，而rnnlm不限制上下文的长度，可以真正充分地利用所有上文信息来预测下一个词，本次预测的中间隐层信息(例如下图中的context信息)可以在下一次预测里循环使用。- 基本结构： 1、输入层： 2、隐藏层： 3、输出层：- 预测步骤： 1、单词w_&#123;m-1&#125;映射到词向量，记作input(t) 2、连接上一次训练的隐藏层context(t–1)，经过sigmoid function，生成当前t时刻的context(t) 3、利用softmax function，预测P(w_m)- 缺点：基于RNN的语言模型利用BPTT(BackPropagation through time)算法比较难于训练，原因就是深度神经网络里比较普遍的梯度消失问题。- 优点：训练精度高 [1.5] lstmlm:长短记忆神经网络 123- lstmlm特点： 1、也是一种RNN网络，通过结构的修改避免了梯度消失。 2、能够记忆更多的信息。 2、N-gram的基本原理 N-gram即N元语法模型，采用马尔科夫假设，认为一个词生成的概率取决于他前面的N-1个词。 123456789- 约定： s:代表句子 w_i:第i个词 w_n:代表第n个词 N:词数- 句子s生成的概率如下： P(s) = p(w_1)p(w_2)p(w_3)...p(w_n)- \"为了避免数据溢出、提高性能，通常会使用取 log 后使用加法运算替代乘法运算\"即：- log(p(s)) = log(p(w_1))+log(p(w_2))+log(p(w_3))...log(p(w_n)) [2.1]、bigram:假设一个词生成的概率、取决于他前面1个词 12345678910- 句子s生成的概率如下： P(s) = p(w_1)p(w_2)p(w_3)...p(w_n)- bigram下词w_m生成的概率： p(w_1) = p(w_1) p(w_2) = p(w_2|w_1) p(w_3) = p(w_3|w_2) ... p(w_m) = p(w_m|w_m-1)- 得到句子生成的概率： P(s) = p(w_1)p(w_2|w_1)p(w_3|w_2)..p(w_m|w_m-1)..p(w_n|w_n-1)- [2.2]、trigram:假设一个词的生成概率取决于他前面的2个词 12345678910- 句子s生成的概率如下： P(s) = p(w_1)p(w_2)p(w_3)...p(w_n)- trigram下词w_m生成的概率： p(w_1) = p(w_1) p(w_2) = p(w_2|w_1) p(w_3) = p(w_3|w_&#123;1,2&#125;) ... p(w_m) = p(w_m|w_&#123;m-2,m-1&#125;)- 得到句子生成的概率： P(s) = p(w_1)p(w_2|w_1)p(w_3|w_1,w_2)..p(w_m|w_m-2,w_m-1)..p(w_n|w_n-1,w_n-2)- 如上知道了如何去计算一个句子生成的概率，那么该如何去算其中的每一项呢？其实不过还是频率来表示概率。 bigram中P(w_m) = p(w_m|w_m-1)=count(w_m,w_m-1)/count(w_m-1)即计算当前词的概率用当前词和前一个词共同出现的次数比上前一个词出现的次数。 [2.3] bigram示例： 1234- 语料： s1：我喜欢西红柿炒鸡蛋 s2：西红柿是我最爱 s3: 我喜欢什么 单词词频： word 我 喜欢 西红柿 炒 鸡蛋 是 最爱 什么 count 2 2 1 1 1 1 1 1 p(word) 0.2 0.2 0.1 0.1 0.1 0.1 0.1 0.1 1234- 句子s1出现的概率： p(s1) = p(我)p(喜欢)p(西红柿)p(炒)p(鸡蛋) = p(我)p(喜欢|我)p(西红柿|喜欢)p(炒|西红柿)p(鸡蛋|炒)- 根据顺序共现得到bigram的共现词频如下： 顺序共现词频： word 我 喜欢 西红柿 炒 鸡蛋 是 最爱 什么 我 2 1 喜欢 1 1 西红柿 1 1 炒 1 鸡蛋 是 1 最爱 什么 1234567- 最后得到句子s1的概率如下： p(s1) = p(我)p(喜欢)p(西红柿)p(炒)p(鸡蛋) = p(我)p(喜欢|我)p(西红柿|喜欢)p(炒|西红柿)p(鸡蛋|炒) = 0.2 * (count(我喜欢)/count(我)) * (count(喜欢西红柿)/count(喜欢)) * (count(西红柿炒)/count(西红柿)) * (count(炒鸡蛋)/count(炒)) 同理可得trigram，n-gram的计算方式，分别去统计N元词出现的次数。得到条件概率。 [2.4]、数据平滑 123456- 数据平滑：大规模数据统计方法与有限的训练语料之间必然产生数据稀疏问题，导致零概率问题。为了解决数据稀疏问题，人们为理论模型实用化而进行了众多尝试与努力，诞生了一系列经典的平滑技术。- 基本思想：“降低已出现 n-gram 的条件概率分布，以使未出现的 n-gram 条件概率分布非零”，且经数据平滑后一定保证概率和为1，- 平滑技术： 1、加一平滑：又称拉普拉斯平滑 Pmle(w_m|w_m-1) = count(w_m,w_m-1)/count(w_m-1) Padd1(w_m|w_m-1) = count(w_m,w_m-1)+1/count(w_m-1)+VV代表2元词型的个数。V个二元词型保证概率和为1。 3、向量化12345678文本的向量化主要有以下几种： [1] countvec：词袋模型，词频表示 [2] Tf-idf：基于bow的权重表示 [3] 主题模型:PLSA，LDA [4] 词嵌入：word2vec,glove,fastext约定： 1、词袋：数据集中所有的的词构成的词集或词典。 2、词频：一篇文档中某个词出现的次数 1、词频向量：countvec12345- 定义：把每一篇文档表示为一个长度为词袋大小的向量，向量的每一个维度表示为词袋中的每一个词在此文档中出现的次数。- 优缺点： 1、能够一定程度反映词频信息，但是对于多篇文档中都出现的高频词没有银锭的区分度， 2、词袋模型向量大多是高维稀疏矩阵。 2、权重向量：tf-idf123456789- 定义：词频(TF)x逆文档词频(IDF)用于表示一个词的权重。- 优点： 1、降低多个文档都出现的高频词汇对重要度的影响。 2、提升低频词汇的重要度。 3、tf-idf+'N-gram'特征一定程度能够反映语序特征- 缺点： 1、权重向量也是高维稀疏矩阵. 2、不能完全反映语序信息 3、不能体现词语间的相似度信息。 3、主题向量：plsa,lda12345678- 定义：通过引入隐变量'主题'将词频向量或者权重向量映射到一个固定维度的稠密向量。- 优点： 1、降维作用，降低了模型的复杂度和计算量。 2、包含了一定的语义信息。- 缺点： 1、主题数难以确定。 2、主题作为一种概率分布表示，难以表述其具体含义，可解释性较差。 3、数据量大的时候主题训练耗时较长。 4、词嵌入向量:word2vec,glove,fasttext12345678- 定义：通过训练将每一个词映射到固定长度的词向量空间中，每个词就是一个点，同时引入距离的概念，就可以描述词语的相似度。- 优点： 1、考虑到了词语的相似度信息。 2、维度固定且低维，便于后续计算。 3、自带平滑功能- 缺点： 1、好的词向量表示依赖于足够的训练预料，尤其是领域性较强的文本依赖于相关的领域文本，否则可能得不到足够好的词向量。 2、词向量的训练需要额外的步骤。且大语料的训练耗时。 Word2Vec12345678910- 模型结构： |--&gt;输入层 |--&gt;隐藏层 |--&gt;softmax- 两种实现方式： 1、cbow 2、skip-gram- 两种优化方式： 1、层次化softmax 2、负采样 🐱目标函数： 123- w:词w- C:语料库- Context(w):词w的上下文信息 cbow:p(w|Context(w))：上下文词出现的情况下，词w出现的概率，即共现的概率。也是优化的目标，最大化这个概率。那么对与整个语料来说，目标函数就是最大化如下的概率积 $$L = \\prod_{w\\epsilon C}p(w|Context(w))$$ skip-gram:以当前词预测上下文词的概率，即目标函数就是最大化当前词出现的时候上下文词出现的概率积。 $$L = \\prod_{w\\epsilon C}p(Context(w)|w)$$ 最大对数似然： $$L = \\sum_{w\\epsilon C}log(p(w|Cotext(w))))$$ $$L = \\sum_{w\\epsilon C}log(p(Context(w)|w))$$ 1234- 训练过程： 1、输入层：随机初始化w的context(w)的每一个词(前后各c个)的词向量V(w)。 2、投影层：将词w的context(w)的所有V(w)求和。 3、输出层：层次化softmax层，一颗二叉树，用计算得到的输出每个词的概率。","categories":[{"name":"ML","slug":"ML","permalink":"http://nextnight.github.io/categories/ML/"},{"name":"NLP","slug":"ML/NLP","permalink":"http://nextnight.github.io/categories/ML/NLP/"}],"tags":[{"name":"分词","slug":"分词","permalink":"http://nextnight.github.io/tags/分词/"},{"name":"向量化","slug":"向量化","permalink":"http://nextnight.github.io/tags/向量化/"}]},{"title":"mac jupyter中文显示设置","slug":"Mac-jupyter中文显示设置","date":"2018-09-13T15:08:34.000Z","updated":"2018-09-29T07:12:29.467Z","comments":true,"path":"2018/09/13/Mac-jupyter中文显示设置/","link":"","permalink":"http://nextnight.github.io/2018/09/13/Mac-jupyter中文显示设置/","excerpt":"","text":"MAC-Jupyter使用matplotlib时设置中文123456789import matplotlib as mplimport matplotlib.pyplot as pltfrom matplotlib.font_manager import _rebuild_rebuild()%matplotlib inline# 设置中文显示mpl.rcParams['font.family']=['Microsoft Yahei']","categories":[{"name":"Python","slug":"Python","permalink":"http://nextnight.github.io/categories/Python/"}],"tags":[{"name":"Jupyter","slug":"Jupyter","permalink":"http://nextnight.github.io/tags/Jupyter/"},{"name":"matplotlib","slug":"matplotlib","permalink":"http://nextnight.github.io/tags/matplotlib/"}]},{"title":"Pandas基本操作","slug":"Pandas基本操作","date":"2018-09-13T14:58:52.000Z","updated":"2018-09-19T01:51:49.094Z","comments":true,"path":"2018/09/13/Pandas基本操作/","link":"","permalink":"http://nextnight.github.io/2018/09/13/Pandas基本操作/","excerpt":"","text":"Pandas 操作分类 基本设置 数据描述 数据清洗 数据索引 数据连接 分组聚合 文本操作 基本设置① ：设置不使用科学计数法，保留5位小数1pd.set_option('display.float_format', lambda x: '%.5f' % x) ② : 设置显示结果的宽度(不截断换行)1pd.set_option('display.width', 1000) 数据读取123456pd.read_csv()pd.read_table()pd.read_excle()pd.read_html()pd.read_json()pd.read_sql() 数据描述12import pandas as pddf = pd.read_csv(\"path\") ①：查看数据属性12345678# \b查看属性df.shapedf.indexdf.columnsdf.size# 去除格式，获取数据数组df.values ②：查看数据信息12345678910111213# 数据信息df.head(n) # 查看前n行df.tail(n) # 查看尾n行# 查看数据信息df.info()# 查看数据描述：df.describe() # 所有列描述df['col_name'].describe()# 某一列描述# 统计某列每个值的量:默认会从大到小排序df['clo_name'].value_counts(sort=True,ascending=False,bins=None,dropna=False) 数据索引索引设置123456789101112131415\"\"\"index,索引操作 1、set_index() 2、reset_index() 3、rename()\"\"\"# 设置索引df.set_index('A') # 设置列A为索引列# 重置索引:恢复默认的自增索引df.reset_index(inplace=True) # 设置inplace的时候会修改原始df,并且没有返回值df = df.reset_index() # 不设置inplace=True需接受返回值，且原始df不变df.rename(index=lambda x:x+1)# 所有索引值+1\"\"\"修改列名\"\"\"df.rename(&#123;\"A\":\"AAAAA\",\"B\":\"BBBB\"&#125;,inplace=True) # 把A修改成AAAAAdf.rename(columns=lambda x: x + 2) # 将全体列重命名 索引数据12345678910111213\"\"\"直接列索引：\"\"\"df[1] # 第一列df['Q'] #Q列\"\"\"\"行索引： 1、iloc 2、loc 3、ix\"\"\"\"df.loc[df['Q']=0] # 索引满足Q=0的虽偶有行数据df.loc[(df['Q'] &amp; df['T']=1)]df.loc[(df['Q'] &amp; df['T']=1) 'x'] = -1 # 满足条件的所有行数据给X 列赋值为-1 数据清洗数据缺失处理12345678910111213141516171819202122232425\"\"\"判断缺失: 1、isnull(), 2、isna()\"\"\"df.isnull() # 返回每个数据是否是nulldf.isna() # 返回每个数据是否是na\"\"\"缺失填充: 1、fillna(), 2、ffill(), 3、bfill()\"\"\"df.ffill()df.bfill()df.fillna(0)\"\"\"缺失删除:dropna()param: axis:1,0 axis=1 删除含有na的列,axis=0 删除含有na的行, how:any,all how='any'表示某列所有行存在为null即删除,how='all'，表示某行所有列都为null才删除 subset:选择子集 inplace:是否覆盖原数据\"\"\"df = df.dropna(self, axis=0, how='any', thresh=None, subset=None,inplace=False)df.dropna(self, axis=1, how='all', thresh=None, subset=['col1','col2'],inplace=True) 数据异常处理123456789101112131415161718\"\"\"数据删除:drop()param: axis:0,1 分别对应indexs,columns\"\"\"df.drop(self, labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')df.drop(['a','b'],axis=1,inplace=True) # 删除a，b列df.drop([0,1],axis=0,inplace=True) # 删除0，1行\"\"\"数据删除：del\"\"\"del df['A'] # 删除A列\"\"\"\b数值替换:replace()\"\"\"df.replace([1,3],['one','three']) # 1替换成one,3替换成threedf.rename(&#123;1:'one',3:'three'&#125;) 数据过滤,选择12345df[(df['age']&gt;10) &amp; (df['age']&lt;40) ]df.loc[(df['age']&gt;10) &amp; (df['age']&lt;40) ]df[df['age']==20]df[df['age'].isin([2,3,5]) ] 数据排序123\"\"\"排序：sort_values(by='column',inplace=None,acsending=True) \"\"\"df.sort_values(by='',inplace=True,acsending=False) # acsending=False 从大到小 数据编码123\"\"\"one_hot:get_dummies()\"\"\"\"dt_one_hot = pd.get_dummies(dt[['A','B']]) # 对A，B 列进行onehot编码 数据分箱123\"\"\"df.cut()\"\"\"df_col1 = pd.cut(df['column1'], bins=[0, 0.5, 0.8, 2, 20, 1000], labels=np.arange(1, 6, 1))","categories":[{"name":"Python","slug":"Python","permalink":"http://nextnight.github.io/categories/Python/"},{"name":"Pandas","slug":"Python/Pandas","permalink":"http://nextnight.github.io/categories/Python/Pandas/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://nextnight.github.io/tags/Python/"},{"name":"Pandas","slug":"Pandas","permalink":"http://nextnight.github.io/tags/Pandas/"}]},{"title":"Numpy基本操作","slug":"Numpy基本操作","date":"2018-09-10T14:45:39.000Z","updated":"2018-09-15T02:25:03.415Z","comments":true,"path":"2018/09/10/Numpy基本操作/","link":"","permalink":"http://nextnight.github.io/2018/09/10/Numpy基本操作/","excerpt":"","text":"12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485import numpy as np\"\"\"numpy的基本应用1、四个属性 ndim:获取数据的维度数 shape:收取数据的维度 dtype:获取数据的类型 size:2、N个方法 1、zeros,ones,empty,arange 2、max,min,std,mean,sum,var,median,cumsum 3、astype,reshape 4、argmax,argmin,argsort 5、all,any,fill,where，diff 6、vstack,hstack 7、unique 8、load,save:操作文件 9、insert,arange,reshape 10、bincount\"\"\"def numpy_filed(): ll = np.array([[1, 2, 3, 5, 6], [2, 3, 4, 5, 7]]) print(ll) print(ll.ndim) # 2；表示2个维度 print(ll.shape) # (5,):表示五个元素 print(ll.dtype) # 元素类型int64:表示数据类型 print(ll.size) # 10：元素个数 print(type(ll)) # 对象类型&lt;class 'numpy.ndarray'&gt;def numpy_method(): \"\"\" max,min,std,mean,sum,var,median,cumsum astype:转换数据类型 argmax,argmin,argsort all,any,fill,where vstack,hstack load,save:操作文件 insert,arange,reshape \"\"\" ll = np.array([[1, 4, 3, 5, 6], [2, 3, 4, 5, 7]]) # max,min,std,mean,sum,var,median,cumsum print(ll.max()) # 全局最大值 print(ll.max(axis=1)) # axis=1：行最大值 [6 7] print(ll.max(axis=0)) # axis=0 列最大 [2 4 4 5 7] print(ll.cumsum()) # 前所有项的和组成的序列 # astype: 转换数据类型 ll = ll.astype(dtype=np.float64) print(ll.dtype) # float64 # argmax, argmin, argsort print(ll.argmax(axis=1)) # axis=1 返回每行的最大数值的索引[4,4] print(ll.argmin(axis=0)) # axis=0 返回每列的最小值的索引 [1 1 1 0 1] print(ll.argsort(axis=0))# 按行排序返回排序后的索引，未指定排序的列，就是返回所有的列排序的结果 # all,any,fill,where，diff # all()全部满足条件，any()存在满足条件的 # vstack,hstack p1 = np.zeros((3,3)) p2 = np.ones((3,3)) print(\"纵向叠加：\\n\",np.vstack((p1,p2))) # 数据拼接，维度不变 print(\"横向叠加：\\n\",np.hstack((p1,p2))) # 数据连接，行数不变 # unique：找出唯一值并返回排序后的结果 print(np.unique(ll)) # [1. 2. 3. 4. 5. 6. 7.]if __name__ == '__main__': numpy_filed() numpy_method()","categories":[{"name":"Python","slug":"Python","permalink":"http://nextnight.github.io/categories/Python/"},{"name":"Numpy","slug":"Python/Numpy","permalink":"http://nextnight.github.io/categories/Python/Numpy/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://nextnight.github.io/tags/Python/"},{"name":"Numpy","slug":"Numpy","permalink":"http://nextnight.github.io/tags/Numpy/"}]},{"title":"Python代码片段","slug":"Python代码片段","date":"2018-09-10T14:03:51.000Z","updated":"2018-09-29T07:25:46.051Z","comments":true,"path":"2018/09/10/Python代码片段/","link":"","permalink":"http://nextnight.github.io/2018/09/10/Python代码片段/","excerpt":"","text":"1、多变量赋值1a, b, c, d = 0, 1, 2, 3 2、列表赋值12names = [\"ami\", \"kimi\", \"jsm\"]a, b, c = names 3、条件表达式12x = 8d = x if x &gt; 5 else 10 4、列表推导式1a = [i for i in range(1000) if i % 2 == 0] 5、条件判断:不是用and12x = 90if 80 &lt; x &lt; 100: print(x) 6、判断是否在/不在某列表,字符串12345if 1 in [1, 2, 3]: print(1)if 1 not in [1, 2, 3]: print(0)if '1' in \"123\": print(2)if '1' not in \"123\": print(2) 7、隐含类型转换判空123456789a, b, c, d = [1, 2, 3], &#123;&#125;, '', []if a: print(\"a not empty\")if b: print(\"b not empty\")if c: print(\"c not empty\")if d: print(\"d not empty\") 8、判断多个条件是否成立:any，all123a, b, c = 1, 2, 3if any([a &gt; 1, b &lt; 2, c == 3]): pass # === a&gt;1 or b&lt;2 or c==3if all([a &gt; 1, b &lt; 2, c == 3]): pass # === a&gt;1 and b&lt;2 and c==3 9、列表推导式+过滤123ls = [1, 2, 3, \"a\", 4, \"v\", 5.5]rs = [i for i in ls if type(i) in [int, float]]print(rs) 10、同时获取下标和数据：enumerate123nums = [1, 2, 3, 4]for index, num in enumerate(nums): print(\"索引为&#123;&#125;的数据是&#123;&#125;\".format(index, num)) 11、线程sleep123import timetime.sleep(1) # 休眠1秒 12、print 输出覆盖1234i, n = 0, 100for i in range(n): time.sleep(0.1) if (i + 1) % 10 == 0: print(i + 1, end = '\\r') 13、lambda匿名函数123names = ['a', 'b', 'xxx', 'vx', 'ccc']rs = filter(lambda x:len(x) &lt;= 1, names)print(list(rs)) # ['a', 'b'] 14、yield生成器收集系列值，不需要return1234567def fun(): a = 0 for i in range(10): a += i yield a# [0, 1, 3, 6, 10, 15, 21, 28, 36, 45]print(list(fun())) 15、装饰器给函数添加插入日志，性能测试等非核心功能12345678910111213141516def runtime(func): def wrapper(*args, **kwargs): start = time.time() result = func(*args, **kwargs) end = time.time() print(\"&#123;&#125; is called,used &#123;&#125;s.\".format(func.__name__, start - end)) return result return (wrapper)@runtimedef process(): s = 0 for i in range(100): time.sleep(1) s += iprocess() 16、内存拷贝使用：copy包中的copy()函数和deepcopy()函数 16.1：赋值：指向同一块地址 1234a = &#123;1: [1, 2, 3]&#125;print('a的内存地址 %s' % id(a)) # 4386109840print('a1的内存地址 %s' % id(a[1])) # 4391711560b = a 16.2：浅拷贝：指向不同的引用，但是不同引用指向相同内容(只拷贝对象，但不拷贝对象内部的对象) 12345b = copy.copy(a)print('a的内存地址 %s' % id(a)) # 4386109840print('b的内存地址 %s' % id(b)) # 4386110200print('a1的内存地址 %s' % id(a[1])) # 4391711560print('b1的内存地址 %s' % id(b[1])) # 4391711560 16.3：深拷贝：对象及对象内部的对象都复制一份 12345b = copy.deepcopy(a)print('a的内存地址 %s' % id(a)) # 4386109840print('b的内存地址 %s' % id(b)) # 4391729264print('a1的内存地址 %s' % id(a[1])) # 4391711560print('b1的内存地址 %s' % id(b[1])) # 4391711368 17、参数传递","categories":[{"name":"Python","slug":"Python","permalink":"http://nextnight.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://nextnight.github.io/tags/Python/"}]},{"title":"数据结构与算法","slug":"数据结构与算法","date":"2018-08-14T03:57:08.000Z","updated":"2018-08-27T06:56:00.076Z","comments":true,"path":"2018/08/14/数据结构与算法/","link":"","permalink":"http://nextnight.github.io/2018/08/14/数据结构与算法/","excerpt":"","text":"是时候沉下心来学习一波数据结构与算法了，好的算法能够轻松的而解决各种难题，而好的数据结构是实现算法的前提，算法设计依托与不同的数据结构，同样，算法也是解决不同形式的数据问题。计划步骤： 1、深入操作基本数据结构 2、深入\b回顾基础算法 3、深入理解基本算法思想 4、算法刷题，书籍阅读 算法思想 递归 穷举 递推 贪心 回溯 分治 动态规划 数据结构 数组 链表 栈 队列 字符串 🌲树 trie树 哈希 图 基础算法排序 交换排序 选择排序 插入排序 冒泡排序 快速排序 堆排序 希尔排序 归并排序 线性排序 桶排序 查找 顺序查找 二分查找 分块查找 动态查找 二叉排序树 平衡二叉树 B树，B+树 Hash查找 经典算法 快速排序 BFS/DFS KMP A*寻路 Dijkstra 遗传算法 动态规划 海量数据处理 Hash Bitmap Bloom filter Trie树 Index Inverted Index simhash","categories":[{"name":"算法","slug":"算法","permalink":"http://nextnight.github.io/categories/算法/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://nextnight.github.io/tags/数据结构/"},{"name":"算法","slug":"算法","permalink":"http://nextnight.github.io/tags/算法/"}]},{"title":"LeetCode-Array","slug":"\b\bLeetCode-Array","date":"2018-07-31T15:21:37.000Z","updated":"2018-09-29T07:29:57.847Z","comments":true,"path":"2018/07/31/\b\bLeetCode-Array/","link":"","permalink":"http://nextnight.github.io/2018/07/31/\b\bLeetCode-Array/","excerpt":"","text":"整数求和1234567891011def a_num_sum(num): \"\"\"整数各位之和：2018-07-31\"\"\" \"\"\" Q: 给一个非负整数，求每一位的数字加和，得到之后的数字在把每一位进行加和，直到结果为一位数 E: 比如：83-&gt;8+3=11-&gt;1+1=2,输出2 case: 尾递归，进入下一层不再需要上一层的环境，因为这个递归完成后不再需要干其他的事，所以直接return这个递归，就会得到最内层的结果 \"\"\" if num &lt; 10: return num if num &gt;= 10: return a_num_sum(sum([int(i) for i in list(str(num))])) # 没有return就不是一个尾递归，返回结果为none 12345# 如果要输出每次计算的值print(num) # 83 11 2if num &gt;= 10: a_num_sum(sum([int(i) for i in list(str(num))]))print(num) # 2,11,83 求两数之和等与目标值基础暴力解法12345678910111213def two_sum(dt, tag): \"\"\"整数翻转：2018-07-31\"\"\" \"\"\" Q:给定一个整数数组和一个目标值，找出数组中和为目标值的两个数。你可以假设每个输入只对应一种答案，且同样的元素不能被重复利用。 E:[2,7,3,11] tag:9 R:[0,1] E:[3,3] tag:6 R:[0,1] \"\"\" for i,it in enumerate(dt): for j,jt in enumerate(dt): if it+jt==tag and i!=j: return [i,j] 以下解法同上：看上去没那么高复杂度，其实是一样的。123for i, item in enumerate(dt): if (tag - item) in dt and dt.index(tag - item)!=i: return [i, dt.index(tag - item)] 遍历一遍数组解法123456789101112//hashmap，dict解法，public int[] twoSum(int[] nums, int target) &#123; Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); for (int i = 0; i &lt; nums.length; i++) &#123; int complement = target - nums[i]; if (map.containsKey(complement)) &#123; return new int[] &#123; map.get(complement), i &#125;; &#125; map.put(nums[i], i); &#125; throw new IllegalArgumentException(\"No two sum solution\");&#125; 整数翻转123456789101112131415def reverse(num): \"\"\"整数翻转：2018-08-01\"\"\" \"\"\" Q:给定一个 32 位有符号整数，将整数中的数字进行反转。假设我们的环境只能存储 32 位有符号整数， 其数值范围是 [−2**31, 2**31 − 1]。根据这个假设，如果反转后的整数溢出，则返回 0。 \"\"\" sum = 0 while True: lens = len(str(num)) - 1 sum += (num % 10) * 10**lens num = int(num/10) if lens==0: if sum&lt;-2**31 or sum&gt;2**31-1: return 0 return sum 数组中子序列最大和1234567891011121314151617181920def max_sum_subarry(nums): \"\"\"子串最大和:2018-08-02\"\"\" \"\"\" Q:给定一个数组求其子数组和最大的值。 E:[1, -2, 3, 10, -4, 7, 2, -5] R:[3,10,-4,7,2]=18 trick: 动态规划 \"\"\" if nums is None: return None if len(nums) == 1: return nums[0] sum = nums[0] # 第i-1个数的最大序列和 cur = nums[0] # 第i个数的最大序列和 for i, it in enumerate(nums[1:]): cur = max(cur + it, it) # 状态方程：max(sum_i) = max(sum_i-1+num[i],num[i]) if sum &lt; cur: sum = cur return sum 数组中最大和的子序列1234567891011121314151617181920212223242526272829def max_subarray(nums): \"\"\"最大和的子串:2018-08-03\"\"\" \"\"\" Q:给定一个数组求其子数组和最大的值。 E:[1, -2, 3, 10, -4, 7, 2, -5] R:[3,10,-4,7,2]=18 trick: 动态规划 \"\"\" if nums is None: return 0 if len(nums) == 1: return nums[0] sum = nums[0] # 第i-1个数的最大序列和 cur = nums[0] # 第i个数的最大序列和 maxls = [nums[0]] # 用于和最大的子序列 for i, it in enumerate(nums[1:]): # cur = max(cur + it, it) # 状态方程：max(sum_i) = max(sum_i-1+num[i],num[i]) if cur + it &gt;= it: cur = cur + it maxls.append(it) else: cur = it maxls = [it] if sum &lt; cur: sum = cur # 去掉子序列尾部的负数 while maxls[-1] &lt; 0: maxls.pop(-1) return sum,maxls 股票的最大利润①🦁 link:Leetcode🐇 思路：记录最小值，和最大利润，不断用当前值-最小值去计算利润，如果大于最大利润则更新。保证当前值永远在最小值之后，因为卖出时间必须晚于或等于买入时间。1234567891011121314151617181920def max_stock(nums): \"\"\"股票最大利润-最多购买一次：2018-08-04\"\"\" \"\"\" A: 给定一个数组，它的第 i 个元素是一支给定股票第 i 天的价格。如果你最多只允许完成一笔交易（即买入和卖出一支股票）， 设计一个算法来计算你所能获取的最大利润 E:[7,1,5,3,6,4] R:[1,6] = 5 E:[7,6,4,3,1] R:0 \"\"\" if len(nums) == 1 or nums==None: return 0 mins = nums[0] stock = 0 for it in nums[1:]: mins = min(it,mins) stock = max(it-mins,stock) return stock 股票的最大利润②🦁 link:Leetcode🐇 思路：找到所有的上升区域，每个区域的差值之和就是总利润。123456789101112131415def max_stock_all(nums): \"\"\"股票最大利润-购买多次不限次数：：2018-08-05\"\"\" \"\"\" E:[7,1,5,3,6,4] R:5-1+6-3=7 \"\"\" if len(nums)==1 or nums==None: return 0 stock = 0 min = nums[0] for it in nums[1:]: if it&gt;min: stock +=it-min min = it return stock 股票购买最大利润③🦁link:🐇思路：12345def max_stock_two(): \"\"\"股票最大利润-购买次数为2：：2018-08-06\"\"\" \"\"\" \"\"\" pass 矩阵转置🦁link:Leetcode🐇思路：二维数组每一个元素的下标i行，j列变j行i列，即a[j][i] =a[i][j]1234567891011121314def transpose(A): \"\"\"转置矩阵:2018-08-07\"\"\" \"\"\" E:[[1,2,3],[4,5,6]] R:[[1,4],[2,5],[3,6]] \"\"\" # r行，c列 转置得到c行r列 r,c = len(A),len(A[0]) # 定义一个c,行r列的矩阵 rs = [ [None]*r for i in np.arange(c)] for r,row in enumerate(A): for c,it in enumerate(row): rs[c][r] = it return rs","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://nextnight.github.io/categories/Algorithm/"},{"name":"LeetCode","slug":"Algorithm/LeetCode","permalink":"http://nextnight.github.io/categories/Algorithm/LeetCode/"}],"tags":[{"name":"Array","slug":"Array","permalink":"http://nextnight.github.io/tags/Array/"}]},{"title":"不忘初心","slug":"不忘初心","date":"2018-07-30T14:16:33.000Z","updated":"2018-10-10T03:13:47.401Z","comments":true,"path":"2018/07/30/不忘初心/","link":"","permalink":"http://nextnight.github.io/2018/07/30/不忘初心/","excerpt":"","text":"MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)'],['\\\\','\\\\']]} }); 不忘初心，方得始终初心易守，始终难得 $$(\\frac12)(12)(12)$$ $$\\begin{matrix}1 &amp; x &amp; x^2 \\\\1 &amp; y &amp; y^2 \\\\1 &amp; z &amp; z^2 \\\\\\end{matrix}$$ $$\\frac { dy }{ dx } =\\frac { { e }^{ x } }{ 3{ y }^{ 2 } }$$ $$\\lim_{ x\\rightarrow \\infty }{ \\sum_{ k=1 }^{ x }{ \\frac { \\sin { k } +\\cos { k } }{ k } } } $$ $$H=-\\sum_{i=1}^N (\\sigma_{i}^x \\sigma_{i+1}^x+g \\sigma_{i}^z)$$ $$f(n) = \\begin{cases} \\frac{n}{2}, &amp; \\text{if } n\\text{ is even}\\\\ 3n+1, &amp; \\text{if } n\\text{ is odd} \\end{cases}$$ $f(x)=ax+b$ $$\\left \\lbrace \\sum_{i=0}^n i^3 = \\frac{(n^2+n)(n+6)}{9} \\right \\rbrace$$$$ \\lbrace \\sum_{i=0}^n i^3 = \\frac{(n^2+n)(n+6)}{9} \\rbrace$$$\\frac xy$$ x+3 \\over y+5 $","categories":[{"name":"Test","slug":"Test","permalink":"http://nextnight.github.io/categories/Test/"}],"tags":[{"name":"Test","slug":"Test","permalink":"http://nextnight.github.io/tags/Test/"}]}]}